[
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research is mainly related to cognitive and social psychology. Areas of research include: student perceptions of classroom grading practices, the “Uncanny Valley”, zero-sum thinking, prejudice, social justice, competitiveness, and online research methods."
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Kennedy, R., Clifford, S., Burleigh, T., Waggoner, P. D., Jewell, R., & Winter, N. J. (2020). The shape of and solutions to the MTurk quality crisis. Political Science Research and Methods, 8(4), 614-629. doi: 10.1017/psrm.2020.6 \n            \n\n            \n            \n                \n                    \n                            Research Methods\n                        \n                    \n                    \n                            Online Research\n                        \n                    \n                    \n                            Mechanical Turk\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Open access\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Schoenherr, J. R., & Burleigh, T. J. (2020). Dissociating affective and cognitive dimensions of uncertainty by altering regulatory focus. Acta Psychologica, 205, 103017. doi: 10.1016/j.actpsy.2020.103017\n            \n\n            \n            \n                \n                    \n                            Uncanny Valley\n                        \n                    \n                    \n                            Cognition\n                        \n                    \n                    \n                            Categorical Perception\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Winter, N., Burleigh, T., Kennedy, R., & Clifford, S. (2019). A simplified protocol to screen out VPS and international respondents using Qualtrics. Available at SSRN 3327274. doi: 10.2139/ssrn.3327274 \n            \n\n            \n            \n                \n                    \n                            Online Research\n                        \n                    \n                    \n                            Research Methods\n                        \n                    \n                    \n                            Mechanical Turk\n                        \n                    \n                    \n                            Qualtrics\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Burleigh, T., Kennedy, R., & Clifford, S. (2018). How to screen out VPS and international respondents using Qualtrics: A protocol. Available at SSRN 3265459. doi: 10.2139/ssrn.3265459 \n            \n\n            \n            \n                \n                    \n                            Research Methods\n                        \n                    \n                    \n                            Online Research\n                        \n                    \n                    \n                            Mechanical Turk\n                        \n                    \n                    \n                            Qualtrics\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Burleigh, T., & Rubel, A. (2018). Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy. doi: 10.1177/1363460718779781\n            \n\n            \n            \n                \n                    \n                            Online Research\n                        \n                    \n                    \n                            Demographics\n                        \n                    \n                    \n                            Sexuality\n                        \n                    \n                    \n                            Consensual Non-Monogamy\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Sparks, A., Burleigh, T., & Barclay, P. (2016). We can see inside: Accurate prediction of Prisoner's Dilemma decisions in announced games following a face-to-face interaction. Evolution and Human Behavior, 37(3), 210-216. doi: 10.1016/j.evolhumbehav.2015.11.003\n            \n\n            \n            \n                \n                    \n                            Social Cognition\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n                    \n                            Meta-Analysis\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Open Access\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Wood, J., Desmarais, S., Burleigh, T., & Milhausen, R. (2018). Reasons for sex and relational outcomes in consensually nonmonogamous and monogamous relationships: A self-determination theory approach. Journal of Social and Personal Relationships, 35(4), 632-654. doi: 10.1177/0265407517743082\n            \n\n            \n            \n                \n                    \n                            Sexuality\n                        \n                    \n                    \n                            Consensual Non-Monogamy\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Burleigh, T. J., & Meegan, D. V. (2018). Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?. Social Psychology of Education, 21, 323-335. doi: 10.1007/s11218-017-9414-x\n            \n\n            \n            \n                \n                    \n                            Social Justice\n                        \n                    \n                    \n                            Social Cognition\n                        \n                    \n                    \n                            Competition\n                        \n                    \n                    \n                            Student Perceptions\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Burleigh, T. J., Rubel, A. N., & Meegan, D. V. (2017). Wanting ‘the whole loaf’: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists. Psychology & Sexuality, 8(1-2), 24-40. doi: 10.1080/19419899.2016.1269020\n            \n\n            \n            \n                \n                    \n                            Sexuality\n                        \n                    \n                    \n                            Prejudice\n                        \n                    \n                    \n                            Consensual Non-Monogamy\n                        \n                    \n                    \n                            Zero-sum Thinking\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Ferrey, A. E., Burleigh, T. J., & Fenske, M. J. (2015). Stimulus-category competition, inhibition, and affective devaluation: a novel account of the uncanny valley. Frontiers in psychology, 6, 249. doi: 10.3389/fpsyg.2015.00249\n            \n\n            \n            \n                \n                    \n                            Cognition\n                        \n                    \n                    \n                            Categorical Perception\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Burleigh, T. J., & Schoenherr, J. R. (2015). A reappraisal of the uncanny valley: categorical perception or frequency-based sensitization?. Frontiers in Psychology, 5, 1488. doi: 10.3389/fpsyg.2014.01488\n            \n\n            \n            \n                \n                    \n                            Uncanny Valley\n                        \n                    \n                    \n                            Categorical Perception\n                        \n                    \n                    \n                            Cognition\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Open access\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Schoenherr, J. R., & Burleigh, T. J. (2015). Uncanny sociocultural categories. Frontiers in Psychology, 5, 1456. doi: 10.3389/fpsyg.2014.01456\n            \n\n            \n            \n                \n                    \n                            Uncanny Valley\n                        \n                    \n                    \n                            Cognition\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Burleigh, T. J., Schoenherr, J. R., & Lacroix, G. L. (2013). Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces. Computers in human behavior, 29(3), 759-771. doi: 10.1016/j.chb.2012.11.021\n            \n\n            \n            \n                \n                    \n                            Uncanny Valley\n                        \n                    \n                    \n                            Categorical Perception\n                        \n                    \n                    \n                            Cognition\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Burleigh, T. J., & Meegan, D. V. (2013). Keeping up with the Joneses affects perceptions of distributive justice. Social justice research, 26, 120-131. doi: 10.1007/s11211-013-0181-3\n            \n\n            \n            \n                \n                    \n                            Social Justice\n                        \n                    \n                    \n                            Social Cognition\n                        \n                    \n                    \n                            Fairness\n                        \n                    \n                    \n                            Student Perceptions\n                        \n                    \n                    \n                            Experimental\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/articles/winter-et-al-2019/index.html",
    "href": "research/articles/winter-et-al-2019/index.html",
    "title": "A simplified protocol to screen out VPS and international respondents using Qualtrics",
    "section": "",
    "text": "This protocol is a much simplified update of our original protocol for screening out international and VPS respondents from Qualtrics surveys. It utilizes Qualtrics’ built-in Web Service functionality to interact with IP Hub in looking up potentially problematic respondents."
  },
  {
    "objectID": "research/articles/winter-et-al-2019/index.html#abstract",
    "href": "research/articles/winter-et-al-2019/index.html#abstract",
    "title": "A simplified protocol to screen out VPS and international respondents using Qualtrics",
    "section": "",
    "text": "This protocol is a much simplified update of our original protocol for screening out international and VPS respondents from Qualtrics surveys. It utilizes Qualtrics’ built-in Web Service functionality to interact with IP Hub in looking up potentially problematic respondents."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2020/index.html",
    "href": "research/articles/schoenherr-burleigh-2020/index.html",
    "title": "Dissociating affective and cognitive dimensions of uncertainty by altering regulatory focus",
    "section": "",
    "text": "Cognitive uncertainty is evidenced across learning, memory, and decision-making tasks. Uncertainty has also been examined in studies of positive affect and preference by manipulating stimulus presentation frequency. Despite the extensive research in both of these areas, there has been little systematic study into the relationship between affective and cognitive uncertainty. Using a categorization task, the present study examined changes in cognitive and affective uncertainty by manipulating stimulus presentation frequency and processing focus (i.e., promotion v. prevention focus). Following training, participants categorized stimuli and provided ratings of both typicality and negative affect. Results indicated that cognitive uncertainty was influenced by a categorical representation of stimuli whereas affective uncertainty was also influenced by exemplar presentation frequency during training. We additionally found that when the training was framed in terms of the avoidance of errors (i.e., a prevention focus), categorization performance was affected across the stimulus continuum whereas affective ratings remained unchanged."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2020/index.html#abstract",
    "href": "research/articles/schoenherr-burleigh-2020/index.html#abstract",
    "title": "Dissociating affective and cognitive dimensions of uncertainty by altering regulatory focus",
    "section": "",
    "text": "Cognitive uncertainty is evidenced across learning, memory, and decision-making tasks. Uncertainty has also been examined in studies of positive affect and preference by manipulating stimulus presentation frequency. Despite the extensive research in both of these areas, there has been little systematic study into the relationship between affective and cognitive uncertainty. Using a categorization task, the present study examined changes in cognitive and affective uncertainty by manipulating stimulus presentation frequency and processing focus (i.e., promotion v. prevention focus). Following training, participants categorized stimuli and provided ratings of both typicality and negative affect. Results indicated that cognitive uncertainty was influenced by a categorical representation of stimuli whereas affective uncertainty was also influenced by exemplar presentation frequency during training. We additionally found that when the training was framed in terms of the avoidance of errors (i.e., a prevention focus), categorization performance was affected across the stimulus continuum whereas affective ratings remained unchanged."
  },
  {
    "objectID": "research/articles/rubel-burleigh-2018/index.html",
    "href": "research/articles/rubel-burleigh-2018/index.html",
    "title": "Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy",
    "section": "",
    "text": "Despite a growing interest in polyamory, it is unknown how many polyamorists there are in the general population. In acknowledging that the meaning of “polyamory” is contested (e.g. Klesse, 2014), we estimated the prevalence of polyamory when it was defined as: (1) an identity, (2) relationship beliefs/preferences, (3) relationship status, and (4) relationship agreements. We recruited 972 individuals from Mechanical Turk and used a sample weighting procedure to approximate a representative sample of the population of the USA. Point prevalence estimates ranged from about 0.6% to 5%, and lifetime estimates ranged from about 2% to 23%. Thus, we estimate that there are at least 1.44 million adults in the USA who count as polyamorous."
  },
  {
    "objectID": "research/articles/rubel-burleigh-2018/index.html#abstract",
    "href": "research/articles/rubel-burleigh-2018/index.html#abstract",
    "title": "Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy",
    "section": "",
    "text": "Despite a growing interest in polyamory, it is unknown how many polyamorists there are in the general population. In acknowledging that the meaning of “polyamory” is contested (e.g. Klesse, 2014), we estimated the prevalence of polyamory when it was defined as: (1) an identity, (2) relationship beliefs/preferences, (3) relationship status, and (4) relationship agreements. We recruited 972 individuals from Mechanical Turk and used a sample weighting procedure to approximate a representative sample of the population of the USA. Point prevalence estimates ranged from about 0.6% to 5%, and lifetime estimates ranged from about 2% to 23%. Thus, we estimate that there are at least 1.44 million adults in the USA who count as polyamorous."
  },
  {
    "objectID": "research/articles/rubel-burleigh-2018/index.html#important-figure",
    "href": "research/articles/rubel-burleigh-2018/index.html#important-figure",
    "title": "Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 1. Interest in polyamory has been increasing over time, as measured by records in Google Ngram database with “polyamory” keyword (top) and records in the Web of Science database with “polyamory” or “polyamorous” keywords (bottom)."
  },
  {
    "objectID": "research/articles/ferrey-et-al-2015/index.html",
    "href": "research/articles/ferrey-et-al-2015/index.html",
    "title": "Stimulus-category competition, inhibition and affective devaluation: A novel account of the Uncanny Valley",
    "section": "",
    "text": "Stimuli that resemble humans, but are not perfectly human-like, are disliked compared to distinctly human and non-human stimuli. Accounts of this “Uncanny Valley” effect often focus on how changes in human resemblance can evoke different emotional responses. We present an alternate account based on the novel hypothesis that the Uncanny Valley is not directly related to ‘human-likeness’ per se, but instead reflects a more general form of stimulus devaluation that occurs when inhibition is triggered to resolve conflict between competing stimulus-related representations. We consider existing support for this inhibitory-devaluation hypothesis and further assess its feasibility through tests of two corresponding predictions that arise from the link between conflict-resolving inhibition and aversive response: (1) that the pronounced disliking of Uncanny-type stimuli will occur for any image that strongly activates multiple competing stimulus representations, even in the absence of any human-likeness, and (2) that the negative peak of an ‘Uncanny Valley’ should occur at the point of greatest stimulus-related conflict and not (in the presence of human-likeness) always closer to the ‘human’ end of a perceptual continuum. We measured affective responses to a set of line drawings representing non-human animal–animal morphs, in which each continuum midpoint was a bistable image (Experiment 1), as well as to sets of human-robot and human-animal computer-generated morphs (Experiment 2). Affective trends depicting classic Uncanny Valley functions occurred for all continua, including the non-human stimuli. Images at continua midpoints elicited significantly more negative affect than images at endpoints, even when the continua included a human endpoint. This illustrates the feasibility of the inhibitory-devaluation hypothesis and the need for further research into the possibility that the strong dislike of Uncanny-type stimuli reflects the negative affective consequences of cognitive inhibition."
  },
  {
    "objectID": "research/articles/ferrey-et-al-2015/index.html#abstract",
    "href": "research/articles/ferrey-et-al-2015/index.html#abstract",
    "title": "Stimulus-category competition, inhibition and affective devaluation: A novel account of the Uncanny Valley",
    "section": "",
    "text": "Stimuli that resemble humans, but are not perfectly human-like, are disliked compared to distinctly human and non-human stimuli. Accounts of this “Uncanny Valley” effect often focus on how changes in human resemblance can evoke different emotional responses. We present an alternate account based on the novel hypothesis that the Uncanny Valley is not directly related to ‘human-likeness’ per se, but instead reflects a more general form of stimulus devaluation that occurs when inhibition is triggered to resolve conflict between competing stimulus-related representations. We consider existing support for this inhibitory-devaluation hypothesis and further assess its feasibility through tests of two corresponding predictions that arise from the link between conflict-resolving inhibition and aversive response: (1) that the pronounced disliking of Uncanny-type stimuli will occur for any image that strongly activates multiple competing stimulus representations, even in the absence of any human-likeness, and (2) that the negative peak of an ‘Uncanny Valley’ should occur at the point of greatest stimulus-related conflict and not (in the presence of human-likeness) always closer to the ‘human’ end of a perceptual continuum. We measured affective responses to a set of line drawings representing non-human animal–animal morphs, in which each continuum midpoint was a bistable image (Experiment 1), as well as to sets of human-robot and human-animal computer-generated morphs (Experiment 2). Affective trends depicting classic Uncanny Valley functions occurred for all continua, including the non-human stimuli. Images at continua midpoints elicited significantly more negative affect than images at endpoints, even when the continua included a human endpoint. This illustrates the feasibility of the inhibitory-devaluation hypothesis and the need for further research into the possibility that the strong dislike of Uncanny-type stimuli reflects the negative affective consequences of cognitive inhibition."
  },
  {
    "objectID": "research/articles/ferrey-et-al-2015/index.html#important-figure",
    "href": "research/articles/ferrey-et-al-2015/index.html#important-figure",
    "title": "Stimulus-category competition, inhibition and affective devaluation: A novel account of the Uncanny Valley",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFIGURE 4. Computer-generated morphs (human-robot, human-stag, human-tiger, human-lion, human-bird)."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2017/index.html",
    "href": "research/articles/burleigh-meegan-2017/index.html",
    "title": "Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?",
    "section": "",
    "text": "When students are faced with the decision of whether to assist a peer, they should be sensitive to the potential risks associated with doing so. Two factors associated with risky helping behaviour in the classroom are: (1) the grading practices that are used, and (2) knowledge of a peer’s relative status. Normative (“curved”) grading creates a situation in which peer-interactions are potentially competitive, but it is only those interactions with peers of a similar status that carry the potential for assistance to be costly to oneself. In two studies, we created hypothetical scenarios in which the grading practices (normative or absolute) and peer-status proximity (proximate, distant, or unknown) were manipulated, and asked participants to report their willingness to cooperate with a peer by sharing their notes from an important lecture. We found that when normative grading was used, individuals were less willing to assist a peer when they knew that the peer’s status was proximate to their own. There was also less cooperation when peer status was unknown, under normative grading, which is consistent with a risk-aversion tendency."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2017/index.html#abstract",
    "href": "research/articles/burleigh-meegan-2017/index.html#abstract",
    "title": "Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?",
    "section": "",
    "text": "When students are faced with the decision of whether to assist a peer, they should be sensitive to the potential risks associated with doing so. Two factors associated with risky helping behaviour in the classroom are: (1) the grading practices that are used, and (2) knowledge of a peer’s relative status. Normative (“curved”) grading creates a situation in which peer-interactions are potentially competitive, but it is only those interactions with peers of a similar status that carry the potential for assistance to be costly to oneself. In two studies, we created hypothetical scenarios in which the grading practices (normative or absolute) and peer-status proximity (proximate, distant, or unknown) were manipulated, and asked participants to report their willingness to cooperate with a peer by sharing their notes from an important lecture. We found that when normative grading was used, individuals were less willing to assist a peer when they knew that the peer’s status was proximate to their own. There was also less cooperation when peer status was unknown, under normative grading, which is consistent with a risk-aversion tendency."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2017/index.html#important-figure",
    "href": "research/articles/burleigh-meegan-2017/index.html#important-figure",
    "title": "Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 1. Histograms of responses in the Normative grading conditions, indicating the presence of a bimodal distribution in the Uncertain condition, and unimodal distributions in the Distant and Proximate conditions."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2018/index.html",
    "href": "research/articles/burleigh-et-al-2018/index.html",
    "title": "How to Screen Out VPS and International Respondents Using Qualtrics: A Protocol",
    "section": "",
    "text": "This protocol provides a method and code used to screen out respondents on Amazon’s Mechanical Turk (MTurk), or other microtask service providers, who are using VPS to cover their location or are responding from a country other than the one the researcher is targeting. It is designed for surveys using Qualtrics software, although it could be easily adapted for other online survey systems that provide JavaScript support. This protocol is likely to be broadly useful in addressing the quality crisis that has affected MTurk studies."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2018/index.html#abstract",
    "href": "research/articles/burleigh-et-al-2018/index.html#abstract",
    "title": "How to Screen Out VPS and International Respondents Using Qualtrics: A Protocol",
    "section": "",
    "text": "This protocol provides a method and code used to screen out respondents on Amazon’s Mechanical Turk (MTurk), or other microtask service providers, who are using VPS to cover their location or are responding from a country other than the one the researcher is targeting. It is designed for surveys using Qualtrics software, although it could be easily adapted for other online survey systems that provide JavaScript support. This protocol is likely to be broadly useful in addressing the quality crisis that has affected MTurk studies."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2013/index.html",
    "href": "research/articles/burleigh-et-al-2013/index.html",
    "title": "Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces",
    "section": "",
    "text": "The uncanny valley theory (UVT) (Mori, 1970) proposes that when stimuli are defined by a near-perfect resemblance to humans they cause people to experience greater negative affect relative to when they have perfect human likeness (HL) or little to no HL. Empirical research to support this non-linear relationship between negative affect and HL has been inconclusive, however, and a satisfactory causal explanation has not yet emerged to explain existing findings. In two studies, we examined the relationship between HL and eeriness using digital human faces. First, we examined the relationship between HL and eeriness while controlling for extraneous variation in stimulus appearance. We created two HL continua by manipulating the facial proportions and polygon count of several digital human models. Second, we proposed and tested two causal hypotheses regarding the uncanny valley phenomenon that we refer to as category conflict and feature atypicality. We created two additional HL continua by manipulating the skin coloration and category membership of models. Across these continua we introduced an atypical feature. Our results suggest that HL is linearly related to emotional response, except under conditions where HL varies by category membership, suggesting that previous empirical findings might be explained as a category conflict."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2013/index.html#abstract",
    "href": "research/articles/burleigh-et-al-2013/index.html#abstract",
    "title": "Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces",
    "section": "",
    "text": "The uncanny valley theory (UVT) (Mori, 1970) proposes that when stimuli are defined by a near-perfect resemblance to humans they cause people to experience greater negative affect relative to when they have perfect human likeness (HL) or little to no HL. Empirical research to support this non-linear relationship between negative affect and HL has been inconclusive, however, and a satisfactory causal explanation has not yet emerged to explain existing findings. In two studies, we examined the relationship between HL and eeriness using digital human faces. First, we examined the relationship between HL and eeriness while controlling for extraneous variation in stimulus appearance. We created two HL continua by manipulating the facial proportions and polygon count of several digital human models. Second, we proposed and tested two causal hypotheses regarding the uncanny valley phenomenon that we refer to as category conflict and feature atypicality. We created two additional HL continua by manipulating the skin coloration and category membership of models. Across these continua we introduced an atypical feature. Our results suggest that HL is linearly related to emotional response, except under conditions where HL varies by category membership, suggesting that previous empirical findings might be explained as a category conflict."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2013/index.html#highlights",
    "href": "research/articles/burleigh-et-al-2013/index.html#highlights",
    "title": "Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces",
    "section": "Highlights",
    "text": "Highlights\nWe create four human likeness continua using multiple digital human models. This is done by manipulating facial proportions, realism, and category membership. We obtain human likeness and emotion ratings, and examine their relationships. Linearity is found among all continua except for the category membership continuum. We suggest that this result may explain previous evidence of the uncanny valley."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2025/01/12/index.html",
    "href": "blog/2025/01/12/index.html",
    "title": "A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices",
    "section": "",
    "text": "I’ve been doing a lot of work with LLM-based evaluations lately, and I’ve been thinking about how to improve the quality of these evaluations.\nI like to read research papers from arXiv for inspiration, and I recently came across a paper called Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates, which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.\nI was initially impressed by the results, as they reported a gain of 6-8% over the baseline method(!!). However, I am often skeptical of comparisons to “baseline” models in these research papers, as I find that they often fail to implement standard best practices and are therefore not represenative of true gains over baseline.\nGiven this skepticism of mine, I decided that it might be interesting to put it to the test: What if I implemented the SAMRE method, and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?\nUsing a sample of 180 conversations from MT-bench for testing and evaluation, I evaluated three methods:\n\nSAMRE, as implemented in the paper\nBaseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)\nBaseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.\n\nAfter running the evaluations and calculating Krippendorff alpha agreement with human judges, I found that although SAMRE did yield better agreement than Baseline-Weak (18% improvement), it was inferior to Baseline-Strong (which was 36% better than SAMRE!). A similar result was found when examining binary classification accuracy using Matthews Correlation Coefficient (MCC).\nThese results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a “baseline model”. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.\n\nBaseline model prompt inadequacies\nFirst, let’s consider some of the inadequacies in the Baseline model’s prompt reported in the paper. The prompt they used was as follows:\nYou are a fair, impartial judge scoring a debate on the following question:\nquestion.\nAnswer 1: answer_1\nAnswer 2: answer_2\nScore each answer on a scale of 1-20 for each of the following criteria:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nProvide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!\nRelevance:\nAccuracy:\nDepth:\nClarity:\nLogic and Factuality:\nAddressing opponent’s points:\nFinal Scores (sum of above) as a tuple (example: (18, 9)):\nExplain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.\nFinally, return the final score tuple (score1, score2) as a tuple (in parentheses).\nExample: (18, 9)\nYour scores and explanation:\nHere are the issues I see with this prompt:\n\nThe prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like , , and , but in a pinch delimiters like triple backticks can be used.\nThe prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.\nAlthough the prompt asks the model to “explain your scoring”, it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.\nIt’s unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.\nAlthough the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.\nThe prompt includes an “Effectiveness in addressing opponent’s points” criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.\nFinally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.\n\nWith all of that in mind, here’s how I would rewrite the prompt:\nYou are a fair, impartial judge scoring a debate on Question.\n\n&lt;Question&gt;\n{question}\n&lt;/Question&gt;\n\nTwo Answers have been given to the Question.\n\n&lt;Answer1&gt;\n{answer_1}\n&lt;/Answer1&gt;\n\n&lt;Answer2&gt;\n{answer_2}\n&lt;/Answer2&gt;\n\nThe Answers are being judged on the following Criteria:\n\n&lt;Criteria&gt;\n&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;\n&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;\n&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;\n&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;\n&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;\n&lt;/Criteria&gt;\n\nFor each Criterion, briefly analyze the performance of \nthe two Answers, then give a score between 1 and 10.\n\nRespond as follows:\n&lt;Criterion1&gt;\n&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;\n&lt;Analysis&gt;\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n&lt;/Analysis&gt;\n&lt;Scores&gt;\n&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;\n&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;\n&lt;/Scores&gt;\n&lt;/Criterion1&gt;\n&lt;Criterion2&gt;\n&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;\n&lt;Analysis&gt;\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n&lt;/Analysis&gt;\n&lt;Scores&gt;\n&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;\n&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;\n&lt;/Scores&gt;\n&lt;/Criterion2&gt;\n...\nNotice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.\nI’ve also change the scale from 1-20 to 1-10, removed the unnecessary “Effectiveness in addressing opponent’s points” criterion, and removed the instruction to summarize the scores, as I would handle this within the code.\n\n\nHypothesis and predictions\nI hypothesize that SAMRE will not perform better than a baseline model that implements standard best practices for prompt engineering.\nMy predictions are as follows:\n\nSAMRE will perform better than Baseline-Weak.\nBaseline-Strong will perform better than Baseline-Weak.\nBaseline-Strong will perform equal to or better than SAMRE.\n\n\n\nMy implementation of SAMRE and Baseline\nOkay, so with those criticisms out of the way, let’s design evaluators to implement three methods:\n\nSAMRE\nBaseline-Weak: The baseline model used in the paper that does not implement standard best practices for prompt engineering.\nBaseline-Strong: A baseline model that does implement standard best practices for prompt engineering.\n\nBelow is my python implementation of the evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper’s Baseline method “Baseline-Weak”). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering.\nThis will allow me to test my hypothesis and predictions.\n\n\nClick to view the implementation code\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Dict\nimport os\nfrom openai import AsyncOpenAI\nimport logging\nimport re\nfrom contextlib import asynccontextmanager\n\n# Define the prompts\nPROMPTS = {\n# Prompt for advocates to defend their assigned answer, used in SAMRE\n\"defend_prompt\": \"\"\"\nYou’re an eloquent advocate (Advocate {advocate_id}) in a group defending this answer:\n{answer} to the question: {question}. Your opponent group defends: {opponent_answer}.\nCollaborate with your fellow advocates to convince the audience your argument is\nbetter. Use the latest feedback, your opponent’s last argument, and your team’s previous\narguments to improve your case.\nLatest feedback: {feedback}\nOpponent’s last argument: {opponent_argument}\nYour team’s previous arguments: team_arguments\nRespond in under 80 words.\nYour defense:\n\"\"\",\n# Prompt for judge to provide feedback on debate progress, used in SAMRE\n\"judge_prompt\": \"\"\"\nYou’re a fair, impartial judge in a debate on: \"{question}\". Answer 1: \"{answer_1}\".\nAnswer 2: \"{answer_2}\". Your goal is to provide feedback that will help advocate groups\nimprove and differentiate their arguments more clearly.\nCurrent round: {current_round}\nTotal rounds: {total_rounds}\nPrevious scores: {previous_scores}\nDefense for 1st answer: {defense_1}\nDefense for 2nd answer: {defense_2}\nProvide specific, constructive feedback to help each advocate group strengthen their\nunique position. Encourage them to address weaknesses and highlight distinctions. Aim\nfor your feedback to lead to more divergent scores in future rounds.\nGive your feedback in under 50 words:\n\"\"\",\n# Prompt for SAMRE method scoring\n\"score_prompt_samre\": \"\"\"\nYou’re a critical, impartial judge in a high-stakes debate on: \"{question}\". Answer\n1: \"{answer_1}\". Answer 2: \"{answer_2}\". Your goal is to provide detailed, constructive\nfeedback that will push advocates to significantly improve their arguments.\nTotal rounds: {total_rounds}\nPrevious scores: {previous_scores}\nDefense for 1st answer: {defense_1}\nDefense for 2nd answer: {defense_2}\nAnalyze each argument meticulously. Be thorough and unbiased in your assessment of:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nFor each criterion, provide a score on a scale of 1-20 and detailed justification.\nScores should be given as [answer_1_score, answer_2_score] for each criterion.\nYour comprehensive feedback for each advocate (50 words each):\nFeedback for Advocate 1:\nFeedback for Advocate 2:\nSum up the scores and return the final score tuple (score1, score2). Example: (95, 87)\nYour detailed scores and final tally:\n\"\"\",\n# Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper\n\"score_prompt_baseline_weak\": \"\"\"\nYou are a fair, impartial judge scoring a debate on the following question:\nquestion.\nAnswer 1: {answer_1}\nAnswer 2: {answer_2}\nScore each answer on a scale of 1-20 for each of the following criteria:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nProvide scores as [Answer1_score, Answer2_score] for each criterion in a list format,\nthen sum for final scores. Please keep an eye on the slightest difference that should\nmake a difference in the scoring. Don’t overthink!\nRelevance:\nAccuracy:\nDepth:\nClarity:\nLogic and Factuality:\nAddressing opponent’s points:\nFinal Scores (sum of above) as a tuple (example: (18, 9)):\nExplain your scoring, focusing on why one answer is better than the other based on the\ncriteria above. Keep your explanation concise but informative.\nFinally, return the final score tuple (score1, score2) as a tuple (in parentheses).\nExample: (18, 9)\nYour scores and explanation:\n\"\"\",\n# Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering\n\"score_prompt_baseline_strong\": \"\"\"\nYou are a fair, impartial judge scoring a debate on Question.\n\n&lt;Question&gt;\n{question}\n&lt;/Question&gt;\n\nTwo Answers have been given to the Question.\n\n&lt;Answer1&gt;\n{answer_1}\n&lt;/Answer1&gt;\n\n&lt;Answer2&gt;\n{answer_2}\n&lt;/Answer2&gt;\n\nThe Answers are being judged on the following Criteria:\n\n&lt;Criteria&gt;\n&lt;Criterion1&gt;Relevance to their task&lt;/Criterion1&gt;\n&lt;Criterion2&gt;Accuracy and credible sources&lt;/Criterion2&gt;\n&lt;Criterion3&gt;Depth and completeness&lt;/Criterion3&gt;\n&lt;Criterion4&gt;Clarity and logical flow&lt;/Criterion4&gt;\n&lt;Criterion5&gt;Reasoning and factual support&lt;/Criterion5&gt;\n&lt;/Criteria&gt;\n\nFor each Criterion, briefly analyze the performance of \nthe two Answers, then give a score between 1 and 10.\n\nRespond as follows:\n&lt;Criterion1&gt;\n&lt;CriterionName&gt;Relevance to their task&lt;/CriterionName&gt;\n&lt;Analysis&gt;\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n&lt;/Analysis&gt;\n&lt;Scores&gt;\n&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;\n&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;\n&lt;/Scores&gt;\n&lt;/Criterion1&gt;\n&lt;Criterion2&gt;\n&lt;CriterionName&gt;Accuracy and credible sources&lt;/CriterionName&gt;\n&lt;Analysis&gt;\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n&lt;/Analysis&gt;\n&lt;Scores&gt;\n&lt;Answer1Score&gt;[score between 1 and 10]&lt;/Answer1Score&gt;\n&lt;Answer2Score&gt;[score between 1 and 10]&lt;/Answer2Score&gt;\n&lt;/Scores&gt;\n&lt;/Criterion2&gt;\n...\n\"\"\"\n}\n\n@dataclass\nclass Memory:\n    \"\"\"Stores debate history including arguments, scores, and feedback for each round, used in SAMRE\"\"\"\n    arguments: List[Tuple[str, str]] = field(default_factory=list)\n    scores: List[Tuple[float, float]] = field(default_factory=list)\n    feedback: List[str] = field(default_factory=list)\n\nclass ModelEvaluator:\n    @classmethod\n    @asynccontextmanager\n    async def create(cls, mode=\"samre\", model=\"gpt-4o-mini\", logging_level=logging.WARNING):\n        \"\"\"Factory method to create evaluator instance with proper async context management\"\"\"\n        instance = cls(mode=mode, model=model, logging_level=logging_level)\n        instance.client = AsyncOpenAI()\n        try:\n            yield instance\n        finally:\n            await instance.client.close()\n\n    def _setup_logger(self, logging_level):\n        \"\"\"Setup logger with word wrapping.\"\"\"\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging_level)\n        if not logger.handlers:\n            handler = logging.StreamHandler()\n            class WrapFormatter(logging.Formatter):\n                def format(self, record):\n                    import textwrap\n                    message = super().format(record)\n                    return '\\n'.join(textwrap.fill(line, width=80) \n                                for line in message.split('\\n'))\n            \n            formatter = WrapFormatter('%(message)s')\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n        return logger\n\n    def __init__(self, mode=\"samre\", model=\"gpt-4o-mini\", logging_level=logging.WARNING):\n        self.mode = mode\n        self.model = model\n        # Modify to handle both baseline modes\n        self.max_rounds = 1 if mode.startswith(\"baseline\") else 4\n        self.logger = self._setup_logger(logging_level)\n        \n        # Initialize all prompts\n        self.defend_prompt = PROMPTS[\"defend_prompt\"]\n        self.judge_prompt = PROMPTS[\"judge_prompt\"]\n\n\n    async def get_completion(self, prompt: str) -&gt; str:\n        \"\"\"Get a completion from the OpenAI API.\"\"\"\n        if not self.client:\n            raise RuntimeError(\"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'\")\n            \n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"system\", \"content\": prompt}],\n            temperature=0\n        )\n        return response.choices[0].message.content\n\n    def _extract_final_scores(self, score_response: str) -&gt; Tuple[float, float]:\n        \"\"\"Extracts final scores from model response based on evaluation mode\"\"\"\n        if self.mode == \"samre\":\n            # Look for final tuple in format (score1, score2)\n            tuple_pattern = r'\\((\\d+\\.?\\d*),\\s*(\\d+\\.?\\d*)\\)'\n            match = re.search(tuple_pattern, score_response)\n            if match:\n                return (float(match.group(1)), float(match.group(2)))\n            raise ValueError(\"Could not find score tuple in SAMRE response\")\n        \n        elif self.mode == \"baseline_weak\":\n            # Look for final tuple in format (score1, score2)\n            tuple_pattern = r'\\((\\d+\\.?\\d*),\\s*(\\d+\\.?\\d*)\\)'\n            match = re.search(tuple_pattern, score_response)\n            if match:\n                return (float(match.group(1)), float(match.group(2)))\n            raise ValueError(\"Could not find score tuple in weak baseline response\")\n        \n        elif self.mode == \"baseline_strong\":\n            # Use XML parsing for strong baseline\n            score_a_pattern = r'&lt;Answer1Score&gt;\\s*(\\d+\\.?\\d*)\\s*&lt;/Answer1Score&gt;'\n            score_b_pattern = r'&lt;Answer2Score&gt;\\s*(\\d+\\.?\\d*)\\s*&lt;/Answer2Score&gt;'\n            \n            scores_a = [float(match.group(1)) for match in re.finditer(score_a_pattern, score_response)]\n            scores_b = [float(match.group(1)) for match in re.finditer(score_b_pattern, score_response)]\n            \n            if not scores_a or not scores_b:\n                raise ValueError(\"Could not find scores for both candidates\")\n            \n            if len(scores_a) != len(scores_b):\n                raise ValueError(f\"Mismatched number of scores: A={len(scores_a)}, B={len(scores_b)}\")\n            \n            final_score_a = sum(scores_a) / len(scores_a)\n            final_score_b = sum(scores_b) / len(scores_b)\n            \n            return (final_score_a, final_score_b)\n        \n        else:\n            raise ValueError(f\"Unknown mode: {self.mode}\")\n\n    async def evaluate(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -&gt; Dict:\n        \"\"\"Main evaluation entry point that routes to appropriate evaluation method based on mode\"\"\"\n        if not self.client:\n            raise RuntimeError(\"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'\")\n            \n        if self.mode.startswith(\"baseline\"):\n            self.logger.info(f\"\\n=== Starting {self.mode.title()} Evaluation ===\\n\")\n            return await self._evaluate_baseline(question, answer_1, answer_2, num_rounds)\n        else:\n            self.logger.info(\"\\n=== Starting SAMRE Evaluation ===\\n\")\n            return await self._evaluate_samre(question, answer_1, answer_2)\n\n    async def _evaluate_baseline(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -&gt; Dict:\n        \"\"\"Implements baseline evaluation methods (both weak and strong)\"\"\"\n        score_history = []\n        \n        num_rounds = 1 if self.mode == \"baseline_weak\" else num_rounds\n        for _ in range(num_rounds):\n            # Select appropriate prompt based on mode\n            prompt_key = \"score_prompt_\" + self.mode\n            score_prompt = PROMPTS[prompt_key].format(\n                question=question,\n                answer_1=answer_1,\n                answer_2=answer_2\n            )\n            score_response = await self.get_completion(score_prompt)\n            self.logger.info(f\"Score response: {score_response}\")\n            \n            try:\n                round_scores = self._extract_final_scores(score_response)\n                score_history.append(list(round_scores))\n            except Exception as e:\n                self.logger.error(f\"Score parsing error: {e}\")\n                self.logger.error(f\"Raw score response: {score_response}\")\n                score_history.append([10.0, 10.0])\n\n        # Calculate average scores across all rounds\n        avg_scores = [\n            sum(scores[i] for scores in score_history) / len(score_history)\n            for i in range(2)\n        ]\n\n        # Determine winner based on average scores\n        winner = (\n            'model_a' if avg_scores[0] &gt; avg_scores[1]\n            else 'model_b' if avg_scores[0] &lt; avg_scores[1]\n            else 'tie'\n        )\n\n        return {\n            \"winner\": winner,\n            \"average_scores\": [round(score, 2) for score in avg_scores] ,\n            \"rounds\": len(score_history),\n            \"score_history\": score_history,\n            \"full_response\": score_response  # Include the final response for analysis\n        }\n        \n    async def _evaluate_samre(self, question: str, answer_1: str, answer_2: str) -&gt; Dict:\n        \"\"\"Implements SAMRE evaluation with multi-round debate process\n        \n        Flow:\n        1. Get defenses from both advocates\n        2. Judge provides feedback and scores\n        3. Repeat until max rounds or convergence\n        4. Return averaged results\n        \"\"\"\n        local_memory = Memory()\n        \n        self.logger.info(\"\\n=== Starting SAMRE Evaluation ===\\n\")\n        \n        for round_num in range(self.max_rounds):\n            self.logger.info(f\"\\n--- Round {round_num + 1} ---\")\n            \n            scores = await self._run_debate_round(\n                question,\n                answer_1, \n                answer_2, \n                round_num,\n                local_memory\n            )\n            \n            if self._has_scores_converged(round_num, local_memory):\n                self.logger.info(\"\\nScores have converged - ending debate early.\")\n                break\n        \n        return self._prepare_results(local_memory)\n\n    async def defend_answer(self, question: str, answer_1: str, answer_2: str, \n                        advocate_id: int, feedback: str = \"\", \n                        opponent_argument: str = \"\",\n                        team_arguments: List[str] = None) -&gt; str:\n        \"\"\"Get defense from an advocate.\n        \n        Args:\n            question: The question being debated\n            answer_1: First answer in the debate\n            answer_2: Second answer in the debate\n            advocate_id: Which advocate (1 or 2) is defending\n            feedback: Previous feedback from judge\n            opponent_argument: Last argument from opponent\n            team_arguments: List of previous arguments from this advocate's team\n        \"\"\"\n        if team_arguments is None:\n            team_arguments = []\n            \n        # Map answers based on advocate_id\n        answer = answer_1 if advocate_id == 1 else answer_2\n        opponent_answer = answer_2 if advocate_id == 1 else answer_1\n            \n        prompt = self.defend_prompt.format(\n            question=question,\n            advocate_id=advocate_id,\n            answer=answer,  # The answer this advocate is defending\n            opponent_answer=opponent_answer,  # The opposing answer\n            feedback=feedback,\n            opponent_argument=opponent_argument,\n            team_arguments=\"\\n\".join(team_arguments)\n        )\n        return await self.get_completion(prompt)\n\n    async def judge_debate(self, question: str, answer_1: str, answer_2: str,\n                          defense_1: str, defense_2: str, \n                          current_round: int,\n                          memory: Memory) -&gt; Tuple[str, Tuple[float, float]]:\n        \"\"\"Judge the debate between two answers.\"\"\"\n        feedback_prompt = self.judge_prompt.format(\n            question=question,\n            answer_1=answer_1,\n            answer_2=answer_2,\n            current_round=current_round,\n            total_rounds=self.max_rounds,\n            previous_scores=memory.scores,\n            defense_1=defense_1,\n            defense_2=defense_2\n        )\n        feedback = await self.get_completion(feedback_prompt)\n        \n        score_prompt = PROMPTS[\"score_prompt_samre\"].format(\n            question=question,\n            answer_1=answer_1,\n            answer_2=answer_2,\n            defense_1=defense_1,\n            defense_2=defense_2,\n            total_rounds=self.max_rounds,\n            previous_scores=memory.scores,\n            feedback=feedback\n        )\n        score_response = await self.get_completion(score_prompt)    \n        self.logger.info(f\"Score response: {score_response}\")\n        \n        try:\n            scores = self._extract_final_scores(score_response)\n        except Exception as e:\n            self.logger.error(f\"Score parsing error: {e}\")\n            self.logger.error(f\"Raw score response: {score_response}\")\n            scores = (10.0, 10.0)\n        \n        return feedback, scores\n\n    async def _run_debate_round(self, question: str, answer_1: str, answer_2: str, \n                               round_num: int, memory: Memory) -&gt; Tuple[float, float]:\n        \"\"\"Executes single debate round in SAMRE evaluation\"\"\"\n        defenses = await self._get_advocate_defenses(question, answer_1, answer_2, memory)\n        memory.arguments.append(defenses)\n        \n        feedback, scores = await self.judge_debate(\n            question, answer_1, answer_2, defenses[0], defenses[1], round_num + 1, memory\n        )\n        \n        self._store_round_results(feedback, scores, memory)\n        self._display_round_results(defenses, feedback, scores)\n        \n        return scores\n\n    async def _get_advocate_defenses(self, question: str, answer_1: str, answer_2: str,\n                                   memory: Memory) -&gt; Tuple[str, str]:\n        \"\"\"Get defenses from both advocates.\"\"\"\n        defense_1 = await self.defend_answer(\n            question, answer_1, answer_2, 1,\n            feedback=memory.feedback[-1] if memory.feedback else \"\",\n            opponent_argument=memory.arguments[-1][1] if memory.arguments else \"\",\n            team_arguments=[args[0] for args in memory.arguments]\n        )\n        \n        defense_2 = await self.defend_answer(\n            question, answer_1, answer_2, 2,\n            feedback=memory.feedback[-1] if memory.feedback else \"\",\n            opponent_argument=memory.arguments[-1][0] if memory.arguments else \"\",\n            team_arguments=[args[1] for args in memory.arguments]\n        )\n        \n        return (defense_1, defense_2)\n\n    def _store_round_results(self, feedback: str, scores: Tuple[float, float],\n                           memory: Memory) -&gt; None:\n        \"\"\"Store feedback and scores from the round.\"\"\"\n        memory.feedback.append(feedback)\n        memory.scores.append(scores)\n\n    def _display_round_results(self, defenses: Tuple[str, str], \n                             feedback: str, scores: Tuple[float, float]) -&gt; None:\n        \"\"\"Display the results of the current round.\"\"\"\n        self.logger.info(f\"\\nAdvocate 1's defense:\\n{defenses[0]}\")\n        self.logger.info(f\"\\nAdvocate 2's defense:\\n{defenses[1]}\")\n        self.logger.info(f\"\\nJudge's feedback:\\n{feedback}\")\n        self.logger.info(f\"Scores for this round: Answer 1 = {round(scores[0], 2)}, Answer 2 = {round(scores[1], 2)}\")\n\n    def _has_scores_converged(self, round_num: int, memory: Memory) -&gt; bool:\n        \"\"\"Checks if debate scores have converged by comparing last two rounds\"\"\"\n        if round_num &gt; 0:\n            prev_diff = memory.scores[-2][0] - memory.scores[-2][1]\n            curr_diff = memory.scores[-1][0] - memory.scores[-1][1]\n            return (prev_diff * curr_diff) &gt; 0\n        return False\n\n    def _prepare_results(self, memory: Memory) -&gt; Dict:\n        \"\"\"Prepare the final results dictionary.\"\"\"\n        avg_scores = [\n            round(sum(scores[i] for scores in memory.scores) / len(memory.scores), 2)\n            for i in range(2)\n        ]\n        \n        winner = (\n            'model_a' if avg_scores[0] &gt; avg_scores[1]\n            else 'model_b' if avg_scores[0] &lt; avg_scores[1]\n            else 'tie'\n        )\n        \n        return {\n            \"winner\": winner,\n            \"average_scores\": avg_scores,\n            \"rounds\": len(memory.scores),\n            \"score_history\": [[round(s[0], 2), round(s[1], 2)] for s in memory.scores],\n            \"argument_history\": memory.arguments,\n            \"feedback_history\": memory.feedback\n        }\n\n\n\n\nLoad the MT-bench dataset\nNext I will read in the MT-bench dataset from disk and prepare it for evaluation. I will use MtBenchHumanJudgementDataset from Llamahub.\n\n# Commented out since the dataset is already downloaded\n#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data\n\nNext, I will load the dataset into a pandas dataframe and take a random sample of 300 rows.\n\n\nClick to view the code that loads the dataset\nimport json\nimport pandas as pd\nfrom llama_index.core.llama_dataset import LabelledPairwiseEvaluatorDataset\n\ndf = LabelledPairwiseEvaluatorDataset.from_json(\n    \"./data/pairwise_evaluator_dataset.json\"\n).to_pandas()\n\ndf = df[['query', 'answer', 'second_answer', 'answer_by', 'second_answer_by', 'reference_score']]\n\n# Rename as follows: query =&gt; question, answer =&gt; model_a_answer, second_answer =&gt; model_b_answer, answer_by =&gt; model_a, second_answer_by =&gt; model_b, reference_score =&gt; human_winner\ndf.rename(columns={'query': 'question', 'answer': 'model_a_answer', 'second_answer': 'model_b_answer', 'answer_by': 'model_a', 'second_answer_by': 'model_b', 'reference_score': 'human_winner'}, inplace=True)\n\n# Reencode human winner as \"model_a\" if 1, \"model_b\" if 0, and \"tie\" if 0.5\ndf['human_winner'] = df['human_winner'].apply(lambda x: 'model_a' if x == 1 else 'model_b' if x == 0 else 'tie')\n\n# Take a random sample of 300 rows\ndf = df.sample(n=300, random_state=42)\n\ndf.head()\n\n# Take first 180 rows\ndf = df.iloc[:180]\n\n\n\n\nUse methods to evaluate MT-bench dataset\nUsing the MT-bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.\nThe code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.\n\n\nClick to view the code that runs the evaluations\nimport asyncio\nfrom asyncio import Semaphore\nimport logging\nimport os\nimport hashlib\nimport json\nlogging.basicConfig(level=logging.WARNING)\n\nasync def evaluate_conversation_pair(row, evaluators, semaphore, idx, total):\n    \"\"\"Evaluate a single conversation pair with all evaluators\"\"\"\n    async with semaphore:\n        # Add delay between API calls\n        #await asyncio.sleep(1)  # Add small delay between conversations\n        \n        # Generate pair_id from conversation hash\n        pair_id = f\"{row['model_a']}_{row['model_b']}_{hashlib.sha256(str(row['question']).encode()).hexdigest()[:12]}\"\n        checkpoint_file = f'checkpoints/{pair_id}.json'\n        \n        # Return existing checkpoint if available\n        if os.path.exists(checkpoint_file):\n            logging.info(f\"Found existing checkpoint file for {pair_id}\")\n            return json.load(open(checkpoint_file))\n        \n        logging.info(f\"No checkpoint file found for {pair_id}\")\n        result = {\n            'model_a': row['model_a'],\n            'model_b': row['model_b'],\n            'human_winner': row['human_winner'],\n            'pair_id': pair_id\n        }\n        \n        try:\n            # First run SAMRE evaluation with retries\n            for attempt in range(3):  # Try up to 3 times\n                try:\n                    samre_evaluator = evaluators['samre']\n                    samre_result = await samre_evaluator.evaluate(\n                        row['question'], \n                        row['model_a_answer'], \n                        row['model_b_answer']\n                    )\n                    result['samre_winner'] = samre_result['winner']\n                    result.update({f'samre_{k}': samre_result[k] for k in ['average_scores', 'rounds', 'score_history']})\n                    result.update({\n                        'samre_argument_history': samre_result['argument_history'],\n                        'samre_feedback_history': samre_result['feedback_history']\n                    })\n                    break  # If successful, break retry loop\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1  # Exponential backoff\n                        print(f\"Rate limit hit on SAMRE, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:  # Last attempt failed\n                            raise\n                    else:\n                        raise  # Re-raise non-rate-limit errors\n\n            await asyncio.sleep(0.5)  # Add small delay between evaluator calls\n            \n            # Run baseline strong with same number of rounds as SAMRE\n            for attempt in range(3):\n                try:\n                    baseline_strong_evaluator = evaluators['baseline_strong']\n                    baseline_strong_result = await baseline_strong_evaluator.evaluate(\n                        row['question'],\n                        row['model_a_answer'],\n                        row['model_b_answer'],\n                        num_rounds=result['samre_rounds']\n                    )\n                    result['baseline_strong_winner'] = baseline_strong_result['winner']\n                    result.update({f'baseline_strong_{k}': baseline_strong_result[k] \n                                 for k in ['average_scores', 'rounds', 'score_history']})\n                    result['baseline_strong_full_response'] = baseline_strong_result['full_response']\n                    break\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1\n                        print(f\"Rate limit hit on baseline strong, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:\n                            raise\n                    else:\n                        raise\n\n            await asyncio.sleep(0.5)  # Add small delay between evaluator calls\n\n            # Run baseline weak with 1 round\n            for attempt in range(3):\n                try:\n                    baseline_weak_evaluator = evaluators['baseline_weak']\n                    baseline_weak_result = await baseline_weak_evaluator.evaluate(\n                        row['question'],\n                        row['model_a_answer'],\n                        row['model_b_answer'],\n                        num_rounds=1\n                    )\n                    result['baseline_weak_winner'] = baseline_weak_result['winner']\n                    result.update({f'baseline_weak_{k}': baseline_weak_result[k] \n                                 for k in ['average_scores', 'rounds', 'score_history']})\n                    result['baseline_weak_full_response'] = baseline_weak_result['full_response']\n                    break\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1\n                        print(f\"Rate limit hit on baseline weak, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:\n                            raise\n                    else:\n                        raise\n                        \n        except Exception as e:\n            print(f\"Error evaluating row {idx}: {str(e)}\")\n            result['samre_winner'] = None\n            result['baseline_strong_winner'] = None\n            result['baseline_weak_winner'] = None\n            result['error'] = str(e)\n        \n        # Save checkpoint after each evaluation\n        os.makedirs('checkpoints', exist_ok=True)\n        json.dump(result, open(checkpoint_file, 'w'))\n        \n        if (idx + 1) % 10 == 0:\n            print(f\"Processed {idx + 1}/{total} conversations\")\n            \n        return result\n\nasync def evaluate_conversations_async(df, evaluators, semaphore_limit=3):\n    \"\"\"Evaluate conversations asynchronously\"\"\"\n    # Reduce semaphore limit\n    semaphore_limit = 1  # Process one at a time to avoid rate limits\n    \n    # Process in smaller batches\n    batch_size = 10\n    results = []\n    \n    for i in range(0, len(df), batch_size):\n        batch = df.iloc[i:i+batch_size]\n        tasks = [\n            evaluate_conversation_pair(row[1], evaluators, Semaphore(semaphore_limit), idx, len(df))\n            for idx, row in enumerate(batch.iterrows(), start=i)\n        ]\n        batch_results = await asyncio.gather(*tasks)\n        results.extend(batch_results)\n        \n        # Add delay between batches\n        if i + batch_size &lt; len(df):\n            print(f\"Completed batch {i//batch_size + 1}, waiting before next batch...\")\n            #await asyncio.sleep(5)  # 5 second delay between batches\n            \n    return pd.DataFrame(results)\n\nasync def main():\n    async with ModelEvaluator.create(mode=\"samre\") as samre_evaluator, \\\n               ModelEvaluator.create(mode=\"baseline_strong\") as baseline_strong_evaluator, \\\n               ModelEvaluator.create(mode=\"baseline_weak\") as baseline_weak_evaluator:\n        return await evaluate_conversations_async(\n            df,\n            {\n                'samre': samre_evaluator, \n                'baseline_strong': baseline_strong_evaluator,\n                'baseline_weak': baseline_weak_evaluator\n            },\n            semaphore_limit=1\n        )\n\n# Run evaluation with checkpoint recovery\ntry:\n    eval_df = await main()\nexcept Exception as e:\n    print(f\"Error during evaluation: {str(e)}\\nRecovering from checkpoints...\")\n    eval_df = pd.DataFrame([json.load(open(f'checkpoints/{f}')) \n                           for f in os.listdir('checkpoints') \n                           if f.endswith('.json')])\nfinally:\n    eval_df.to_csv('eval_df.csv', index=False)\n    eval_df.head()\n\n# Drop rows with any null values on the model winner columns\neval_df = eval_df.dropna(subset=['baseline_strong_winner', 'baseline_weak_winner', 'samre_winner'])\n\n\nCompleted batch 1, waiting before next batch...\nCompleted batch 2, waiting before next batch...\nCompleted batch 3, waiting before next batch...\nCompleted batch 4, waiting before next batch...\nCompleted batch 5, waiting before next batch...\nCompleted batch 6, waiting before next batch...\nCompleted batch 7, waiting before next batch...\nCompleted batch 8, waiting before next batch...\nCompleted batch 9, waiting before next batch...\nCompleted batch 10, waiting before next batch...\nCompleted batch 11, waiting before next batch...\nCompleted batch 12, waiting before next batch...\nCompleted batch 13, waiting before next batch...\nCompleted batch 14, waiting before next batch...\nCompleted batch 15, waiting before next batch...\nCompleted batch 16, waiting before next batch...\nCompleted batch 17, waiting before next batch...\n\n\n\n\nResults\nNow that the evaluation is complete, I will evaluate the performance of each of the three methods by looking at how well each method agreed with the human judgments. I’ll use Krippendorff’s alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).\n\n\nClick to view the code that calculates agreement\nfrom krippendorff import alpha\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef calculate_agreement(df, rater1_col, rater2_col):\n    \"\"\"\n    Calculate Krippendorff's alpha between two raters.\n    \n    Args:\n        df: DataFrame containing the ratings\n        rater1_col: Name of first rater's column\n        rater2_col: Name of second rater's column\n    \n    Returns:\n        float: Krippendorff's alpha score\n    \"\"\"\n    # Create label encoder\n    le = LabelEncoder()\n    \n    # Combine all unique values from both columns\n    all_values = pd.concat([df[rater1_col], df[rater2_col]]).unique()\n    le.fit(all_values)\n    \n    # Transform the ratings to numeric values\n    ratings1 = le.transform(df[rater1_col].fillna('missing'))\n    ratings2 = le.transform(df[rater2_col].fillna('missing'))\n    \n    # Reshape data for krippendorff alpha calculation\n    # Each row represents one item, each column represents one rater\n    reliability_data = np.vstack([ratings1, ratings2])\n    \n    return alpha(reliability_data=reliability_data, level_of_measurement='nominal')\n\n# Calculate agreement scores for all methods\nhuman_baseline_strong_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_strong_winner')\nhuman_baseline_weak_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_weak_winner')\nhuman_samre_agreement = calculate_agreement(eval_df, 'human_winner', 'samre_winner')\n\n# Create a DataFrame with the agreement scores\nagreement_df = pd.DataFrame({\n    'Evaluator Pair': ['Human vs. Baseline-Strong', 'Human vs. Baseline-Weak', 'Human vs. SAMRE'],\n    'Krippendorff Alpha': [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]\n})\n\n# Round the scores to 3 decimal places\nagreement_df['Krippendorff Alpha'] = agreement_df['Krippendorff Alpha'].round(3)\n\n# Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong\nbaseline_strong_baseline_weak_diff = (human_baseline_strong_agreement - human_baseline_weak_agreement) / human_baseline_strong_agreement\nbaseline_strong_samre_diff = (human_baseline_strong_agreement - human_samre_agreement) / human_baseline_strong_agreement\nsamre_baseline_weak_diff = (human_samre_agreement - human_baseline_weak_agreement) / human_samre_agreement\n\n# Print raw values\nprint(agreement_df)\n\n# Display the percent difference\nprint(\"\\nKrippendorff Alpha Improvements:\")\nprint(f\"SAMRE vs. Baseline-Weak: {samre_baseline_weak_diff:.0%}\")\nprint(f\"Baseline-Strong vs. Baseline-Weak: {baseline_strong_baseline_weak_diff:.0%}\")\nprint(f\"Baseline-Strong vs. SAMRE: {baseline_strong_samre_diff:.0%}\")\n\n\n              Evaluator Pair  Krippendorff Alpha\n0  Human vs. Baseline-Strong               0.391\n1    Human vs. Baseline-Weak               0.252\n2            Human vs. SAMRE               0.307\n\nKrippendorff Alpha Improvements:\nSAMRE vs. Baseline-Weak: 18%\nBaseline-Strong vs. Baseline-Weak: 36%\nBaseline-Strong vs. SAMRE: 21%\n\n\nAlthough none of the methods yielded particularly strong agreement with the human judges, we can observe a few things: 1. As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.307 vs. 0.252, an increase of ~18%). 2. Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.391 vs. 0.252, an increase of ~36%). 3. Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.391 vs. 0.252, an increase of ~21%)!\nWe can also measure performance in terms of binary classification accuracy, using Matthews Correlation Coefficient (MCC) as the metric, while re-encoding the “winner” columns to indicate whether model_a was selected as better (1) or not better (0) in each case.\n\n\nClick to view the code that calculates Matthews Correlation Coefficient (MCC)\n# Encode winner as binary\ndef encode_winner_as_binary(winner):\n    return 1 if winner == 'model_a' else 0\n\n# Create binary columns for each evaluator\neval_df['human_model_a_better'] = eval_df['human_winner'].apply(encode_winner_as_binary)\neval_df['baseline_strong_model_a_better'] = eval_df['baseline_strong_winner'].apply(encode_winner_as_binary)\neval_df['baseline_weak_model_a_better'] = eval_df['baseline_weak_winner'].apply(encode_winner_as_binary)\neval_df['samre_model_a_better'] = eval_df['samre_winner'].apply(encode_winner_as_binary)\n\nfrom sklearn.metrics import matthews_corrcoef\n\n# Calculate MCC for each method\nmetrics_df = pd.DataFrame({\n    'Method': ['Baseline-Strong', 'Baseline-Weak', 'SAMRE'],\n    'MCC': [\n        matthews_corrcoef(\n            eval_df['human_model_a_better'], \n            eval_df['baseline_strong_model_a_better']\n        ),\n        matthews_corrcoef(\n            eval_df['human_model_a_better'], \n            eval_df['baseline_weak_model_a_better']\n        ),\n        matthews_corrcoef(\n            eval_df['human_model_a_better'], \n            eval_df['samre_model_a_better']\n        )\n    ]\n})\n\n# Round the scores to 3 decimal places\nmetrics_df['MCC'] = metrics_df['MCC'].round(3)\n\n# Calculate the percent differences\ndef calc_percent_diff(new, old):\n    return (new - old) / old * 100\n\n# MCC differences\nsamre_baseline_weak_mcc_diff = calc_percent_diff(\n    metrics_df.loc[metrics_df['Method'] == 'SAMRE', 'MCC'].iloc[0],\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Weak', 'MCC'].iloc[0]\n)\nbaseline_strong_baseline_weak_mcc_diff = calc_percent_diff(\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Strong', 'MCC'].iloc[0],\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Weak', 'MCC'].iloc[0]\n)\nbaseline_strong_samre_mcc_diff = calc_percent_diff(\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Strong', 'MCC'].iloc[0],\n    metrics_df.loc[metrics_df['Method'] == 'SAMRE', 'MCC'].iloc[0]\n)\n\n# Print raw values\nprint(metrics_df)\n\nprint(\"\\nMCC Improvements:\")\nprint(f\"SAMRE vs. Baseline-Weak: {samre_baseline_weak_mcc_diff:.0f}%\")\nprint(f\"Baseline-Strong vs. Baseline-Weak: {baseline_strong_baseline_weak_mcc_diff:.1f}%\")\nprint(f\"Baseline-Strong vs. SAMRE: {baseline_strong_samre_mcc_diff:.0f}%\")\n\n\n            Method    MCC\n0  Baseline-Strong  0.464\n1    Baseline-Weak  0.331\n2            SAMRE  0.331\n\nMCC Improvements:\nSAMRE vs. Baseline-Weak: 0%\nBaseline-Strong vs. Baseline-Weak: 40.2%\nBaseline-Strong vs. SAMRE: 40%\n\n\nLooking at MCC values, we observe the following:\n\nSAMRE did not perform better than Baseline-Weak (MCCs = 0.331 in both cases).\nBaseline-Strong performed better than Baseline-Weak (0.464 vs. 0.331, an increase of ~40%).\nBaseline-Strong did not perform better than SAMRE (0.464 vs. 0.331, an increase of ~40%).\n\nWhy does this metric disagree with the Krippendorff alpha results on the SAMRE vs. Baseline-Weak comparison? I would guess this is due to how ties were resolved when encoding the winner as binary. Also note that the same MCC values for SAMRE and Baseline-Weak is not an error. We can see that the confusion matrices were different.\n\n\nClick to view the code that generates confusion matrices\nfrom sklearn.metrics import confusion_matrix\n\nprint(\"\\nBaseline-Weak Confusion Matrix:\")\nprint(confusion_matrix(\n    eval_df['human_model_a_better'], \n    eval_df['baseline_weak_model_a_better']\n))\n\nprint(\"\\nSAMRE Confusion Matrix:\")\nprint(confusion_matrix(\n    eval_df['human_model_a_better'], \n    eval_df['samre_model_a_better']\n))\n\n\n\nBaseline-Weak Confusion Matrix:\n[[72 42]\n [16 41]]\n\nSAMRE Confusion Matrix:\n[[70 44]\n [15 42]]\n\n\nThus, across both of these measures of performance, we see that SAMRE did not perform better than a baseline that is designed with best practices.\n\n\nConclusion\nIn this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline."
  },
  {
    "objectID": "blog/2023/12/04/index.html",
    "href": "blog/2023/12/04/index.html",
    "title": "Tackling the GSM8K (grade school math) with GPT-3.5 and self-consistency prompting",
    "section": "",
    "text": "I’ve been interested in AI tutoring applications for education for a long time, and with today’s Large Language Models like GPT, which have been shown to perform extremely well on standardized tests like the SAT and GRE, it seems that building these applications is now achievable. In fact, some companies have already started building these applications, like Khan Academy’s own Khanmigo.\nThe current generation of LLMs perform pretty well at many tasks, and research (like this paper on chain-of-thought prompting) has shown that there are a variety of “prompt engineering” techniques that can be used to boost performance even further. Basically, prompt engineering refers to the process of writing effective instructions for the model, as well as the process of breaking a complex task into sub-tasks and “chaining” prompts together.\nIn this post, I use the Self-Consistency prompt engineering strategy to improve the performance of a GPT-3.5 based model tasked with solving problems from the GSM8K (grade school math) benchmark dataset. Conceptually, the Self-Consistency strategy involves asking the LLM to follow multiple reasoning paths to generate multiple answers, and then taking a majority vote of its answers.\nI explore implementing the Self-Consistency strategy first by using a single prompt that instructs the model to generate multiple reasoning paths and answers, followed by identifying the majority vote of its own answers – all within a single response. In addition, I explore an implementation in which the LLM is asked to generate an answer multiple times using independent requests to the API, followed by using a simple frequency count to obtain the majority vote of its answers.\nUsing the Self-Consistency strategy, model performance was increased from 75% correct answers at baseline to 93% with the multiple-attempts implementation. Overall, this analysis suggests that Self-Consistency is an effective strategy to improve LLM performance on cognitive tasks like answering grade school math questions.\n(Note: This blog post uses a mix of python and R code.)"
  },
  {
    "objectID": "blog/2023/12/04/index.html#validity-checks",
    "href": "blog/2023/12/04/index.html#validity-checks",
    "title": "Tackling the GSM8K (grade school math) with GPT-3.5 and self-consistency prompting",
    "section": "Validity checks",
    "text": "Validity checks\nCheck that there are 1200 total records.\n\n\n\nR\n\nnrow(df)\n\n\n[1] 1200\n\n\nCheck that each of the 12 levels has 100 questions each.\n\n\n\nR\n\ndf %&gt;%\n  group_by(n_experts, n_attempts) %&gt;%\n  count() %&gt;%\n  filter(n == 100)\n\n\n# A tibble: 12 × 3\n# Groups:   n_experts, n_attempts [12]\n   n_experts n_attempts     n\n   &lt;fct&gt;     &lt;fct&gt;      &lt;int&gt;\n 1 1         1            100\n 2 1         3            100\n 3 1         5            100\n 4 1         10           100\n 5 3         1            100\n 6 3         3            100\n 7 3         5            100\n 8 3         10           100\n 9 5         1            100\n10 5         3            100\n11 5         5            100\n12 5         10           100"
  },
  {
    "objectID": "blog/2023/12/04/index.html#baseline-performance",
    "href": "blog/2023/12/04/index.html#baseline-performance",
    "title": "Tackling the GSM8K (grade school math) with GPT-3.5 and self-consistency prompting",
    "section": "Baseline performance",
    "text": "Baseline performance\nAs a baseline, I’ll use performance when the model is given 1 attempt using 1 expert.\nBaseline performance is 75%.\n\n\n\nR\n\ndf %&gt;%\n  filter(n_experts == 1, n_attempts == 1) %&gt;%\n  summarize(pct_correct = mean(is_correct)*100)\n\n\n  pct_correct\n1          75"
  },
  {
    "objectID": "blog/2023/12/04/index.html#single-prompt-imagined-experts",
    "href": "blog/2023/12/04/index.html#single-prompt-imagined-experts",
    "title": "Tackling the GSM8K (grade school math) with GPT-3.5 and self-consistency prompting",
    "section": "Single-prompt “imagined experts”",
    "text": "Single-prompt “imagined experts”\nIf the single-prompt “imagined experts” implementation improves performance, then I would expect that prompting the model to imagine 3 (or 5) experts would perform better than asking it to imagine only 1 expert. Contrary to this expectation, I see no improvement.\n\n\n\nR\n\ndf %&gt;%\n  group_by(n_experts) %&gt;%\n  summarize(pct_correct = mean(is_correct)*100) %&gt;%\n  ggplot(aes(x = n_experts, y = pct_correct, group = 1)) +\n    geom_line() +\n    labs(x=\"# of Imagined Experts\",\n         y='% Correct Answers',\n         title=\"Imagined experts had minimal benefits when averaged over\\ndifferent numbers of attempts\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_experts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)\n\n\n\n\n\n\n\n\nHowever, it’s possible that the improvement is obscured by the fact that I’m taking the average across number of attempts. It’s possible that the two implementations are redundant, and that the single-prompt “imagined experts” implementation is only beneficial when the model is given a single attempt.\nBelow we can see that if the model is given only 1 attempt, the prompt with 3 experts performed better than the prompt with only 1 imagined expert. There’s also a non-linear pattern, which may suggest that asking the model to imagine too many experts with different reasoning leads to some experts engaging sub-optimal reasoning, which then leads to poorer performance.\n\n\n\nR\n\ndf %&gt;%\n  filter(n_attempts == 1) %&gt;%\n  group_by(n_experts) %&gt;%\n  summarize(pct_correct = mean(is_correct)*100) %&gt;%\n  ggplot(aes(x = n_experts, y = pct_correct, group = 1)) +\n    geom_line() +\n    labs(x=\"# of Imagined Experts\",\n         y='% Correct Answers',\n         title=\"In the absence of multiple attempts, imagining 3 experts was best\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_experts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)"
  },
  {
    "objectID": "blog/2023/12/04/index.html#multiple-attempts",
    "href": "blog/2023/12/04/index.html#multiple-attempts",
    "title": "Tackling the GSM8K (grade school math) with GPT-3.5 and self-consistency prompting",
    "section": "Multiple attempts",
    "text": "Multiple attempts\nI expect that when I take the most frequent answer across attempts, performance would improve with the number of attempts (at least, up to a certain point). This would support the multiple-attempts implementation. Indeed this does seem to be the case: Model performance increased linearly with number of attempts, with 10 attempts performing better than 5, 5 better than 3, and 3 better than 1.\n\n\n\nR\n\ndf %&gt;%\n  group_by(n_attempts) %&gt;%\n  summarize(pct_correct = mean(is_correct)*100) %&gt;%\n  ggplot(aes(x = n_attempts, y = pct_correct, group = 1)) +\n    geom_line() +\n    labs(x=\"# of Attempts\",\n         y='% Correct Answers',\n         title=\"More attempts means more accuracy\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_attempts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)\n\n\n\n\n\n\n\n\nThe strongest model was one that was given 10 attempts, with only 1 imagined expert per attempt, which achieved 93% correct answers. This suggests that giving the model multiple attempts, while generating only 1 answer per attempt may be the best implementation of the Self-Consistency strategy.\n\n\n\nR\n\ndf %&gt;%\n  group_by(n_attempts, n_experts) %&gt;%\n  summarize(pct_correct = mean(is_correct)*100, .groups = 'drop') %&gt;%\n  ggplot(aes(x = n_attempts, y = pct_correct, group = n_experts, color = n_experts)) +\n    geom_line() +\n    labs(x=\"# of Attempts\",\n         y='% Correct Answers',\n         color=\"# of Imagined Experts\",\n         title=\"The best implementation: 10 attempts with 1 imagined expert\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_attempts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)\n\n\n\n\n\n\n\n\nOverall, this analysis provides strong evidence in favor of the multiple-attempts implementation of Self-Consistency, and weaker evidence in favor of the single-prompt “imagined experts” implementation. The best implementation involved asking the LLM to generate only 1 answer per attempt, while giving it 10 attempts in total. Using this implementation of the strategy, performance was increased from 75% correct answers at baseline to 93% correct answers."
  },
  {
    "objectID": "blog/2023/09/19/index.html",
    "href": "blog/2023/09/19/index.html",
    "title": "Encoding high cardinality features with “embeddings”",
    "section": "",
    "text": "Embedding is categorical encoding method that that uses deep learning to represent categorical features as vectors. It’s particularly useful for categorical features with many levels, since it can be used to project high-dimensional features into low-dimensional space.\nIn this blog post, I’ll show how ML models with embedding encoding outperform models with other common categorical encoding methods (frequency, label, one-hot, and target). For this demonstration, I’ll be using the dataset from Kaggle’s Playground Series S3E22: Predict Health Outcomes of Horses."
  },
  {
    "objectID": "blog/2023/09/19/index.html#one-hot-encoding",
    "href": "blog/2023/09/19/index.html#one-hot-encoding",
    "title": "Encoding high cardinality features with “embeddings”",
    "section": "One-hot encoding",
    "text": "One-hot encoding\n\nrecipe_1hot_with_novel &lt;- \n  recipe(outcome ~ ., data = train %&gt;% select(-id)) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_novel(all_nominal_predictors(), new_level = \"NA\") %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot=T)\n\nThe first – and probably most popular – type of categorical encoding is one-hot encoding. One-hot encoding transforms a single categorical variable with N levels into binary variables encoding each of the N levels.\nFor example, age is a categorical variable with 2 levels.\n\nlevels(train$age)\n\n[1] \"adult\" \"young\"\n\nlength(levels(train$age))\n\n[1] 2\n\n\nWhen age is one-hot encoded, a column is created for each level to encode the value (e.g., if the original value was adult, then the age_adult column gets a 1 and the other columns get a 0). And since I’ve also included a step to encode novel levels as NA, there is also a third column for that.\n\nrecipe_1hot_with_novel %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL) %&gt;%\n  select(starts_with('age')) %&gt;%\n  head(3)\n\n# A tibble: 3 × 3\n  age_adult age_young age_NA.\n      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1         1         0       0\n2         1         0       0\n3         1         0       0"
  },
  {
    "objectID": "blog/2023/09/19/index.html#label-encoding",
    "href": "blog/2023/09/19/index.html#label-encoding",
    "title": "Encoding high cardinality features with “embeddings”",
    "section": "Label encoding",
    "text": "Label encoding\n\nrecipe_label &lt;- \n  recipe(outcome ~ ., data = train %&gt;% select(-id)) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_integer(all_nominal_predictors())\n\nWith label encoding, each level of the categorical variable is given an (arbitrary) number. In the tidymodels framework, step_integer works like scikit’s LabelEncoder, and encodes new values as zero. Here we see that one level of age was encoded as “1” and the other was encoded as “2”.\n\nrecipe_label %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL) %&gt;%\n  select(age) %&gt;%\n  distinct\n\n# A tibble: 2 × 1\n    age\n  &lt;int&gt;\n1     1\n2     2"
  },
  {
    "objectID": "blog/2023/09/19/index.html#frequency-encoding",
    "href": "blog/2023/09/19/index.html#frequency-encoding",
    "title": "Encoding high cardinality features with “embeddings”",
    "section": "Frequency encoding",
    "text": "Frequency encoding\n\nfreq_encoding &lt;- encodeR::frequency_encoder(\n  X_train = train,\n  X_test = test, \n  cat_columns = colnames(df %&gt;% select(where(is.factor), -outcome))\n)\n\ntrain_freq &lt;- freq_encoding$train\ntest_freq &lt;- freq_encoding$test\n\nWith frequency encoding, levels of the categorical variable are replaced with their frequency. Here, we can see how the levels of age have been replaced with their frequency in the training set. (When this is applied to the test set, these same training frequencies will be used.)\n\ntrain_freq %&gt;%\n  select(age) %&gt;%\n  distinct()\n\n# A tibble: 2 × 1\n     age\n   &lt;dbl&gt;\n1 0.937 \n2 0.0626\n\n\n\nrecipe_freq &lt;- \n  recipe(outcome ~ ., data = train_freq %&gt;% select(-id)) %&gt;%\n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "blog/2023/09/19/index.html#target-encoding",
    "href": "blog/2023/09/19/index.html#target-encoding",
    "title": "Encoding high cardinality features with “embeddings”",
    "section": "Target encoding",
    "text": "Target encoding\nFor target encoding (also called “effect encoding” or “likelihood encoding”), I’ll be using the h2o package because it supports multi-class targets. (The embed package can also do target encoding and integrates better with a tidymodels workflow, but at the moment it only supports binary targets.)\nUsing h2o requires some additional setup.\n\n# Convert to h2o format\ndf_h2o &lt;- as.h2o(df)\n\n# Split the dataset into train and test\nsplits_h2o &lt;- h2o.splitFrame(data = df_h2o, ratios = .8, seed = 42)\ntrain_h2o &lt;- splits_h2o[[1]]\ntest_h2o &lt;- splits_h2o[[2]]\n\nWith target encoding, the levels of the categorical variable are replaced with their mean value on the target. For example, if the level “young” was associated with a mean target value of 0.75, then this is the value with which that level would be replaced.\nBecause the outcome is being used for encoding, care needs to be taken when using this method to avoid leakage and overfitting. In this case, I’ll use the “Leave One Out” method: for each row, the mean is calculated over all rows excluding that row.\n\n# Choose which columns to encode\nencode_columns &lt;- colnames(df %&gt;% select(where(is.factor), -outcome)) # All categorical variables\n\n# Train a TE model\nte_model &lt;- h2o.targetencoder(x = encode_columns,\n                              y = 'outcome', \n                              keep_original_categorical_columns=T,\n                              training_frame = train_h2o,\n                              noise=0,\n                              seed=100,\n                              blending = T, # Blending helps with levels that are more rare\n                              data_leakage_handling = \"LeaveOneOut\")\n\n# New target encoded training and test datasets\ntrain_te &lt;- h2o.transform(te_model, train_h2o)\ntest_te &lt;- h2o.transform(te_model, test_h2o)\n\nHere we can see how the target encoding strategy encoded age: Two new variables are created, age_euthanized_te and age_lived_te. The encoded values represent the proportion of cases that were euthanized, or lived, for each level of age. (Note: The “died” level of the outcome variable is missing. This is because if we know the proportion that were euthanized and lived, we also know the proportion that died.)\n\ntrain_te %&gt;%\n  as.data.frame() %&gt;%\n  select(starts_with('age') & ends_with('te'), age) %&gt;%\n  distinct()\n\n  age_euthanized_te age_lived_te   age\n1        0.20937841    0.4776445 adult\n2        0.06005923    0.2157985 young\n\n\n\n# Drop the unencoded columns\ntrain_te %&gt;% \n  as.data.frame() %&gt;%\n  select(-all_of(encode_columns)) %&gt;%\n  as.h2o() -&gt; train_te\ntest_te %&gt;% \n  as.data.frame() %&gt;%\n  select(-all_of(encode_columns)) %&gt;%\n  as.h2o() -&gt; test_te\n\n\n# Create a recipe to use later\nrecipe_target &lt;- \n  recipe(outcome ~ ., data = train_te %&gt;% as.data.frame() %&gt;% select(-id)) %&gt;%\n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "blog/2023/09/19/index.html#model-fit-function",
    "href": "blog/2023/09/19/index.html#model-fit-function",
    "title": "Encoding high cardinality features with “embeddings”",
    "section": "Model fit function",
    "text": "Model fit function\nWith 3 models and 5 categorical encodings, I’ll need to fit 15 models. To streamline this process, I’ll define two functions:\n\n\nfit_model(): Given training and test datasets, a workflow containing a recipe for the categorical encoding, a model type, and an encoding type, this function will evaluate the model in-sample using cross-validation, then evaluate it out-of-sample, and then return a dataframe containing the results\n\nfit_encodings(): Given a model and model type, this function will generate recipes for each of the 5 categorical encodings, fit the 5 encodings using the model, and then return a dataframe with the results\n\n\nfit_model &lt;- function(train, test, workflow, model_type, encoding_type){\n  \n  set.seed(42)\n  folds &lt;- vfold_cv(train, v = 5)\n  \n  resampled_fit &lt;- \n    workflow %&gt;% \n    fit_resamples(folds,\n                  metrics = metric_set(f_meas))\n  \n  # Get in-sample F1\n  (resampled_fit %&gt;%\n    collect_metrics())$mean -&gt; train_perf\n  \n  # Get out-of-sample F1\n  fit &lt;- \n    workflow %&gt;%\n    fit(train)\n  \n  test$pred &lt;- predict(fit, test)$.pred_class\n  (f_meas(test, outcome, pred, estimator='micro'))$.estimate -&gt; test_perf\n  \n  # Combine in-sample and out-of-sample into a dataframe\n  df_perf &lt;- data.frame(model_type = model_type,\n                        encoding_type = encoding_type,\n                        train_perf = train_perf,\n                        test_perf = test_perf)\n  return(df_perf)\n}\n\n\n# Given a model, run it across the 4 encodings and return a dataframe that summarizes the results\nfit_encodings &lt;- function(model, model_type){\n  \n  set.seed(42)\n  tensorflow::set_random_seed(42)\n  \n  # One-hot encoded model\n  wflow_1hot &lt;- \n    workflow() %&gt;% \n    add_model(model) %&gt;%\n    add_recipe(recipe_1hot_with_novel)\n  \n  fit_model(train %&gt;% select(-id), \n            test %&gt;% select(-id), \n            wflow_1hot, \n            model_type,\n            'onehot') -&gt; onehot_model_results\n  \n  # Label encoded model\n  wflow_label &lt;- \n    workflow() %&gt;% \n    add_model(model) %&gt;%\n    add_recipe(recipe_label)\n  \n  fit_model(train %&gt;% select(-id), \n            test %&gt;% select(-id), \n            wflow_label, \n            model_type,\n            'label') -&gt; label_model_results\n  \n  # Frequency encoded model\n  wflow_freq &lt;- \n    workflow() %&gt;% \n    add_model(model) %&gt;%\n    add_recipe(recipe_freq)\n  \n  fit_model(train_freq %&gt;% select(-id), \n            test_freq %&gt;% select(-id), \n            wflow_freq, \n            model_type,\n            'frequency') -&gt; freq_model_results\n  \n  # Target encoded model\n  wflow_target &lt;- \n    workflow() %&gt;% \n    add_model(model) %&gt;%\n    add_recipe(recipe_target)\n  \n  fit_model(train_te %&gt;% as.data.frame() %&gt;% select(-id), \n            test_te %&gt;% as.data.frame() %&gt;% select(-id), \n            wflow_target, \n            model_type,\n            'target') -&gt; target_model_results\n  \n  \n  # Embedding encoded model\n  wflow_embedding &lt;- \n    workflow() %&gt;% \n    add_model(model) %&gt;%\n    add_recipe(recipe_embedding)\n  \n  fit_model(train %&gt;% as.data.frame() %&gt;% select(-id), \n            test %&gt;% as.data.frame() %&gt;% select(-id), \n            wflow_embedding, \n            model_type,\n            'embedding') -&gt; embedding_model_results\n  \n  # Compile results into a dataframe\n  onehot_model_results %&gt;%\n    bind_rows(label_model_results) %&gt;%\n    bind_rows(freq_model_results) %&gt;%\n    bind_rows(target_model_results) %&gt;%\n    bind_rows(embedding_model_results) -&gt; results\n  \n  results\n}\n\nI’ll run each of the models using the fit_encodings() and fit_model() functions that I just defined.\n\nfit_encodings(multinom_mod, 'multinomial logistic') -&gt; multinom_results\nfit_encodings(ranger_mod, 'random forest') -&gt; rf_results\nfit_encodings(xgboost_mod, 'xgboost') -&gt; xgb_results"
  },
  {
    "objectID": "blog/2023/08/31/index.html",
    "href": "blog/2023/08/31/index.html",
    "title": "Joining messy dataframes using fuzzy joining, string cleaning, and column binding",
    "section": "",
    "text": "From the dataset description:\nThis last point is what I’ll be focusing on in this post: The challenge of joining two datasets together that don’t line up for a clean join.\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(gt)\nfair_use_cases &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-29/fair_use_cases.csv')\nfair_use_findings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-29/fair_use_findings.csv')"
  },
  {
    "objectID": "blog/2023/08/31/index.html#validation-checks",
    "href": "blog/2023/08/31/index.html#validation-checks",
    "title": "Joining messy dataframes using fuzzy joining, string cleaning, and column binding",
    "section": "Validation checks",
    "text": "Validation checks\nWhen inspecting the final result I expect to see 251 rows; none of which should be duplicates, and I do.\n\nfinal_join %&gt;%\n  summarize(\n    n_rows = n(),\n    n_distinct_rows = n_distinct(case, title)\n  )\n\n# A tibble: 1 × 2\n  n_rows n_distinct_rows\n   &lt;int&gt;           &lt;int&gt;\n1    251             251\n\n\nAs another check, I can look at the same records that were misaligned with the first approach. I can see they are now aligned.\n\nfinal_join %&gt;% \n  select(case, title, case_number) %&gt;%\n  head(111) %&gt;%\n  tail(3) %&gt;%\n  gt() %&gt;%\n  opt_interactive() %&gt;%\n  tab_header(title = \"Rows that were previously misaligned with bind_cols() are now aligned\") %&gt;%\n  tab_options(table.background.color = '#f1f3f5',\n              ihtml.page_size_default = 3)\n\n\n\n\nRows that were previously misaligned with bind_cols() are now aligned\n\n\n\n\n\n\n\nAnd here is the full table.\n\nfinal_join %&gt;% \n  select(case, title, case_number) %&gt;%\n  gt() %&gt;%\n  opt_interactive() %&gt;%\n  tab_header(title = \"Final results table (case, title, case_number)\") %&gt;%\n  tab_options(table.background.color = '#f1f3f5',\n              ihtml.page_size_default = 3)\n\n\n\n\nFinal results table (case, title, case_number)\n\n\n\n\n\n\n\nIt looks like the matches are now correct."
  },
  {
    "objectID": "blog/2023/08/19/index.html",
    "href": "blog/2023/08/19/index.html",
    "title": "Building a prediction model to detect spam email",
    "section": "",
    "text": "Getting back into the swing of things. This is my first blog post in more than 3 years!\nFor this post, I’ll be using the Week 33 Tidy Tuesday dataset. This one is all about spam email.\nFrom the dataset description:\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(tidymodels)\nlibrary(usemodels)\nlibrary(future)\nlibrary(rpart)\nlibrary(rpart.plot)\nknitr::opts_chunk$set(echo = TRUE, fig.width = 4.5, fig.height = 2.5)"
  },
  {
    "objectID": "blog/2023/08/19/index.html#average-values-by-spam",
    "href": "blog/2023/08/19/index.html#average-values-by-spam",
    "title": "Building a prediction model to detect spam email",
    "section": "Average values, by spam",
    "text": "Average values, by spam\nLet’s start by looking at some averages (mean and median), split by the outcome variable.\n\ndf %&gt;%\n  group_by(yesno) %&gt;%\n  summarise_all(mean)\n\n# A tibble: 2 × 7\n  yesno crl.tot dollar  bang  money    n000   make\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 TRUE     471. 0.174  0.514 0.213  0.247   0.152 \n2 FALSE    161. 0.0116 0.110 0.0171 0.00709 0.0735\n\n\nWe can see that on average, spam emails have higher mean values for each of the predictors. No surprise there.\nHowever, the medians of some variables are zero, which suggests those variables have heavily positively skewed distributions with many zero values (sometimes called “zero-inflation”).\n\ndf %&gt;%\n  group_by(yesno) %&gt;%\n  summarise_all(median)\n\n# A tibble: 2 × 7\n  yesno crl.tot dollar  bang money  n000  make\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 TRUE      194   0.08 0.331     0     0     0\n2 FALSE      54   0    0         0     0     0\n\n\nWe can confirm this by looking at the counts of zero values, in relation to the total counts.\nAs we see, the vast majority of the spam emails had non-zero values on these variables, and non-spam emails had significantly fewer non-zero values, with the exception of crl.tot. In particular, spam emails were MUCH more likely to contain “!”, “$”, “000”, and “money”.\n\nno = df %&gt;% filter(yesno == FALSE) %&gt;% select(-yesno)\nyes = df %&gt;% filter(yesno == TRUE) %&gt;% select(-yesno)\n\nround(colSums(no&gt;0)/nrow(no)*100)\n\ncrl.tot  dollar    bang   money    n000    make \n    100      10      27       2       3      15 \n\nround(colSums(yes&gt;0)/nrow(yes)*100)\n\ncrl.tot  dollar    bang   money    n000    make \n    100      61      83      38      33      35"
  },
  {
    "objectID": "blog/2023/08/19/index.html#distributions-by-spam",
    "href": "blog/2023/08/19/index.html#distributions-by-spam",
    "title": "Building a prediction model to detect spam email",
    "section": "Distributions, by spam",
    "text": "Distributions, by spam\nNext, let’s look at the distributions.\nFor these plots, since they all have extreme skew, I’m going to truncate them at the 90th percentile and look at the left side where most of the mass is.\n\nfor (c in c('crl.tot', 'dollar', 'bang', 'money', 'n000', 'make')){\n  \n  qtile_90 &lt;- quantile(df[[c]], .90)\n  \n  df %&gt;%\n    filter(!!sym(c) &lt; qtile_90) %&gt;%\n    ggplot(aes(x = !!sym(c), fill=yesno)) +\n      geom_density(alpha=.7) +\n      ggtitle(c) -&gt; plot\n  print(plot)\n  \n}"
  },
  {
    "objectID": "blog/2023/08/19/index.html#feature-correlations",
    "href": "blog/2023/08/19/index.html#feature-correlations",
    "title": "Building a prediction model to detect spam email",
    "section": "Feature correlations",
    "text": "Feature correlations\nIt doesn’t look like the features are very strongly correlated. The strongest correlation is between n000 and dollar, which is not particularly surprising since I would expect that “000” would tend to appear in the context of a dollar value like “$1000”.\n\ndf %&gt;% \n  select(-yesno) %&gt;%\n  cor(use = \"complete.obs\")\n\n           crl.tot    dollar       bang      money       n000       make\ncrl.tot 1.00000000 0.2019477 0.03632120 0.08099318 0.16597657 0.08916478\ndollar  0.20194768 1.0000000 0.14291296 0.10469131 0.31097072 0.11741853\nbang    0.03632120 0.1429130 1.00000000 0.05107591 0.07010334 0.05829200\nmoney   0.08099318 0.1046913 0.05107591 1.00000000 0.05258693 0.18815518\nn000    0.16597657 0.3109707 0.07010334 0.05258693 1.00000000 0.13407211\nmake    0.08916478 0.1174185 0.05829200 0.18815518 0.13407211 1.00000000\n\n\nIf we convert the features to boolean, we can see that the presence of features have stronger correlations. The strongest correlation is again between dollar and n000, but money and dollar also occur together more often than not.\n\ndf %&gt;%\n  select(-yesno, -crl.tot) %&gt;%\n  mutate(dollar = dollar &gt; 0,\n         bang = bang &gt; 0,\n         money = money &gt; 0,\n         n000 = n000 &gt; 0,\n         make = make &gt; 0\n  ) %&gt;%\n  cor(use = \"complete.obs\")\n\n          dollar      bang     money      n000      make\ndollar 1.0000000 0.3779057 0.5058803 0.5372603 0.4133374\nbang   0.3779057 1.0000000 0.3290505 0.3404896 0.2641339\nmoney  0.5058803 0.3290505 1.0000000 0.4140177 0.4092119\nn000   0.5372603 0.3404896 0.4140177 1.0000000 0.3757565\nmake   0.4133374 0.2641339 0.4092119 0.3757565 1.0000000"
  },
  {
    "objectID": "blog/2023/08/19/index.html#simple-classification-algorithm",
    "href": "blog/2023/08/19/index.html#simple-classification-algorithm",
    "title": "Building a prediction model to detect spam email",
    "section": "Simple classification algorithm",
    "text": "Simple classification algorithm\nJust for fun, let’s see how well we can distinguish spam vs. not spam using a simple heuristic.\nI’ll label anything as spam if it contained at least 1 “money”, “$”, “000”, and “!” OR if it contained more than 100 uninterrupted sequences of capital letters and at least 1 “!”. This is just what comes to mind after looking at the frequency plots above.\n\ndf %&gt;%\n  mutate(simple_spam_flag = factor((money &gt; 0 & dollar &gt; 0 & bang &gt; 0 & n000 &gt; 0) | \n                                     (crl.tot &gt; 100 & bang &gt; 0), \n                                   levels=c(TRUE, FALSE))\n         ) -&gt; df_flag\n\nThis simple classification algorithm achieved an accuracy of 79%, with 64% sensitivity and 89% specificity. This doesn’t seem too bad. But what is a good baseline of performance?\n\nconfusionMatrix(df_flag$simple_spam_flag, df_flag$yesno, mode='everything')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE  1172   309\n     FALSE  641  2479\n                                          \n               Accuracy : 0.7935          \n                 95% CI : (0.7815, 0.8051)\n    No Information Rate : 0.606           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5533          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.6464          \n            Specificity : 0.8892          \n         Pos Pred Value : 0.7914          \n         Neg Pred Value : 0.7946          \n              Precision : 0.7914          \n                 Recall : 0.6464          \n                     F1 : 0.7116          \n             Prevalence : 0.3940          \n         Detection Rate : 0.2547          \n   Detection Prevalence : 0.3219          \n      Balanced Accuracy : 0.7678          \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\nWe can see that the base rate of spam is 39%.\n\nmean(df$yesno == TRUE)\n\n[1] 0.3940448\n\n\nA good baseline model might be to predict the majority class, which in this case is not-spam.\nThis “always predict FALSE” baseline model can be expected to achieve 1 minus the base rate of spam (i.e., 61%), and we can see that this is the case if we construct just such a model. This tells us that the heuristic model above is quite a bit better than a completely naive model.\n\nconfusionMatrix(factor(rep('FALSE',nrow(df_flag))), df_flag$yesno, mode='everything')\n\nWarning in confusionMatrix.default(factor(rep(\"FALSE\", nrow(df_flag))), :\nLevels are not in the same order for reference and data. Refactoring data to\nmatch.\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE     0     0\n     FALSE 1813  2788\n                                          \n               Accuracy : 0.606           \n                 95% CI : (0.5917, 0.6201)\n    No Information Rate : 0.606           \n    P-Value [Acc &gt; NIR] : 0.5064          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.000           \n            Specificity : 1.000           \n         Pos Pred Value :   NaN           \n         Neg Pred Value : 0.606           \n              Precision :    NA           \n                 Recall : 0.000           \n                     F1 :    NA           \n             Prevalence : 0.394           \n         Detection Rate : 0.000           \n   Detection Prevalence : 0.000           \n      Balanced Accuracy : 0.500           \n                                          \n       'Positive' Class : TRUE"
  },
  {
    "objectID": "blog/2023/08/19/index.html#decision-tree",
    "href": "blog/2023/08/19/index.html#decision-tree",
    "title": "Building a prediction model to detect spam email",
    "section": "Decision Tree",
    "text": "Decision Tree\nProceeding from simpler to more complex, we can go a step further and try fitting a decision tree model. The decision tree will help us to identify a more sophisticated rule set for classifying spam mail. Decision trees also have the advantage of being highly interpretable.\nWe’ll start by splitting our data into training and test – that way, we won’t be testing performance on the same data that our model was trained on, and we can minimize the risk of overfitting.\n\nset.seed(200)\nsplit &lt;- initial_split(df)\ntrain &lt;- training(split)\ntest &lt;- testing(split)\n\nNext, we’ll fit the model and then visualize its logic.\nWe can read this chart by starting at the root node and following the branches until we reach a terminal node. The predicted value at this terminal node will give us the prediction that the model has made, and the path that we followed to get there provides its reasoning for the prediction.\nSo for example, if we follow the tree to the left-most terminal node, we can see that it would predict that an email was spam if it contained dollar &gt;= 0.056. If we follow the tree to the right-most terminal, we can see that it would predict that an email was not spam if it contained dollar &lt; 0.056 and bang &gt;= 0.12. The percentage value in the node tells us what percentage of emails met these criteria. So 24% of emails met the former criteria, and 54% the latter.\n\ndecision_tree &lt;- rpart(yesno ~ ., data=train, method='class')\ndecision_tree\n\nn= 3450 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 3450 1358 FALSE (0.3936232 0.6063768)  \n   2) dollar&gt;=0.0555 839   94 TRUE (0.8879619 0.1120381) *\n   3) dollar&lt; 0.0555 2611  613 FALSE (0.2347759 0.7652241)  \n     6) bang&gt;=0.1205 739  330 TRUE (0.5534506 0.4465494)  \n      12) crl.tot&gt;=80.5 361   75 TRUE (0.7922438 0.2077562) *\n      13) crl.tot&lt; 80.5 378  123 FALSE (0.3253968 0.6746032)  \n        26) bang&gt;=0.802 85   34 TRUE (0.6000000 0.4000000) *\n        27) bang&lt; 0.802 293   72 FALSE (0.2457338 0.7542662) *\n     7) bang&lt; 0.1205 1872  204 FALSE (0.1089744 0.8910256) *\n\nrpart.plot(decision_tree)\n\n\n\n\n\n\n\nFinally, we can test the model on the out-of-sample test dataset and see how it performs.\nOverall it achieved an accuracy of 85%, with 78% sensitivity and 89% specificity. This model has similar specificity as the heuristic model, but better sensitivity, and therefore better overall accuracy.\n\ntest_pred &lt;- predict(decision_tree, test, type='class')\nconfusionMatrix(test_pred, test$yesno, mode='everything')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE   355    77\n     FALSE  100   619\n                                          \n               Accuracy : 0.8462          \n                 95% CI : (0.8241, 0.8666)\n    No Information Rate : 0.6047          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6755          \n                                          \n Mcnemar's Test P-Value : 0.0982          \n                                          \n            Sensitivity : 0.7802          \n            Specificity : 0.8894          \n         Pos Pred Value : 0.8218          \n         Neg Pred Value : 0.8609          \n              Precision : 0.8218          \n                 Recall : 0.7802          \n                     F1 : 0.8005          \n             Prevalence : 0.3953          \n         Detection Rate : 0.3084          \n   Detection Prevalence : 0.3753          \n      Balanced Accuracy : 0.8348          \n                                          \n       'Positive' Class : TRUE"
  },
  {
    "objectID": "blog/2023/08/19/index.html#random-forest",
    "href": "blog/2023/08/19/index.html#random-forest",
    "title": "Building a prediction model to detect spam email",
    "section": "Random forest",
    "text": "Random forest\nNext, we’ll try a random forest model. Random forest models tend to perform better than decision trees, due to the fact that they are ensemble decision trees, meaning they group together the decisions of lots of decision trees. But as a result, they tend to be less interpretable. So if our goal was only to create the most accurate prediction model possible, then a random forest would be better suited to the task.\n\ncv &lt;- vfold_cv(train)\n\nI’ll use usemodels::use_ranger to give me a starting template.\n\nuse_ranger(yesno ~ ., train)\n\nranger_recipe &lt;- \n  recipe(formula = yesno ~ ., data = train) \n\nranger_spec &lt;- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") \n\nranger_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(ranger_recipe) %&gt;% \n  add_model(ranger_spec) \n\nset.seed(17214)\nranger_tune &lt;-\n  tune_grid(ranger_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n\n\nI’ll remove the parameter tuning to keep things simple.\n\nranger_recipe &lt;- \n  recipe(formula = yesno ~ ., data = df)\n\nranger_spec &lt;- \n  rand_forest(trees = 1000) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") \n\nranger_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(ranger_recipe) %&gt;% \n  add_model(ranger_spec) \n\nNext, I’ll fit the model using a resampling approach.\n\nset.seed(100)\nplan(multisession)\n\nfit_rf &lt;- fit_resamples(\n  ranger_workflow,\n  cv,\n  metrics = metric_set(accuracy, sens, spec),\n  control = control_resamples(verbose = TRUE,\n                              save_pred = TRUE,\n                              extract = function(x) x)\n)\n\nOverall, accuracy is pretty good: 89% accuracy, 79% sensitivity, and 95% specificity.\n\nfit_rf %&gt;%\n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.886    10 0.00497 Preprocessor1_Model1\n2 sens     binary     0.793    10 0.00797 Preprocessor1_Model1\n3 spec     binary     0.946    10 0.00523 Preprocessor1_Model1\n\n\nNext, we can check the performance on the test set.\nWe can use collect_metrics() function on the last fit.\n\nranger_workflow %&gt;%\n  last_fit(split) %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.888 Preprocessor1_Model1\n2 roc_auc  binary         0.930 Preprocessor1_Model1\n\n\nOr we can use confusionMatrix() to get a bit more information.\nPerformance on the test set is similar to the training performance. Overall, accuracy is pretty good – and better than the decision tree. For a spam detection filter, we’d want to bias towards minimizing false positives (it would arguably be worse for people to lose legitimate mail to the filter, than to have spam mail slip through), and here we see that the specificity was quite good at ~95%.\n\nranger_workflow %&gt;%\n  last_fit(split) %&gt;% \n  extract_workflow() -&gt; final_model\n\nconfusionMatrix(predict(final_model, test)$.pred_class, test$yesno, mode='everything', positive='TRUE')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE   362    34\n     FALSE   93   662\n                                          \n               Accuracy : 0.8897          \n                 95% CI : (0.8701, 0.9072)\n    No Information Rate : 0.6047          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7639          \n                                          \n Mcnemar's Test P-Value : 2.652e-07       \n                                          \n            Sensitivity : 0.7956          \n            Specificity : 0.9511          \n         Pos Pred Value : 0.9141          \n         Neg Pred Value : 0.8768          \n              Precision : 0.9141          \n                 Recall : 0.7956          \n                     F1 : 0.8508          \n             Prevalence : 0.3953          \n         Detection Rate : 0.3145          \n   Detection Prevalence : 0.3440          \n      Balanced Accuracy : 0.8734          \n                                          \n       'Positive' Class : TRUE"
  },
  {
    "objectID": "blog/2020/04/11/index.html",
    "href": "blog/2020/04/11/index.html",
    "title": "Estimating how many people live near a landmark / point-of-interest",
    "section": "",
    "text": "It can be useful to know how many people live near a landmark / point-of-interest (POI). For example, a location is often considered “walkable” if you can walk to it in 10 minutes or less. Understanding how many people live near a POI is one way of estimating how many people are within walking distance of a POI, if they were to walk from their home to the POI.\nIn this post, I start with a point-of-interest, “Times Square, NYC”, and using the Census API I find out how many people live within the census tract that contains this POI (a tract is one of the smallest sub-divisions for which the Census provides population estimates).\nIf we wanted to go a bit further down this path of estimating “population within walking distance” we could take this approach and expand it to include all census tracts within a certain distance of our POI census tract. I used this approach one time to understand the potential “reach” of outdoor neighborhood advertising."
  },
  {
    "objectID": "blog/2020/04/11/index.html#getting-shapefiles",
    "href": "blog/2020/04/11/index.html#getting-shapefiles",
    "title": "Estimating how many people live near a landmark / point-of-interest",
    "section": "Getting shapefiles",
    "text": "Getting shapefiles\nshpurls = states.NY.shapefile_urls()\nfor region, url in shpurls.items():\n    print(region, url)\ntract https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_36_tract10.zip\ncd https://www2.census.gov/geo/tiger/TIGER2010/CD/111/tl_2010_36_cd111.zip\ncounty https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_36_county10.zip\nstate https://www2.census.gov/geo/tiger/TIGER2010/STATE/2010/tl_2010_36_state10.zip\nzcta https://www2.census.gov/geo/tiger/TIGER2010/ZCTA5/2010/tl_2010_36_zcta510.zip\nblock https://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_36_tabblock10.zip\nblockgroup https://www2.census.gov/geo/tiger/TIGER2010/BG/2010/tl_2010_36_bg10.zip\nNow we’ll download and unzip the shapefiles for the tract zip.\nwget.download('https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_36_tract10.zip')\nwith zipfile.ZipFile('tl_2010_36_tract10.zip', 'r') as zip_ref:\n    zip_ref.extractall()\n-1 / unknown"
  },
  {
    "objectID": "blog/2020/04/11/index.html#converting-shapefile-to-lnglat-coords",
    "href": "blog/2020/04/11/index.html#converting-shapefile-to-lnglat-coords",
    "title": "Estimating how many people live near a landmark / point-of-interest",
    "section": "Converting shapefile to LNG/LAT coords",
    "text": "Converting shapefile to LNG/LAT coords\nNext, we’ll read the shape files into a dataframe, giving us a “coords” column, using the shapefile library. Note that these will be in LNG, LAT (not LAT, LNG).\ndef read_shapefile(sf):\n    \"\"\"\n    Read a shapefile into a Pandas dataframe with a 'coords' \n    column holding the geometry information. This uses the pyshp\n    package\n    \"\"\"\n    fields = [x[0] for x in sf.fields][1:]\n    records = sf.records()\n    shps = [s.points for s in sf.shapes()]    \n    df = pd.DataFrame(columns=fields, data=records)\n    df = df.assign(coords=shps)\n    return df\n\nshp_path = 'tl_2010_36_tract10.shp'\nsf = shp.Reader(shp_path)\ndf = read_shapefile(sf)\nThis is the tract we’re interested in:\npoi_tract = df[df['GEOID10'] == poi_cg['Census Tracts'][0]['GEOID']].reset_index()\npoi_tract['coords']\n0    [(-73.985085, 40.758589), (-73.98225599999999,...\nName: coords, dtype: object"
  },
  {
    "objectID": "blog/2020/04/11/index.html#map-the-poi-marker-and-tract-polygon",
    "href": "blog/2020/04/11/index.html#map-the-poi-marker-and-tract-polygon",
    "title": "Estimating how many people live near a landmark / point-of-interest",
    "section": "Map the POI marker and tract polygon",
    "text": "Map the POI marker and tract polygon\nFinally, we’re ready to map the POI and the tract polygon in which it is located. We’ll use shapely to get the polygon and folium to do the mapping.\n# Convert to Polygon\npoi_poly = Polygon(poi_tract['coords'][0])\n\n# Initialize map with zoom and custom tileset, centered on POI\nm = folium.Map(location=[poi['lat'], poi['lng']],\n               zoom_start=16,\n               tiles='cartodbpositron')\n\n# Add a map pin\nfolium.Marker([poi['lat'], poi['lng']]).add_to(m)\n\n# Add the polygon\nfolium.GeoJson(poi_poly).add_to(m)\n\nm"
  },
  {
    "objectID": "blog/2020/04/01/index.html",
    "href": "blog/2020/04/01/index.html",
    "title": "Using tensorflow with EfficientNet to predict plant diseases",
    "section": "",
    "text": "As I continue to practice using tensorflow for image recognition tasks, I thought I would experiment with the Plant Pathology dataset on Kaggle. Like MNIST, this is an image recognition challenge. But in contrast to the simplicity of MNIST, this challenge is about making “fine-grained” visual discriminations. The images are larger and in RGB color, and the features are smaller and more nuanced.\nI ran into a few challenges here because the task was so compute intensive. The first challenge was getting tensorflow setup and working with my ultrabook’s GPU instead of the CPU. This was an important step to speed up how quickly I could iterate on models. The second challenge was getting past the initial poor performance of a custom convolutional neural network. I noticed that some Kagglers were using EfficientNet as a base model, so I decided to give that a try.\nEfficientNet is a CNN derived from ImageNet with similar accuracy but “an order of magnitude fewer parameters and FLOPS”. In other words, it’s a really efficient drop-in replacement for ImageNet. Once I added this as a base model, I quickly reached high validation accuracy in relatively few epochs.\nI’m starting to understand better the value of training these models with lots of compute power. My ultrabook’s GPU only has 4GB memory, which imposed a significant limitation on the batch size and image size that I could train the model with. In comparison to this, when I used a GPU-powered notebook on Kaggle that has 15GB of GPU memory, I was able to train on batch sizes and image sizes almost twice as large, which allowed the model to reach higher validation accuracy.\nUsing the code below, I was able to reach 91% validation accuracy. With large batch and image size settings on Kaggle, this model reached 94% test accuracy (see here).\n\nRead in data/libraries\n# Download data from Kaggle\n#!kaggle competitions download -c plant-pathology-2020-fgvc7\nimport pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\n\n# Append \".jpg\" to make things easier later\ntrain['image_id'] = train['image_id'] + '.jpg'\ntest['image_id'] = test['image_id'] + '.jpg'\nCheck devices available. Hopefully we see a GPU :)\nimport tensorflow as tf\n\n# Check devices\ntf.config.list_physical_devices(None)\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\ntrain.head()\n\n\n\n\n\n\n\n\nimage_id\n\n\nhealthy\n\n\nmultiple_diseases\n\n\nrust\n\n\nscab\n\n\n\n\n\n\n0\n\n\nTrain_0.jpg\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n1\n\n\nTrain_1.jpg\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\nTrain_2.jpg\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\nTrain_3.jpg\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n4\n\n\nTrain_4.jpg\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\nLet’s take a look at some of these images.\nfrom matplotlib import pyplot as plt\nfrom matplotlib import image as mpimg\n\nIMG_PATH = 'images/'\n\nfor i in range(5):\n    plt.imshow(mpimg.imread(IMG_PATH + train.iloc[i,:]['image_id']))\n    if train.iloc[i,:]['healthy'] == 1:\n        plt.title('healthy')\n    elif train.iloc[i,:]['multiple_diseases'] == 1:\n        plt.title('multiple_diseases')\n    elif train.iloc[i,:]['rust'] == 1:\n        plt.title('rust')\n    else:\n        plt.title('scab')\n    plt.show()\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\nModel with EfficientNet Transfer Learning\nNow we’ll train a model using EfficientNet transfer learning.\nFor this model, we will use the following:\n\nA variety of image augmentations\nA ModelCheckpoint callback, so we can load the best model at the end\nReduceLROnPlateau to reduce the learning rate when the training gets stuck\nSigmoidFocalCrossEntropy loss function, which is good for imbalanced classes\n128x128 image sizes, because my GPU only has 4GB of memory :)\n\nfrom sklearn.model_selection import train_test_split\n\n# Training-validation split\ntraining, validation = train_test_split(train, \n                                        test_size = 0.2,\n                                        random_state = 42)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nSIZE = 128\nBATCH = 16\nTARGETS = ['healthy','multiple_diseases','rust','scab']\n\n# image augmentations\nimage_gen = ImageDataGenerator(rescale=1./255,\n                                rotation_range=20,\n                                width_shift_range=0.2,\n                                height_shift_range=0.2,\n                                zoom_range=0.2,\n                                brightness_range=[0.5, 1.5],\n                                horizontal_flip=True,\n                                vertical_flip=True)\n\n# flow_from_dataframe generators\ntrain_generator = image_gen\\\n    .flow_from_dataframe(train,\n                        directory=IMG_PATH,\n                        target_size=(SIZE, SIZE),\n                        x_col=\"image_id\",\n                        y_col=TARGETS,\n                        class_mode='raw',\n                        shuffle=False,\n                        batch_size=BATCH)\n\nvalidation_generator = image_gen\\\n    .flow_from_dataframe(validation,\n                        directory=IMG_PATH,\n                        target_size=(SIZE, SIZE),\n                        x_col=\"image_id\",\n                        y_col=TARGETS,\n                        class_mode='raw',\n                        shuffle=False,\n                        batch_size=BATCH)\n\ntest_generator = image_gen\\\n    .flow_from_dataframe(test,\n                        directory=IMG_PATH,\n                        target_size=(SIZE, SIZE),\n                        x_col=\"image_id\",\n                        y_col=None,\n                        class_mode=None,\n                        shuffle=False,\n                        batch_size=BATCH)\nFound 1821 validated image filenames.\nFound 365 validated image filenames.\nFound 1821 validated image filenames.\nimport efficientnet.keras as efn \nimport tensorflow_addons as tfa\nfrom tensorflow.keras.callbacks import Callback\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adadelta\n\n# Callbacks\n## Keep the best model\nmc = ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0, monitor='val_loss', mode='min')\n\n## Reduce learning rate if it gets stuck in a plateau\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.000001, verbose=1)\n\n# Model\n## Define the base model with EfficientNet weights\nmodel = efn.EfficientNetB4(weights = 'imagenet', \n                           include_top = False, \n                           input_shape = (SIZE, SIZE, 3))\n\n## Output layer\nx = model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation=\"relu\")(x)\nx = Dense(64, activation=\"relu\")(x)\npredictions = Dense(4, activation=\"softmax\")(x)\n\n## Compile and run\nmodel = Model(inputs=model.input, outputs=predictions)\n\nmodel.compile(optimizer='adam',\n              loss=tfa.losses.SigmoidFocalCrossEntropy(), \n              metrics=['accuracy'])\n\nmodel_history = model.fit(train_generator,\n                            validation_data=validation_generator,\n                            steps_per_epoch=train_generator.n/BATCH,\n                            validation_steps=validation_generator.n/BATCH,\n                            epochs=7,\n                            verbose=1,\n                            callbacks = [rlr, mc])\nEpoch 1/7\n114/113 [==============================] - 96s 844ms/step - loss: 0.1770 - accuracy: 0.6425 - val_loss: 0.2638 - val_accuracy: 0.7589\nEpoch 2/7\n114/113 [==============================] - 68s 595ms/step - loss: 0.1193 - accuracy: 0.8034 - val_loss: 0.1677 - val_accuracy: 0.7890\nEpoch 3/7\n114/113 [==============================] - 68s 597ms/step - loss: 0.1116 - accuracy: 0.8276 - val_loss: 0.1171 - val_accuracy: 0.8137\nEpoch 4/7\n114/113 [==============================] - 68s 597ms/step - loss: 0.0851 - accuracy: 0.8655 - val_loss: 0.1628 - val_accuracy: 0.8630\nEpoch 5/7\n114/113 [==============================] - 69s 601ms/step - loss: 0.0758 - accuracy: 0.8836 - val_loss: 0.0551 - val_accuracy: 0.9068\nEpoch 6/7\n114/113 [==============================] - 68s 600ms/step - loss: 0.0724 - accuracy: 0.8984 - val_loss: 0.0455 - val_accuracy: 0.9096\nEpoch 7/7\n114/113 [==============================] - 69s 602ms/step - loss: 0.0714 - accuracy: 0.8874 - val_loss: 0.0827 - val_accuracy: 0.8959\n# Load best model\n#model.load_weights(\"model.hdf5\")\n# Plot training and validation accuracy\nacc = model_history.history['accuracy']\nval_acc = model_history.history['val_accuracy']\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\npng\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\nMake predictions\n# Make predictions\npreds = model.predict(test_generator, steps=test_generator.n/BATCH)\n\n\nPrepare submission\n# Make submission\nsample_sub = pd.read_csv('sample_submission.csv')\n\nsubmission = pd.DataFrame({'image_id': sample_sub['image_id'],\n                           'healthy': preds[:,0],\n                           'multiple_diseases': preds[:,1],\n                           'rust': preds[:,2],\n                           'scab': preds[:,3]\n                         })\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()\n\n\n\n\n\n\n\n\nimage_id\n\n\nhealthy\n\n\nmultiple_diseases\n\n\nrust\n\n\nscab\n\n\n\n\n\n\n0\n\n\nTest_0\n\n\n0.092860\n\n\n0.169678\n\n\n0.656923\n\n\n0.080539\n\n\n\n\n1\n\n\nTest_1\n\n\n0.111859\n\n\n0.198063\n\n\n0.606325\n\n\n0.083752\n\n\n\n\n2\n\n\nTest_2\n\n\n0.026520\n\n\n0.044308\n\n\n0.003016\n\n\n0.926157\n\n\n\n\n3\n\n\nTest_3\n\n\n0.604854\n\n\n0.147050\n\n\n0.111277\n\n\n0.136820\n\n\n\n\n4\n\n\nTest_4\n\n\n0.078862\n\n\n0.118420\n\n\n0.750966\n\n\n0.051751"
  },
  {
    "objectID": "blog/2020/03/20/index.html",
    "href": "blog/2020/03/20/index.html",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "",
    "text": "I decided to explore and model the Heart Disease UCI dataset from Kaggle. The original source can be found at the UCI Machine Learning Repository. The dataset contains 303 individuals and 14 attribute observations (the original source data contains additional features). The features included various heart disease-related measurements, like chest pain and resting ECG, as well as age and sex, and represented a mix of binary, categorical, ordinal, and numeric data. The outcome variable represented the presence or absence of heart disease.\nI applied a fairly standard machine learning process, beginning with understanding what the data represented – this involved looking at the raw data and reading the data dictionary and other documentation – followed by splitting the data into training and test sets, Exploratory Data Analysis to understand how the features relate to the outcome, feature encoding, model fitting (logistic regression), and model evaluation.\nUltimately, the model was 75% accurate in predicting heart disease. It correctly predicted 46 out of the 61 test cases. Of the errors it made, 5 were false-positive errors and 10 were false-negative errors."
  },
  {
    "objectID": "blog/2020/03/20/index.html#binary",
    "href": "blog/2020/03/20/index.html#binary",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Binary",
    "text": "Binary\n\nsex (0 = female; 1 = male)\nfbs: Fasting blood sugar &gt; 120 mg/dl\nexang: Exercise induced angina (0 = no; 1 = yes)"
  },
  {
    "objectID": "blog/2020/03/20/index.html#categorical",
    "href": "blog/2020/03/20/index.html#categorical",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Categorical",
    "text": "Categorical\n\ncp: Chest pain type (0 = Asymptomatic angina; 1 = Atypical angina; 2 = Non-angina; 3 = Typical angina)\nrestecg: Resting ECG (0 = Left ventricular hypertrophy; 1 = Normal; 2 = ST-T wave abnormality)\nslope: Slope of the peak exercise ST segment (0 = downsloping; 1 = upsloping; 2 = flat)\nthal: Thalium stress test result (0 = NA; 1 = Fixed defect; 2 = Normal; 3 = Reversible defect)"
  },
  {
    "objectID": "blog/2020/03/20/index.html#ordinal",
    "href": "blog/2020/03/20/index.html#ordinal",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Ordinal",
    "text": "Ordinal\n\nca: number of major vessels (0-3) colored by flourosopy"
  },
  {
    "objectID": "blog/2020/03/20/index.html#numeric",
    "href": "blog/2020/03/20/index.html#numeric",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Numeric",
    "text": "Numeric\n\nage\noldpeak: ST depression induced by exercise relative to rest\ntrestbps: Resting blood pressure\nchol: Serum cholestoral in mg/dl\nthalach: Maximum heart rate achieved during thalium stress test"
  },
  {
    "objectID": "blog/2020/03/20/index.html#target",
    "href": "blog/2020/03/20/index.html#target",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Target",
    "text": "Target\n\ntarget: 1 = heart disease; 0 = no heart disease\n\nI’ll create arrays so the features are easier to access later by type.\nbins = ['sex', 'fbs', 'exang']\ncats = ['cp', 'restecg', 'slope', 'thal']\nords = ['ca']\nnums = ['age', 'oldpeak', 'trestbps', 'chol', 'thalach']\ntarget = ['target']\nI’ll recode the categorical variables so the data exploration is easier.\ndf.cp = df.cp.replace({0:'Asympt.', 1:'Atypical', 2:'Non', 3:'Typical'})\ndf.restecg = df.restecg.replace({0:'LV hyper', 1:'Normal', 2:'ST-T wave'})\ndf.slope = df.slope.replace({0:'down', 1:'up', 2:'flat'})\ndf.thal = df.thal.replace({0:'NA', 1:'Fixed', 2:'Normal', 3:'Revers.'})"
  },
  {
    "objectID": "blog/2020/03/20/index.html#numeric-and-ordinal-descriptives",
    "href": "blog/2020/03/20/index.html#numeric-and-ordinal-descriptives",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Numeric and ordinal descriptives",
    "text": "Numeric and ordinal descriptives\nX_train[X_train.target == 0].drop(cats + bins + target, \n                                  axis=1).describe().loc[['mean', 'std']]\n\n\n\n\n\n\n\n\nage\n\n\ntrestbps\n\n\nchol\n\n\nthalach\n\n\noldpeak\n\n\nca\n\n\n\n\n\n\nmean\n\n\n52.598485\n\n\n129.000000\n\n\n247.340909\n\n\n158.280303\n\n\n0.585606\n\n\n0.30303\n\n\n\n\nstd\n\n\n9.366058\n\n\n16.414944\n\n\n56.661824\n\n\n18.929082\n\n\n0.759282\n\n\n0.73036\n\n\n\n\n\nX_train[X_train.target == 1].drop(cats + bins + target, \n                                  axis=1).describe().loc[['mean', 'std']]\n\n\n\n\n\n\n\n\nage\n\n\ntrestbps\n\n\nchol\n\n\nthalach\n\n\noldpeak\n\n\nca\n\n\n\n\n\n\nmean\n\n\n56.554545\n\n\n134.236364\n\n\n252.190909\n\n\n139.718182\n\n\n1.616364\n\n\n1.090909\n\n\n\n\nstd\n\n\n8.085082\n\n\n18.436067\n\n\n49.041547\n\n\n21.377400\n\n\n1.314669\n\n\n1.018593\n\n\n\n\n\nHere we see that individuals with heart disease are/have…\n\nOlder [age]\nHigher resting blood pressure [trestbps]\nHigher cholesterol [chol]\nLower maximum heart rate [thalach]\nHigher exercise-induced ST depression [oldpeak]\nMore vessels colored by fluoroscopy [ca]"
  },
  {
    "objectID": "blog/2020/03/20/index.html#categorical-and-boolean-counts",
    "href": "blog/2020/03/20/index.html#categorical-and-boolean-counts",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Categorical and boolean counts",
    "text": "Categorical and boolean counts\n\nCategorical\nfig = plt.figure(figsize=(8, 6))\nfig.subplots_adjust(hspace=0.4, wspace=0.4, bottom=0.01, top=0.95)\n\nfor i, var in enumerate(cats):\n    i = i + 1\n    ax = fig.add_subplot(2, 2, i)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n    sns.countplot(data = X_train, x = var, hue = 'target', ax = ax)\n\nplt.show()\n\n\n\nRelationship between categorical variables and outcome.\n\n\nHere we see that individuals with heart disease are/have…\n\nMore likely to present with asymptomatic angina [cp]\nLess likely to present with atypical or no angina [cp]\nLess likely to present with normal resting ECG [restecg]\nMore likely to present with an up slope [slope]\nLess likely to present with a flat slope [slope]\nMore likely to preset with a reversible defect [thal]\nLess likely to present with a normal result on the thalium test [thal]\n\n\nCombining sparse classes\nSome of the categorical feature classes are sparse. There are instances where it might also make conceptual sense to collapse them. For example, the resting ECG test [restecg] has values that essentially correspond with “normal” and “abnormal”. The “ST-T wave” class is relatively rare, but it behaves similar to “LV hyper”. So this is a case where it might make sense to collapse the two abnormal classes.\ndf.restecg = df.restecg.replace({'Normal':0, 'LV hyper':1, 'ST-T wave':1})\ndf.thal = df.thal.replace({'NA':0, 'Normal':0, 'Fixed': 1, 'Revers.': 1})\nX_train, X_test, y_train, y_test = train_test_split(df, \n                                                    df.target, \n                                                    test_size = 0.2, \n                                                    random_state = 42,\n                                                    stratify = df.target)\nWe’ll also re-classify the features for which we just now collapsed classes to binary, and remove them from the categorical feature list.\nbins = ['sex', 'fbs', 'exang', 'thal', 'restecg']\ncats = ['cp', 'slope']\n\n\n\nBinary\nfig = plt.figure(figsize=(8, 6))\nfig.subplots_adjust(hspace=0.4, wspace=0.4, bottom=0.01, top=0.95)\n\nfor i, var in enumerate(bins):\n    i = i + 1\n    ax = fig.add_subplot(2, 3, i)\n    sns.countplot(data = X_train, x = var, hue = 'target', ax = ax)\n\nplt.show()\n\n\n\nRelationship between binary variables and outcome.\n\n\nHere we see that individuals with heart disease are/have…\n\nMore likely to be male [sex]\nLess likely to present with fbs &lt;= 120 mg/dl [fbs]\nMore likely to experience exercise-induced angina [exang]\nMore likely to have an abnormal reading on the thalium test [thal]\nLess likely to have a normal reading on the resting ECG test [restecg]"
  },
  {
    "objectID": "blog/2020/03/20/index.html#fit-the-model",
    "href": "blog/2020/03/20/index.html#fit-the-model",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Fit the model",
    "text": "Fit the model\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train_transformed, y_train)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
  },
  {
    "objectID": "blog/2020/03/20/index.html#score-the-model",
    "href": "blog/2020/03/20/index.html#score-the-model",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Score the model",
    "text": "Score the model\nlr.score(X_test_transformed, y_test)\n0.7540983606557377\n75% is not terrible, but not terribly great either. Generally 80% and above is considered acceptable, though of course it depends on the application, and specific kinds of errors might be important here. We fell short of this rule of thumb but not by a huge margin."
  },
  {
    "objectID": "blog/2020/03/20/index.html#confusion-matrix",
    "href": "blog/2020/03/20/index.html#confusion-matrix",
    "title": "Using a logistic regression model to predict heart disease",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred = lr.predict(X_test_transformed)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nconfusion_matrix\narray([[28,  5],\n       [10, 18]])\nHere we can see where the mistakes were made. We made 46 (28+18) correct predictions and 15 (10+5) incorrect predictions. 5 errors were false-positive errors and 10 were false-negative errors. Depending on how we plan to use this data, one of these types of errors might be more important than the other. For example, maybe we want to minimize false-negatives, because we don’t want to run the risk of missing a diagnosis and taking proactive measures to treat it."
  },
  {
    "objectID": "blog/2019/09/27/index.html",
    "href": "blog/2019/09/27/index.html",
    "title": "Predicting t-shirt size from height and weight",
    "section": "",
    "text": "Today I was given a task that sounded pretty straight-forward: What t-shirt size would you send to someone if you don’t know their shirt size, but instead you know their height, weight, and gender?\nIn fact, it seemed so straight-forward that I was sure there must be prior art out there that I could re-use. A StackOverflow, a mathematical formula, a GitHub repo, a blog post – there had to be something! To my surprise, there wasn’t any, not that I could find anyway.\nI guess this is a problem that hasn’t received a lot of attention. Or at least, it’s not the sort of problem that someone in the open source / open science community has tackled.\nSo I set out to build my own predictive algorithm."
  },
  {
    "objectID": "blog/2019/09/27/index.html#read-in-the-data",
    "href": "blog/2019/09/27/index.html#read-in-the-data",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Read in the data",
    "text": "Read in the data\nOf course the data had to be in a SAS format. 🙄\nLuckily there’s an R package for reading SAS data files.\n\nlibrary(foreign)\nlibrary(tidyverse)\nlibrary(MASS)\n\na &lt;- read.xport(\"ARX_F.XPT\") # Arthritis data\nb &lt;- read.xport(\"BMX_F.XPT\") # Body measurements\nd &lt;- read.xport(\"DEMO_F.XPT\") # Demographics\n\nThese are the variables I’ll pull out from the different dataframes.\n\nSEQN - Participant ID\nARXCCIN - Inhale chest circumference in CM\nBMXWT - Weight in KG\nBMXHT - Height in CM\nBMXBMI - Body Mass Index (BMI)\nDMDHRGND - Gender of participant (1 = Male, 2 = Female)\n\nI’m using the inhale chest circumference because I found in some exploratory analyses (not reported here) that it has a stronger correlation to the other measurements. I guess the exhale circumference was more noisy for some reason?\n\na %&gt;% dplyr::select(id = SEQN, chest_in_cm = ARXCCIN) -&gt; chest_measures\nb %&gt;% dplyr::select(id = SEQN, weight_kg = BMXWT, height_cm = BMXHT, bmi = BMXBMI) -&gt; height_weight\nd %&gt;% dplyr::select(id = SEQN, gender = DMDHRGND) %&gt;%\n  mutate(gender = case_when(gender == 1 ~ \"M\", TRUE ~ \"F\")) -&gt; gender\n\n# Join datasets and select only the rows that have all measurements\nchest_measures %&gt;%\n  left_join(., height_weight, by = c('id')) %&gt;%\n  left_join(., gender, by = c('id')) %&gt;%\n  filter(!is.na(chest_in_cm),\n         !is.na(height_cm), \n         !is.na(weight_kg),\n         !is.na(gender), \n         !is.na(bmi)) -&gt; df"
  },
  {
    "objectID": "blog/2019/09/27/index.html#height-and-weight-model",
    "href": "blog/2019/09/27/index.html#height-and-weight-model",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Height and weight model",
    "text": "Height and weight model\nTo build a model, I’ll run a stepwise linear regression to determine the model of best fit. I’ll enter height, weight, and gender as predictors and allow interaction terms. I’ll use AIC (Akaike’s Information Criterion) for model selection, since it penalizes models with added complexity and I want a parsimonious model that doesn’t overfit the data.\nI see that the best model includes all terms, including weight*height and weight*gender interaction terms.\n\nlm(data = subset(df, select= c(chest_in_cm, height_cm, weight_kg, gender)), \n   chest_in_cm ~ .) -&gt; mod\nstep.model &lt;- stepAIC(mod, direction = \"both\", trace = TRUE, scope = . ~ .^2)\n\nStart:  AIC=14857.43\nchest_in_cm ~ height_cm + weight_kg + gender\n\n                      Df Sum of Sq    RSS   AIC\n+ height_cm:weight_kg  1       220 115291 14851\n- height_cm            1         2 115513 14856\n+ weight_kg:gender     1        89 115421 14856\n&lt;none&gt;                             115510 14857\n+ height_cm:gender     1         7 115503 14859\n- gender               1      2052 117563 14937\n- weight_kg            1    397116 512627 21725\n\nStep:  AIC=14850.64\nchest_in_cm ~ height_cm + weight_kg + gender + height_cm:weight_kg\n\n                      Df Sum of Sq    RSS   AIC\n+ weight_kg:gender     1    144.02 115147 14847\n&lt;none&gt;                             115291 14851\n+ height_cm:gender     1     13.37 115277 14852\n- height_cm:weight_kg  1    219.87 115510 14857\n- gender               1   2064.63 117355 14930\n\nStep:  AIC=14846.88\nchest_in_cm ~ height_cm + weight_kg + gender + height_cm:weight_kg + \n    weight_kg:gender\n\n                      Df Sum of Sq    RSS   AIC\n&lt;none&gt;                             115147 14847\n+ height_cm:gender     1     3.499 115143 14849\n- weight_kg:gender     1   144.020 115291 14851\n- height_cm:weight_kg  1   274.743 115421 14856\n\nsummary(step.model)\n\n\nCall:\nlm(formula = chest_in_cm ~ height_cm + weight_kg + gender + height_cm:weight_kg + \n    weight_kg:gender, data = subset(df, select = c(chest_in_cm, \n    height_cm, weight_kg, gender)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.3179  -3.2347   0.2087   3.4462  15.4911 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         41.491415   4.708485   8.812  &lt; 2e-16 ***\nheight_cm            0.086786   0.028263   3.071 0.002148 ** \nweight_kg            0.666925   0.055507  12.015  &lt; 2e-16 ***\ngenderM             -0.030639   0.599462  -0.051 0.959239    \nheight_cm:weight_kg -0.001087   0.000328  -3.314 0.000925 ***\nweight_kg:genderM    0.016955   0.007065   2.400 0.016449 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.001 on 4604 degrees of freedom\nMultiple R-squared:  0.8152,    Adjusted R-squared:  0.815 \nF-statistic:  4061 on 5 and 4604 DF,  p-value: &lt; 2.2e-16\n\n# Save the model for later\nheight_weight_model &lt;- step.model"
  },
  {
    "objectID": "blog/2019/09/27/index.html#body-mass-index-bmi-model",
    "href": "blog/2019/09/27/index.html#body-mass-index-bmi-model",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Body Mass Index (BMI) model",
    "text": "Body Mass Index (BMI) model\nNow I’ll do the same with BMI and gender.\n\nlm(data = subset(df, select= c(chest_in_cm, bmi, gender)), chest_in_cm ~ .) -&gt; mod\nstep.model &lt;- stepAIC(mod, direction = \"both\", trace = TRUE, scope = . ~ .^2)\n\nStart:  AIC=17986.13\nchest_in_cm ~ bmi + gender\n\n             Df Sum of Sq    RSS   AIC\n+ bmi:gender  1       725 227076 17973\n&lt;none&gt;                    227801 17986\n- gender      1     11229 239030 18206\n- bmi         1    385144 612946 22547\n\nStep:  AIC=17973.44\nchest_in_cm ~ bmi + gender + bmi:gender\n\n             Df Sum of Sq    RSS   AIC\n&lt;none&gt;                    227076 17973\n- bmi:gender  1    724.73 227801 17986\n\nsummary(step.model)\n\n\nCall:\nlm(formula = chest_in_cm ~ bmi + gender + bmi:gender, data = subset(df, \n    select = c(chest_in_cm, bmi, gender)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.7682  -5.0630   0.2613   5.2463  23.2969 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 56.77733    0.66042  85.971  &lt; 2e-16 ***\nbmi          1.31235    0.02203  59.573  &lt; 2e-16 ***\ngenderM     -0.34122    0.92789  -0.368 0.713084    \nbmi:genderM  0.11906    0.03105   3.834 0.000128 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.021 on 4606 degrees of freedom\nMultiple R-squared:  0.6355,    Adjusted R-squared:  0.6353 \nF-statistic:  2677 on 3 and 4606 DF,  p-value: &lt; 2.2e-16\n\n# Save the model for later\nbmi_model &lt;- step.model"
  },
  {
    "objectID": "blog/2019/09/27/index.html#predict-chest-size-given-height-weight-and-gender",
    "href": "blog/2019/09/27/index.html#predict-chest-size-given-height-weight-and-gender",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Predict chest size given height, weight, and gender",
    "text": "Predict chest size given height, weight, and gender\nNext we’ll take the final model selected from the above procedure and use it to predict chest circumference, in inches, given height, weight, and gender.\nTo test the model, I’ll use average values.\n\nmean(df$weight_kg)\n\n[1] 82.48026\n\nmean(df$height_cm)\n\n[1] 168.0299\n\nmean(df$bmi)\n\n[1] 29.12334\n\n\nHeight, weight, and gender\n\ninput &lt;- data.frame(height_cm = 168, weight_kg = 83, gender = \"F\")\n\n# 1 cm = 0.393701 inches\npredict(height_weight_model, input) * 0.393701\n\n       1 \n37.90109 \n\n\nBMI and gender\n\ninput &lt;- data.frame(bmi = 29, gender = \"F\")\npredict(bmi_model, input) * 0.393701\n\n       1 \n37.33684"
  },
  {
    "objectID": "blog/2019/09/27/index.html#chest-size-to-shirt-size",
    "href": "blog/2019/09/27/index.html#chest-size-to-shirt-size",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Chest size to shirt size",
    "text": "Chest size to shirt size\nFinally, I’ll need to take the chest size prediction and convert it to a shirt size. Remember the size chart?\n\nFor some reason the chart leaves out “XS” and the ranges lso don’t provide full coverage of the possible chest size values (34-36” and then 38-40”???). So I’ll extend the upper bound of each range to meet the lower bound of each size above it. This will cause the predictions to err on the size of larger sizes rather than smaller sizes, which I think is better because if a shirt is too big, at least you can still wear it!\n\ninput &lt;- data.frame(height_cm = 168, weight_kg = 83, gender = \"F\")\n\ndata.frame(chest = predict(height_weight_model,input)[[1]] * 0.393701) %&gt;%\n  mutate(shirt_size = case_when(\n          chest &lt; 32 ~ \"XS\",\n          between(chest, 32, 36) ~ \"S\",\n          between(chest, 36, 40) ~ \"M\",\n          between(chest, 40, 44) ~ \"L\",\n          between(chest, 44, 48) ~ \"XL\",\n          between(chest, 48, 52) ~ \"2XL\",\n          between(chest, 52, 56) ~ \"3XL\",\n          between(chest, 56, 64) ~ \"4XL\",\n          chest &gt; 64 ~ \"5XL\"\n    )\n)\n\n     chest shirt_size\n1 37.90109          M"
  },
  {
    "objectID": "blog/2020/03/21/index.html",
    "href": "blog/2020/03/21/index.html",
    "title": "COVID-19 case growth and the Big 5 Personality traits",
    "section": "",
    "text": "Here I pose the following question:\nTo answer this question, I will need country-level aggregates on the Big 5 test, and a country-level aggregate that represents for “growth” over time in coronavirus cases.\nHere’s how I operationalize it: I take all the countries that reached at least 50 “confirmed cases” of the coronavirus, using data that’s up to date as of March 20, 2020. Then I take the number of cases those countries had 14-days after reaching 50 confirmed cases. This gives an estimate of growth within a country that can be compared across countries, because it puts them all on a level playing-field.\nNext, I compute country-level averages on the Big 5 Personality Test using data from the Open Source Psychometrics Project, and I only include countries with at least 1000 observations.\nFinally, I look at the correlation between Confirmed Cases at Day 14 and average scores on each of the Big 5 personality traits (openness, conscientiousness, extraversion, agreeableness, neuroticism [a.k.a. emotional stability]).\nFor easy reference, the following datasets are used:"
  },
  {
    "objectID": "blog/2020/03/21/index.html#filter",
    "href": "blog/2020/03/21/index.html#filter",
    "title": "COVID-19 case growth and the Big 5 Personality traits",
    "section": "Filter",
    "text": "Filter\nNext we’ll filter to countries that reached at least 50 confirmed cases, and had at least 14 days of data beyond reaching that point.\ncovid19 = covid19[covid19.ConfirmedCases &gt; 50]\ncovid19_numdays = covid19.loc[:, ['Country/Region', 'Date']]\\\n    .drop_duplicates()\\\n    .groupby('Country/Region')\\\n    .count()\\\n    .rename_axis('country')\\\n    .reset_index()\nprint(covid19_numdays.head())\n\ncovid19_mindays = covid19_numdays[covid19_numdays.Date &gt;= 14].reset_index()\ncovid19 = covid19[covid19['Country/Region'].isin(covid19_mindays.country)].reset_index()\n     country  Date\n0    Albania     5\n1    Algeria     5\n2    Andorra     2\n3  Argentina     5\n4    Armenia     5\nWhat/how many countries does that leave us with?\nprint(len(list(set(covid19['Country/Region'].values))))\nprint(set(covid19['Country/Region'].values))\n21\n{'Cruise Ship', 'Spain', 'United Kingdom', 'Singapore', 'Sweden', 'Kuwait', 'Bahrain', 'Germany', 'Iraq', 'Austria', 'Korea, South', 'Norway', 'China', 'Netherlands', 'Belgium', 'France', 'Malaysia', 'Switzerland', 'Iran', 'Japan', 'Italy'}\nObviously “Cruise Ship” isn’t a country. I won’t worry about it at this point, since it will get filtered out in later steps."
  },
  {
    "objectID": "blog/2020/03/21/index.html#compute-growth-over-14-days",
    "href": "blog/2020/03/21/index.html#compute-growth-over-14-days",
    "title": "COVID-19 case growth and the Big 5 Personality traits",
    "section": "Compute growth over 14 days",
    "text": "Compute growth over 14 days\nNext, we’ll compute the growth in cases for each country, from the date they reached 50 Confirmed Cases to the 14th day following that date. First we’ll need to collapse over province, since some countries are represented multiple times under different provinces.\ncovid19[covid19['Country/Region'] == 'China'].head()\n\n\n\n\n\n\n\n\nindex\n\n\nCountry/Region\n\n\nDate\n\n\nConfirmedCases\n\n\n\n\n\n\n47\n\n\n2777\n\n\nChina\n\n\n2020-01-26\n\n\n60.0\n\n\n\n\n48\n\n\n2778\n\n\nChina\n\n\n2020-01-27\n\n\n70.0\n\n\n\n\n49\n\n\n2779\n\n\nChina\n\n\n2020-01-28\n\n\n106.0\n\n\n\n\n50\n\n\n2780\n\n\nChina\n\n\n2020-01-29\n\n\n152.0\n\n\n\n\n51\n\n\n2781\n\n\nChina\n\n\n2020-01-30\n\n\n200.0\n\n\n\n\n\ncovid19_collapse_province = covid19\\\n    .groupby(['Country/Region', 'Date'])\\\n    .sum()\\\n    .reset_index()\ncovid19_collapse_province[covid19_collapse_province['Country/Region'] == 'China'].head()\n\n\n\n\n\n\n\n\nCountry/Region\n\n\nDate\n\n\nindex\n\n\nConfirmedCases\n\n\n\n\n\n\n47\n\n\nChina\n\n\n2020-01-22\n\n\n3540\n\n\n444.0\n\n\n\n\n48\n\n\nChina\n\n\n2020-01-23\n\n\n3541\n\n\n444.0\n\n\n\n\n49\n\n\nChina\n\n\n2020-01-24\n\n\n6612\n\n\n602.0\n\n\n\n\n50\n\n\nChina\n\n\n2020-01-25\n\n\n14172\n\n\n958.0\n\n\n\n\n51\n\n\nChina\n\n\n2020-01-26\n\n\n26818\n\n\n1628.0\n\n\n\n\n\ncovid19 = covid19_collapse_province\\\n    .groupby('Country/Region')\\\n    .head(14)\\\n    .groupby('Country/Region')\\\n    .tail(1)\n\ncovid19\n\n\n\n\n\n\n\n\nCountry/Region\n\n\nDate\n\n\nindex\n\n\nConfirmedCases\n\n\n\n\n\n\n13\n\n\nAustria\n\n\n2020-03-19\n\n\n1060\n\n\n2013.0\n\n\n\n\n28\n\n\nBahrain\n\n\n2020-03-17\n\n\n1176\n\n\n228.0\n\n\n\n\n45\n\n\nBelgium\n\n\n2020-03-19\n\n\n1414\n\n\n1795.0\n\n\n\n\n60\n\n\nChina\n\n\n2020-02-04\n\n\n90949\n\n\n23524.0\n\n\n\n\n119\n\n\nCruise Ship\n\n\n2020-02-20\n\n\n5103\n\n\n634.0\n\n\n\n\n162\n\n\nFrance\n\n\n2020-03-12\n\n\n6009\n\n\n2281.0\n\n\n\n\n184\n\n\nGermany\n\n\n2020-03-13\n\n\n6718\n\n\n3675.0\n\n\n\n\n205\n\n\nIran\n\n\n2020-03-08\n\n\n7657\n\n\n6566.0\n\n\n\n\n231\n\n\nIraq\n\n\n2020-03-20\n\n\n7728\n\n\n208.0\n\n\n\n\n245\n\n\nItaly\n\n\n2020-03-06\n\n\n7891\n\n\n4636.0\n\n\n\n\n273\n\n\nJapan\n\n\n2020-02-29\n\n\n8003\n\n\n241.0\n\n\n\n\n307\n\n\nKorea, South\n\n\n2020-03-04\n\n\n8302\n\n\n5621.0\n\n\n\n\n337\n\n\nKuwait\n\n\n2020-03-15\n\n\n8431\n\n\n112.0\n\n\n\n\n356\n\n\nMalaysia\n\n\n2020-03-19\n\n\n8907\n\n\n900.0\n\n\n\n\n371\n\n\nNetherlands\n\n\n2020-03-18\n\n\n9909\n\n\n2051.0\n\n\n\n\n387\n\n\nNorway\n\n\n2020-03-17\n\n\n10144\n\n\n1463.0\n\n\n\n\n404\n\n\nSingapore\n\n\n2020-02-26\n\n\n11481\n\n\n93.0\n\n\n\n\n441\n\n\nSpain\n\n\n2020-03-14\n\n\n11793\n\n\n6391.0\n\n\n\n\n461\n\n\nSweden\n\n\n2020-03-18\n\n\n12033\n\n\n1279.0\n\n\n\n\n477\n\n\nSwitzerland\n\n\n2020-03-16\n\n\n12090\n\n\n2200.0\n\n\n\n\n495\n\n\nUnited Kingdom\n\n\n2020-03-16\n\n\n16456\n\n\n1543.0"
  },
  {
    "objectID": "blog/2020/03/21/index.html#scoring-the-big-five-personality-test-items",
    "href": "blog/2020/03/21/index.html#scoring-the-big-five-personality-test-items",
    "title": "COVID-19 case growth and the Big 5 Personality traits",
    "section": "Scoring the Big Five Personality Test items",
    "text": "Scoring the Big Five Personality Test items\nThe Big 5 personality inventory contains 5 factors. Like most personality scales, the Big 5 has a mix of items that positively and negatively load onto these personality factors. For example, the factor Extraversion describes someone who is outgoing, energetic, talkative, and enjoys human interaction. The first Extraversion item [EXT1] is “I am the life of the party.”, a positively-keyed item; whereas the second item [EXT2] is “I don’t talk a lot.”, a negatively-keyed item.\nTo find out which items are positively or negatively keyed, we can look at the scale documentation on the IPIP website: https://ipip.ori.org/newBigFive5broadKey.htm"
  },
  {
    "objectID": "blog/2020/03/21/index.html#reverse-coding",
    "href": "blog/2020/03/21/index.html#reverse-coding",
    "title": "COVID-19 case growth and the Big 5 Personality traits",
    "section": "Reverse-coding",
    "text": "Reverse-coding\nBefore analyzing the data from a personality test, a psychologist will generally “reverse-code” the items that are negatively-keyed. This results in a dataset where the item values all have a common direction and interpretetion (i.e., a higher value corresponds with more of that trait). Mathematically, it allows you to then compute sums and averages for each of the factors. For example, after scoring the test items, we could compute an individual’s average for Extraversion items to get their Extraversion score.\nThis version of the Big 5 scale asks individuals to rate their level of agreement from 1 to 5, where 1 is strong disagreement and 5 is strong agreement. Reverse-coding is as simple as subtracting 6 from every reverse-keyed item.\nThe code below will accomplish this task.\npositively_keyed = ['EXT1', 'EXT3', 'EXT5', 'EXT7', 'EXT9',\n                    'EST1', 'EST3', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10',\n                    'AGR2', 'AGR4', 'AGR6', 'AGR8', 'AGR9', 'AGR10',\n                    'CSN1', 'CSN3', 'CSN5', 'CSN7', 'CSN9', 'CSN10', \n                    'OPN1', 'OPN3', 'OPN5', 'OPN7', 'OPN8', 'OPN9', 'OPN10']\n\nnegatively_keyed = ['EXT2', 'EXT4', 'EXT6', 'EXT8', 'EXT10',\n                    'EST2', 'EST4',\n                    'AGR1', 'AGR3', 'AGR5', 'AGR7', \n                    'CSN2', 'CSN4', 'CSN6', 'CSN8', \n                    'OPN2', 'OPN4', 'OPN6']\nbig5.loc[:, negatively_keyed] = 6 - big5.loc[:, negatively_keyed]"
  },
  {
    "objectID": "blog/2020/03/21/index.html#country-level-big-5-aggregates",
    "href": "blog/2020/03/21/index.html#country-level-big-5-aggregates",
    "title": "COVID-19 case growth and the Big 5 Personality traits",
    "section": "Country-Level Big 5 Aggregates",
    "text": "Country-Level Big 5 Aggregates\nFirst, we should eliminate any country that doesn’t have very many observations. Somewhat arbitrarily, we’ll draw a line at N = 1000.\nbig5_country_count = big5.country\\\n    .value_counts()\\\n    .rename_axis('country')\\\n    .reset_index(name='counts')\n\nprint(len(big5_country_count[big5_country_count.counts &gt; 1000]))\nprint(big5_country_count[big5_country_count.counts &gt; 1000].country.values)\n58\n['US' 'GB' 'CA' 'AU' 'PH' 'IN' 'DE' 'NONE' 'NZ' 'NO' 'MY' 'MX' 'SE' 'NL'\n 'SG' 'ID' 'BR' 'FR' 'DK' 'IE' 'IT' 'ES' 'PL' 'FI' 'RO' 'BE' 'ZA' 'CO'\n 'HK' 'PK' 'RU' 'AR' 'CH' 'AE' 'TR' 'PT' 'GR' 'VN' 'HR' 'AT' 'CL' 'RS'\n 'CZ' 'TH' 'JP' 'PE' 'KR' 'HU' 'IL' 'KE' 'CN' 'BG' 'VE' 'EC' 'LT' 'SA'\n 'EG' 'EE']\nThere are 58 countries with at least 1000 observations. Let’s go with these.\nbig5 = big5[big5.country.isin(big5_country_count[big5_country_count.counts &gt; 1000].country.values)]\n\n# Filter on the columns we're going to use\nbig5 = big5.loc[:,['country'] + positively_keyed + negatively_keyed]\n\nFactor aggregation\nNext, we’ll compute averages for each of the five factors at the level of the individual.\nEXT = ['EXT' + str(i) for i in range(1,11)]\nEST = ['EST' + str(i) for i in range(1,11)]\nAGR = ['AGR' + str(i) for i in range(1,11)]\nCSN = ['CSN' + str(i) for i in range(1,11)]\nOPN = ['OPN' + str(i) for i in range(1,11)]\nbig5['EXT'] = big5.loc[:, EXT].mean(axis=1)\nbig5['EST'] = big5.loc[:, EST].mean(axis=1)\nbig5['AGR'] = big5.loc[:, AGR].mean(axis=1)\nbig5['CSN'] = big5.loc[:, CSN].mean(axis=1)\nbig5['OPN'] = big5.loc[:, OPN].mean(axis=1)\nbig5 = big5.loc[:, ['country', 'EXT', 'EST', 'AGR', 'CSN', 'OPN']]\nDrop NAs, and any with country = ‘NONE’\nbig5 = big5.dropna()\nbig5 = big5[big5.country != 'NONE']\n\n\nCountry-level averages\nNow we can calculate the country-level averages.\nbig5_cavgs = big5.groupby('country')\\\n                    .mean()\\\n                    .rename_axis('country')\\\n                    .reset_index()\nJust to illustrate, these are the top 5 countries by country-level Extraversion scores.\nbig5_cavgs.loc[:, ['country', 'EXT']]\\\n    .sort_values(by=['EXT'])\\\n    .tail()\\\n    .plot(x = 'country', \n          y = 'EXT', \n          kind='barh', \n          legend=False)\n\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "blog/2020/04/04/index.html",
    "href": "blog/2020/04/04/index.html",
    "title": "Identifying pneumonia from chest x-rays using EfficientNet",
    "section": "",
    "text": "In my last post I used EfficientNet to identify plant diseases. I was surprised at how well this pre-trained model worked, with so few modifications, and I was curious how an approach like this might generalize to other visual image detection problems. In this post I use a similar approach to identify childhood pneumonia from chest x-ray images, using the Chest X-Ray Images (Pneumonia) dataset on Kaggle. Using this approach, I was able to achieve 97% accuracy, 97% precision, and 97% recall.\nThe code below implements this model. See also my notebook on Kaggle."
  },
  {
    "objectID": "blog/2020/04/04/index.html#training-performance",
    "href": "blog/2020/04/04/index.html#training-performance",
    "title": "Identifying pneumonia from chest x-rays using EfficientNet",
    "section": "Training performance",
    "text": "Training performance\n# Plot training and validation accuracy by epoch\nacc = model_history.history['accuracy']\nval_acc = model_history.history['val_accuracy']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\npng\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "blog/2020/04/04/index.html#confusion-matrix",
    "href": "blog/2020/04/04/index.html#confusion-matrix",
    "title": "Identifying pneumonia from chest x-rays using EfficientNet",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nCM = confusion_matrix(test_generator.classes, labels)\nfig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(5, 5))\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "blog/2020/04/04/index.html#classification-report",
    "href": "blog/2020/04/04/index.html#classification-report",
    "title": "Identifying pneumonia from chest x-rays using EfficientNet",
    "section": "Classification report",
    "text": "Classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_generator.classes, labels))\n              precision    recall  f1-score   support\n\n           0       0.95      0.92      0.94       317\n           1       0.97      0.98      0.98       855\n\n    accuracy                           0.97      1172\n   macro avg       0.96      0.95      0.96      1172\nweighted avg       0.97      0.97      0.97      1172"
  },
  {
    "objectID": "blog/2020/05/12/index.html",
    "href": "blog/2020/05/12/index.html",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "",
    "text": "In this blog post, I build a machine learning model to predict possible cases of cognitive impairment / dementia in a population of individuals over the age of 60. My data for this model comes from the 2013-2014 NHANES (National Health and Nutrition Examination Survey) study cohort, which is a nationally representative, longitudinal study of health in the US.\nAs an outcome measure, I’ll create a composite index of cognition by combining data from the Animal Fluency and Digit Symbol Substitution tasks. As predictors of this outcome, I’ll pull together variables that seem relevant to predicting dementia at a population level. For example, variables like age, race, gender, BMI, depression symptoms, alcohol use, blood lead levels, etc.\nI’ll use xgboost to train the model, and I’ll walk through a hypothetical use-case for the model, discussing what might be the appropriate model bias (e.g., specificity/recall).\nThe data is obtained from the NHANES website, for the 2013-2014 study cohort.\nData files are converted from XPT to CSV.\nRead the files into a single dataframe by joining on participant ID (SEQN)."
  },
  {
    "objectID": "blog/2020/05/12/index.html#cognitive-functioning-data",
    "href": "blog/2020/05/12/index.html#cognitive-functioning-data",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Cognitive functioning data",
    "text": "Cognitive functioning data\nThis dataset contains multiple tests on which cognitive functioning was assessed. For this analysis, I will be considering the Animal Fluency Task (AFT) and the Digit Symbol Substitution Task (DSST). These tasks measure different aspects of cognition.\nIn past research, these tasks have each been shown to discriminate between populations with and without dementia. For example, see Howe (2007) or Rosano et al. [2016].\nI will take these measures and combine them to create a composite measure of cognition. By creating a composite, I will be able to reduce measurement noise or bias.\nTo create a composite, I will first “standardize” each score using z-score normalization so that each score represents a standardized difference from the the mean of its distribution. I will then take the average of the standardized scores."
  },
  {
    "objectID": "blog/2020/05/12/index.html#number-of-observations",
    "href": "blog/2020/05/12/index.html#number-of-observations",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Number of observations",
    "text": "Number of observations\nAccording to the documentation on NHANES, there should be 1661 individuals with scores on the Animal Fluency Task (AFT), and 1592 participants with scores on the Digit Symbol Substitution Task (DSST). Here we can see that the intersection of individuals who completed AFT and DSST tasks is 1575.\nprint(len(df)) # All participants in the dataset\nprint(len(df[df['RIDAGEYR'] &gt; 60])) # All participants over 60\nprint(df['CFDAST'].notnull().sum()) # Participants with AFT scores\nprint(df['CFDDS'].notnull().sum()) # Participants with DSST scores\nlen(df[df['CFDAST'].notnull() & df['CFDDS'].notnull()]) # Individuals with both AFT and DSST\n\ndf2 = df[df['CFDAST'].notnull() & df['CFDDS'].notnull()].reset_index()\n10175\n1729\n1661\n1592"
  },
  {
    "objectID": "blog/2020/05/12/index.html#score-distributions",
    "href": "blog/2020/05/12/index.html#score-distributions",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Score distributions",
    "text": "Score distributions\nBelow we can see that the distribution of scores on each task are approximately normal.\nplt.hist(df2['CFDAST'], bins=35)\nplt.title(\"Animal Fluency Scores\")\nplt.show()\nplt.hist(df2['CFDDS'], bins=35)\nplt.title(\"Digit Symbol Substitution Scores\")\n\n\n\npng\n\n\nText(0.5, 1.0, 'Digit Symbol Substitution Scores')\n\n\n\npng"
  },
  {
    "objectID": "blog/2020/05/12/index.html#composite-scoring",
    "href": "blog/2020/05/12/index.html#composite-scoring",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Composite scoring",
    "text": "Composite scoring\nFirst I’ll standardize each of the test columns.\nfrom scipy.stats import zscore\ndf2['DSST_z'] = df2['CFDDS'].pipe(zscore)\ndf2['AFT_z'] = df2['CFDAST'].pipe(zscore)\nNext, I’ll create a composite by taking the average of the two standardized scores.\ndf2['COG'] = (df2['DSST_z'] + df2['AFT_z']) / 2\nFinally, I’ll check to make sure that the distribution of composite scores is still normal.\nplt.hist(df2['COG'], bins=35)\nplt.title(\"Composite Cognition\")\nText(0.5, 1.0, 'Composite Cognition')\n\n\n\npng"
  },
  {
    "objectID": "blog/2020/05/12/index.html#what-is-low-cognition",
    "href": "blog/2020/05/12/index.html#what-is-low-cognition",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "What is “low” cognition?",
    "text": "What is “low” cognition?\nSince these are standardized scores, we can say that low cognition represents 1 standardized unit below the mean.\nplt.hist(df2['COG'], bins=35)\nplt.axvline(x=-1, color='red', linestyle='--')\nplt.title(\"Composite Cognition with Threshold\")\nText(0.5, 1.0, 'Composite Cognition with Threshold')\n\n\n\npng\n\n\nI’ll create a variable to track this classification.\ndf2['COG_low'] = df2['COG'] &lt; -1"
  },
  {
    "objectID": "blog/2020/05/12/index.html#model-features",
    "href": "blog/2020/05/12/index.html#model-features",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Model features",
    "text": "Model features\nI’ll collect all the features that I think might be interesting.\nfeatures = {# Composite cognition score\n            'COG': 'cognition',\n            'COG_low': 'cognition_impaired',\n    \n            # Demographics\n            'RIAGENDR': 'gender', \n            'RIDAGEYR': 'age',\n            'RIDRETH1': 'race',\n            \n            # Poverty level\n            'INDFMMPI': 'poverty',\n            \n            # Body mass index\n            'BMXBMI': 'bmi',\n            \n            # Alcohol use\n            'ALQ120Q': 'alcohol_days',\n            \n            # Blood vitamin levels\n            'LBXVIDMS': 'vit_d',\n            'LBDB12': 'vit_b12',\n            \n            # Diet\n            'DBQ700': 'diet_healthy',\n            \n            # Lead\n            'LBXBPB': 'blood_lead',\n            \n            # Blood nicotine metabolite\n            'LBXCOT': 'cotinine',\n            \n            # Grip strength\n            # https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/MGX_H.htm#MGDCGSZ\n            'MGDCGSZ': 'grip_strength',\n            \n            # Smell test\n            'CSXCHOOD': 'smell_choco',\n            'CSXSBOD': 'smell_strawberry',\n            'CSXSMKOD': 'smell_smoke',\n            'CSXLEAOD': 'smell_leather',\n            'CSXSOAOD': 'smell_soap',\n            'CSXGRAOD': 'smell_grape',\n            'CSXONOD': 'smell_onion',\n            'CSXNGSOD': 'smell_gas',\n            \n            # Health in general\n            'HSD010': 'health_general',\n            \n            # Depression screener\n            'DPQ010': 'dep_1',\n            'DPQ020': 'dep_2',\n            'DPQ030': 'dep_3',\n            'DPQ040': 'dep_4',\n            'DPQ050': 'dep_5',\n            'DPQ060': 'dep_6',\n            'DPQ070': 'dep_7',\n            'DPQ080': 'dep_8',\n            'DPQ090': 'dep_9',\n            \n            # Sleep\n            'SLD010H': 'sleep',\n            \n            # Heart rate\n            'BPXPLS': 'heart_rate'\n            }\n\ndf2 = df2.loc[:, list(features)]\ndf2 = df2.rename(columns=features)\n\nRecoding variables\nSome of the features need to be combined. For example, the smell tests can all be combined into a single “smell test” score; the depression scale can be converted into a single depression score.\nOn some features the values are numeric but not ordinal. For example, on the depression scale scores 0-3 represent increasing levels of depression symptom severity, but a score of “9” means “don’t know” and a score of “7” means “refused”. This will be important to catch in the variable coding.\n\nDepression items\nFirst I’ll score the depression items. I’ll take the mean of all the items responded to.\nitems = ['dep_1', 'dep_2', 'dep_3', 'dep_4',\n        'dep_5', 'dep_6', 'dep_7', 'dep_8',\n        'dep_9']\n\ndef score_depression(x):\n    \n    scores = []\n    \n    for item in items:\n        value = x[item]\n        if value &lt; 4:\n            scores.append(value)\n    \n    if len(scores) == 0:\n        return None\n    else:\n        return np.mean(scores)\n    \n\ndf2.loc[:, 'dep_tot'] = df2.apply(score_depression, axis=1)\ndf2.drop(items, axis=1, inplace=True)\n\n\nSmell test\nNext, I’ll score the smell test items. Each item has one correct response and several incorrect responses. I’ll take the mean number of correct responses.\nitems = ['smell_choco', 'smell_strawberry',\n         'smell_smoke', 'smell_leather',\n         'smell_soap', 'smell_grape',\n         'smell_onion', 'smell_gas']\n\ndef score_smell(x):\n    \n    scores = []\n    \n    if x['smell_choco'] == 2:\n        scores.append(1)\n    elif ~np.isnan(x['smell_choco']):\n        scores.append(0)\n        \n    if x['smell_strawberry'] == 1:\n        scores.append(1)\n    elif ~np.isnan(x['smell_strawberry']):\n        scores.append(0)\n        \n    if x['smell_smoke'] == 3:\n        scores.append(1)\n    elif ~np.isnan(x['smell_smoke']):\n        scores.append(0)\n        \n    if x['smell_leather'] == 3:\n        scores.append(1)\n    elif ~np.isnan(x['smell_leather']):\n        scores.append(0)\n        \n    if x['smell_soap'] == 1:\n        scores.append(1)\n    elif ~np.isnan(x['smell_soap']):\n        scores.append(0)\n        \n    if x['smell_grape'] == 2:\n        scores.append(1)\n    elif ~np.isnan(x['smell_grape']):\n        scores.append(0)\n        \n    if x['smell_onion'] == 3:\n        scores.append(1)\n    elif ~np.isnan(x['smell_onion']):\n        scores.append(0)\n        \n    if x['smell_gas'] == 4:\n        scores.append(1)\n    elif ~np.isnan(x['smell_gas']):\n        scores.append(0)\n    \n    if len(scores) == 0:\n        return None\n    else:\n        return np.nanmean(scores)\n\ndf2.loc[:, 'smell_tot'] = df2.apply(score_smell, axis=1)\ndf2.drop(items, axis=1, inplace=True)\n\n\nOthers\nReplace “Don’t know” and “Refused” with missing.\ndf2['health_general'].replace([9.0, 7.0], [np.nan, np.nan], inplace=True)\ndf2['sleep'].replace([99.0, 77.0], [np.nan, np.nan], inplace=True)\ndf2['alcohol_days'].replace([999.0, 777.0], [np.nan, np.nan], inplace=True)\ndf2['diet_healthy'].replace([9.0, 7.0], [np.nan, np.nan], inplace=True)\n\n\n\nMissing values\nHow many values are missing in each column?\nprint('% missing values')\nround(df2.isnull().sum() / len(df2) * 100)\n% missing values\n\n\n\n\n\ncognition              0.0\ncognition_impaired     0.0\ngender                 0.0\nage                    0.0\nrace                   0.0\npoverty               11.0\nbmi                    1.0\nalcohol_days          18.0\nvit_d                  3.0\nvit_b12                4.0\ndiet_healthy           0.0\nblood_lead            51.0\ncotinine               4.0\ngrip_strength         11.0\nhealth_general         2.0\nsleep                  0.0\nheart_rate             3.0\ndep_tot                2.0\nsmell_tot              3.0\ndtype: float64"
  },
  {
    "objectID": "blog/2020/05/12/index.html#baseline-model",
    "href": "blog/2020/05/12/index.html#baseline-model",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Baseline model",
    "text": "Baseline model\nI’ll train a baseline xgboost model, without any parameter tuning. This will give me a general sense of the model’s performance.\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\nXGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n              objective='binary:logistic', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n              validate_parameters=False, verbosity=None)\nmodel.score(X_test, y_test)\n0.8571428571428571\n\nTest performance\nHow did the model perform on the test set?\npreds = model.predict(X_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, preds) * 100.0))\nprint(\"Precision: %.2f%%\" % (precision_score(y_test, preds) * 100.0))\nprint(\"Recall: %.2f%%\" % (recall_score(y_test, preds) * 100.0))\nplot_roc_curve(model, X_test, y_test)\nAccuracy: 85.71%\nPrecision: 40.91%\nRecall: 21.95%\n\n\n\n\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x25115f7c7c8&gt;\n\n\n\npng\n\n\ntn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\nprint(f\"True Pos: {tp}\" , f\"\\nFalse Neg: {fn}\", f\"\\nFalse Pos: {fp}\")\nTrue Pos: 9 \nFalse Neg: 32 \nFalse Pos: 13"
  },
  {
    "objectID": "blog/2020/05/12/index.html#biasing-the-model-towards-specificity",
    "href": "blog/2020/05/12/index.html#biasing-the-model-towards-specificity",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Biasing the model towards specificity",
    "text": "Biasing the model towards specificity\nThe model didn’t perform very well on specificity (a.k.a. recall), meaning that among the 41 individuals with cognitive impairment, the model was only able to correctly identify 22% of them.\nLet’s imagine a scenario in which we might want to bias the model towards high specificity:\nImagine the model is will be used to identify possible cases of dementia and when the model identifies a case of dementia they will be invited back for further testing to make a diagnosis with much higher precision. In this case, false positives would arguably carry a low risk because (let’s say for the sake of argument) further testing will be harmless and individuals who are healthy will be discovered as healthy. On the other side of the equation, let’s say that there is a high risk of negative consequences for anyone cases of dementia that go missed (and therefore untreated).\nThis is a case where we would want to optimize for detecting cases of dementia at risk of having an inflated false-positive rate.\nNow, one of the reasons for poor recall performance was the imbalanced classes used to train the data. That is to say, the impaired class had many fewer examples than the healthy class. I can correct for this using a weighting parameter when I instantiate the model, setting it to the ratio of healthy-to-impaired examples.\nimpaired = len(df2[df2['cognition_impaired'] == True])\nhealthy = len(df2[df2['cognition_impaired'] == False])\nimbalance_ratio = healthy / impaired\nprint(imbalance_ratio)\n6.720588235294118\nmodel = XGBClassifier(scale_pos_weight = imbalance_ratio)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n0.8444444444444444\npreds = model.predict(X_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, preds) * 100.0))\nprint(\"Precision: %.2f%%\" % (precision_score(y_test, preds) * 100.0))\nprint(\"Recall: %.2f%%\" % (recall_score(y_test, preds) * 100.0))\nplot_roc_curve(model, X_test, y_test)\nAccuracy: 84.44%\nPrecision: 39.47%\nRecall: 36.59%\n\n\n\n\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x25116152d08&gt;\n\n\n\npng\n\n\ntn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\nprint(f\"True Pos: {tp}\" , f\"\\nFalse Neg: {fn}\", f\"\\nFalse Pos: {fp}\")\nTrue Pos: 15 \nFalse Neg: 26 \nFalse Pos: 23"
  },
  {
    "objectID": "blog/2020/05/12/index.html#model-with-hyperparameter-tuning",
    "href": "blog/2020/05/12/index.html#model-with-hyperparameter-tuning",
    "title": "Modeling cognitive impairment using NHANES data",
    "section": "Model with hyperparameter tuning",
    "text": "Model with hyperparameter tuning\nI’ve managed to bias the model to improve its specificity. This has resulted in more true positives at the cost of more false positives. But I think I can improve this metric even further by searching through the “hyperparameter space” and considering alternative models that optimize for this metric. This is what I’ll do next using random search.\n# Parameter grid\ngbm_param_grid = {\n    'n_estimators': np.arange(20, 100, 10),\n    'max_depth': range(2, 10),\n    'colsample_bytree': np.arange(0.1, 1.0, 0.005),\n    'eta': np.arange(0.1, 1.0, 0.01),\n    'scale_pos_weight': [imbalance_ratio]\n}\n\n# Initialize regressor\ngbm = XGBClassifier()\n\n# Random search\nrandomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator=gbm, \n                                    scoring='recall', \n                                    n_iter=500, \n                                    cv=4, \n                                    random_state=42,\n                                    verbose=1)\n\n# Fit to data\nrandomized_mse.fit(X_train, y_train)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", randomized_mse.best_params_)\nFitting 4 folds for each of 500 candidates, totalling 2000 fits\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n\n\nBest parameters found:  {'scale_pos_weight': 6.720588235294118, 'n_estimators': 20, 'max_depth': 2, 'eta': 0.21999999999999995, 'colsample_bytree': 0.49500000000000033}\n\n\n[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:  2.1min finished\n\nTest performance\npreds = randomized_mse.predict(X_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, preds) * 100.0))\nprint(\"Precision: %.2f%%\" % (precision_score(y_test, preds) * 100.0))\nprint(\"Recall: %.2f%%\" % (recall_score(y_test, preds) * 100.0))\nplot_roc_curve(randomized_mse, X_test, y_test)\nAccuracy: 72.38%\nPrecision: 27.00%\nRecall: 65.85%\n\n\n\n\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x251159b4a88&gt;\n\n\n\npng\n\n\ntn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\nprint(f\"True Pos: {tp}\" , f\"\\nFalse Neg: {fn}\", f\"\\nFalse Pos: {fp}\")\nTrue Pos: 27 \nFalse Neg: 14 \nFalse Pos: 73"
  },
  {
    "objectID": "blog/2023/08/25/index.html",
    "href": "blog/2023/08/25/index.html",
    "title": "Using data normalization to better compare change over time in regions with different population sizes",
    "section": "",
    "text": "For this post, I’ll be using the Week 34 Tidy Tuesday dataset, which contains data on refugee movement around the world. I want to look at the change in refugee outflows over time in different nations, and see if I can identify countries with meaningfully large increases in refugee outflows.\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(gghighlight)\ndf &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-22/population.csv')\n\nData cleaning\nFirst, some data cleaning.\nTo keep things simple, I’m only going to keep nations that had refugee data for all of the 13 years spanning 2010-2022.\n\n(df %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(n_years = n_distinct(year)) %&gt;%\n  filter(n_years == 13))$coo_name -&gt; coo_to_keep\n\ndf %&gt;%\n  filter(coo_name %in% coo_to_keep) %&gt;%\n  select(coo_name,\n         coo_iso,\n         year,\n         refugees) -&gt; df_clean\n\nNormalization\nNext, to make comparisons between nations more apples-apples, I’m going to do some normalization.\nI want to normalize in terms of population size and change over baseline.\nFirst, I’ll fetch population data from World Bank using wbstats.\n\nwb_search(\"SP.POP.TOTL\", fields='indicator_id') %&gt;%\n  head(1)\n\n# A tibble: 1 × 3\n  indicator_id indicator         indicator_desc                                 \n  &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;                                          \n1 SP.POP.TOTL  Population, total Total population is based on the de facto defi…\n\n\n\npops &lt;- wb_data(\"SP.POP.TOTL\", start_date = 2010, end_date = 2022) %&gt;%\n  select(iso3c, date, \"SP.POP.TOTL\") %&gt;%\n  rename(pop = \"SP.POP.TOTL\",\n         iso = iso3c)\n\ndf_clean %&gt;%\n  left_join(pops, by=c('coo_iso'='iso', 'year'='date')) -&gt; df_enriched\n\ndf_enriched %&gt;%\n  head()\n\n# A tibble: 6 × 5\n  coo_name               coo_iso  year refugees        pop\n  &lt;chr&gt;                  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan            AFG      2010        0   28189672\n2 Iran (Islamic Rep. of) IRN      2010       30   75373855\n3 Iraq                   IRQ      2010        6   31264875\n4 Pakistan               PAK      2010     6398  194454498\n5 Egypt                  EGY      2010        5   87252413\n6 China                  CHN      2010        6 1337705000\n\n\nNext, I’ll compute a new variable: refugees_per_1k_pop that represents refugees leaving per 1000 persons in the original population. This is a good way to normalize, because we’d expect a larger count of refugees leaving from countries that had more people to begin with.\n\ndf_enriched %&gt;%\n  group_by(year, coo_name, coo_iso) %&gt;%\n  summarize(refugees = sum(refugees),\n            pop = first(pop)) %&gt;%\n  mutate(refugees_per_1k_pop = refugees/(pop/1000)) -&gt; df_enriched\n\ndf_enriched %&gt;%\n  head()\n\n# A tibble: 6 × 6\n# Groups:   year, coo_name [6]\n   year coo_name            coo_iso refugees      pop refugees_per_1k_pop\n  &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;               &lt;dbl&gt;\n1  2010 Afghanistan         AFG      3054699 28189672            108.    \n2  2010 Albania             ALB        14771  2913021              5.07  \n3  2010 Algeria             DZA         6665 35856344              0.186 \n4  2010 Angola              AGO       134851 23364185              5.77  \n5  2010 Antigua and Barbuda ATG           28    85695              0.327 \n6  2010 Argentina           ARG          553 40788453              0.0136\n\n\nI’ll do a bit of cleaning again, to remove those nations for whom I didn’t have a complete record of population data, and so couldn’t calculate refugees_per_1k_pop for every year.\n\n(df_enriched %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(n_years = sum(refugees_per_1k_pop &gt; 0, na.rm=T)) %&gt;%\n  filter(n_years == 13))$coo_name -&gt; coo_to_keep_2\n\ndf_enriched %&gt;%\n  filter(coo_name %in% coo_to_keep_2)-&gt; df_enriched_clean\n\nNext, I’ll use 2010 as a baseline year, and subtract each year’s value from that. This will allow me to measure change over time from this common baseline, and compare nations in terms of a normalized change.\n\ndf_enriched_clean %&gt;%\n  filter(year == 2010) %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(baseline_refugees_per_1k_pop = sum(refugees)/(first(pop)/1000)) -&gt; baseline_year\n\ndf_enriched_clean %&gt;%\n  left_join(baseline_year, by='coo_name') %&gt;%\n  mutate(change_from_baseline = refugees_per_1k_pop - baseline_refugees_per_1k_pop) -&gt; df_enriched_clean\n\ndf_enriched_clean %&gt;%\n  head()\n\n# A tibble: 6 × 8\n# Groups:   year, coo_name [6]\n   year coo_name            coo_iso refugees      pop refugees_per_1k_pop\n  &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;               &lt;dbl&gt;\n1  2010 Afghanistan         AFG      3054699 28189672            108.    \n2  2010 Albania             ALB        14771  2913021              5.07  \n3  2010 Algeria             DZA         6665 35856344              0.186 \n4  2010 Angola              AGO       134851 23364185              5.77  \n5  2010 Antigua and Barbuda ATG           28    85695              0.327 \n6  2010 Argentina           ARG          553 40788453              0.0136\n# ℹ 2 more variables: baseline_refugees_per_1k_pop &lt;dbl&gt;,\n#   change_from_baseline &lt;dbl&gt;\n\n\nIdentifying regions of interest\nNext, I want to identify a smaller set of “interesting” COOs that have experienced large increases over the baseline. I’ll identify an upper bound percentile of max change over baseline, and then I’ll use a value that approximates that as a filter. This gives me 4 “interesting” nations.\n\ndf_enriched_clean %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(max_change_from_baseline = max(change_from_baseline)) %&gt;%\n  summarize(p90_change = quantile(max_change_from_baseline, .975, na.rm=T))\n\n# A tibble: 1 × 1\n  p90_change\n       &lt;dbl&gt;\n1       30.0\n\ndf_enriched_clean %&gt;%\n  group_by(coo_name, coo_iso) %&gt;%\n  summarize(max_change_from_baseline = max(change_from_baseline),\n            last_value = last(change_from_baseline, order_by=year)) %&gt;%\n  filter(max_change_from_baseline &gt; 32) -&gt; coos_with_large_changes_over_baseline\n\n`summarise()` has grouped output by 'coo_name'. You can override using the\n`.groups` argument.\n\ncoos_with_large_changes_over_baseline\n\n# A tibble: 4 × 4\n# Groups:   coo_name [4]\n  coo_name             coo_iso max_change_from_baseline last_value\n  &lt;chr&gt;                &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt;\n1 Central African Rep. CAF                         99.8       98.7\n2 Eritrea              ERI                         76.9       67.3\n3 Syrian Arab Rep.     SYR                        343.       295. \n4 Ukraine              UKR                        149.       149. \n\n\nData visualization\nFinally, I’ll plot change over the 2010 baseline (in refugees per 1k population), and highlight the 4 interesting nations identified above.\nI’ll use this to help me pick colors for the ggtitle text.\n\nscales::show_col(scales::hue_pal()(4))\n\n\n\n\n\n\n\n\ndf_enriched_clean %&gt;%\n  mutate(class = coo_name %in% coos_with_large_changes_over_baseline$coo_name,\n         year = as.Date(paste0(as.character(year), '-01-01'))) %&gt;%\n  arrange(year, desc(class)) %&gt;%\n  mutate(coo_iso = fct_inorder(coo_iso)) %&gt;%\n  ggplot(aes(x=year, y=change_from_baseline, color=coo_iso)) +\n    geom_line() +\n    scale_x_date(date_labels=\"%Y\", date_breaks=\"1 year\") +\n    ggthemes::theme_solarized() +\n    gghighlight::gghighlight(class == TRUE) +\n    ggtitle(\"&lt;strong&gt;&lt;span style='color:#00BFC4'&gt;SYR&lt;/span&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;span style='color:#C77CFF'&gt;UKR&lt;/span&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;span style='color:#F8766D'&gt;CAF&lt;/span&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;span style='color:#7CAE00'&gt;ERI&lt;/span&gt;&lt;/strong&gt; experienced large increases in&lt;br&gt;normalized refugee outflow (i.e., refugees per 1k population),&lt;br&gt; compared to their 2010 baseline.\") +\n    xlab('Year') +\n    ylab('Change in Normalized Refugee Outflow*') +\n    labs(caption = \"&lt;span style='font-size:7pt'&gt;*Change in refugees per 1k population from the baseline value observed in 2010.&lt;/span&gt;\") +\n    theme(plot.title = ggtext::element_markdown(),\n          plot.caption = ggtext::element_markdown()) -&gt; plot\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\nplot"
  },
  {
    "objectID": "blog/2023/09/08/index.html",
    "href": "blog/2023/09/08/index.html",
    "title": "Using random forest based outlier detection to clean a training dataset",
    "section": "",
    "text": "For this blog post, I will be tackling the Kaggle compettition Improve a Fixed Model the Data-Centric Way!\nThis is the challenge of the competition: Improve a dataset that is being used to train a random forest model. The model is fixed, so model performance can only be improved by modifying the dataset. In terms of dataset modifications, there are some additional limitations:\n\nRows can be removed, but not added\nColumns cannot be removed or added\nValues in the dataset that are used to train the model can be transformed, but those transformations will not be applied to the validation dataset (which is held out until the challenge comes to an end)\n\nThis means that many of the tools available to a data scientist for improving an ML model, such as hyperparameter tuning, or data pre-processing applied to both training and test/validation datasets, are not available. The best options, therefore, will be to find ways to clean the training dataset that will yield better performance in the untouched validation dataset.\nIn this post, I will explore whether the training data can be improved using multivariate outlier imputation and by reducing feature multicollinearity."
  },
  {
    "objectID": "blog/2023/09/08/index.html#model-pipeline",
    "href": "blog/2023/09/08/index.html#model-pipeline",
    "title": "Using random forest based outlier detection to clean a training dataset",
    "section": "Model pipeline",
    "text": "Model pipeline\nBefore doing anything else, I want to create a model pipeline function and establish a baseline of model performance. This pipeline and baseline model will allow me to quickly iterate, test, and benchmark changes to the training dataset.\n\nfit_ranger_cv &lt;- function(train, test, model_name){\n  set.seed(42)\n  model_ranger &lt;-\n    # The parameters here mirror those that\n    # will be used in the competition model\n    rand_forest(trees = 1000, \n                min_n = 7) %&gt;% \n    set_engine(\"ranger\") %&gt;% \n    set_mode(\"regression\")\n  \n  workflow_ranger &lt;- \n    workflow() %&gt;% \n    add_formula(target ~ .) %&gt;% \n    add_model(model_ranger)\n  \n  folds &lt;- vfold_cv(train, v = 5)\n  \n  fit_ranger &lt;- \n    workflow_ranger %&gt;% \n    fit_resamples(folds)\n  \n  # Get in-sample performance over resamples\n  round((fit_ranger %&gt;%\n    collect_metrics() %&gt;%\n    filter(.metric == 'rmse'))$mean, 3) -&gt; train_perf\n  \n  # Evaluate performance on out-of-sample (test) data\n  ranger_fit &lt;- \n    workflow_ranger %&gt;%\n    fit(train)\n  \n  round(rmse(test$target, predict(ranger_fit, test)$.pred), 3) -&gt; test_perf\n  \n  # Combine in-sample and out-of-sample into a dataframe\n  df_perf &lt;- data.frame(model = model_name,\n                        train_perf = train_perf,\n                        test_perf = test_perf)\n  return(df_perf)\n}"
  },
  {
    "objectID": "blog/2023/09/08/index.html#baseline-performance",
    "href": "blog/2023/09/08/index.html#baseline-performance",
    "title": "Using random forest based outlier detection to clean a training dataset",
    "section": "Baseline performance",
    "text": "Baseline performance\n\nbaseline_fit &lt;- fit_ranger_cv(train %&gt;% select(-id), test %&gt;% select(-id), 'baseline')\nbaseline_fit\n\n     model train_perf test_perf\n1 baseline      1.231     2.134\n\n\nI’m going to run that again so I can be sure the RNG seed is set properly and the results are reproducible – otherwise I’ll be chasing a moving target!\n\nbaseline_fit &lt;- fit_ranger_cv(train %&gt;% select(-id), test %&gt;% select(-id), 'baseline')\nbaseline_fit\n\n     model train_perf test_perf\n1 baseline      1.231     2.134"
  },
  {
    "objectID": "blog/2023/09/08/index.html#outlier-detection",
    "href": "blog/2023/09/08/index.html#outlier-detection",
    "title": "Using random forest based outlier detection to clean a training dataset",
    "section": "1. Outlier detection",
    "text": "1. Outlier detection\nUnivariate outlier detection\nOne simple univariate outlier detection method involves a “Z-score threshold”. In a normally distributed dataset, 99% of values will tend to fall between a Z-score of -3 to +3. This is why a Z-score threshold of +/- 3 is often used to identify outliers in practice.\nFor example, here’s a plot of the percentage of outliers found for each variable. I can see that some variables contained more outliers than others. In particular, there were 6 features with more than 3% extreme values.\n\ntrain %&gt;%\n  select(-id) %&gt;%\n  outliers::scores(type=\"z\") %&gt;%\n  pivot_longer(everything()) %&gt;%\n  group_by(name) %&gt;%\n  summarize(n_outlier = sum(abs(value) &gt; 3),\n            pct_outlier = round(sum(abs(value) &gt; 3)/n()*100,2)) %&gt;%\n  arrange(pct_outlier) -&gt; train_pct_outlier\n\ntrain_pct_outlier %&gt;%\n  ggplot(aes(x = fct_inorder(name), y = pct_outlier)) +\n    geom_bar(stat='identity') +\n    coord_flip() +\n    xlab(\"Feature\") +\n    ylab(\"% values exceeding 3 z-score threshold\") +\n    geom_hline(yintercept = 3)\n\n\n\n\n\n\ntrain_pct_outlier %&gt;%\n  filter(pct_outlier &gt; 3) %&gt;%\n  arrange(desc(pct_outlier))\n\n# A tibble: 6 × 3\n  name   n_outlier pct_outlier\n  &lt;chr&gt;      &lt;int&gt;       &lt;dbl&gt;\n1 NH4_7        137        5.22\n2 NH4_1        122        4.65\n3 BOD5_3       113        4.3 \n4 O2_2         111        4.23\n5 NO2_1        104        3.96\n6 NH4_2         99        3.77\n\n\nOther options for univariate outlier detection include using the inter-quartile range (IQR) or percentile-based thresholds.\nMultivariate outlier detection\nAnother option is multivariate outlier detection. For multivariate outlier detection, random forest based methods have been growing in popularity within the data science community. There are two methods that I’ll consider here.\nIsolation forest\nThe “isolation forest” works by trying to identify variables that can be isolated in branches when randomly splitting the data. From the isotree package documentation:\n\nIsolation Forest is an algorithm originally developed for outlier detection that consists in splitting sub-samples of the data according to some attribute/feature/column at random. The idea is that, the rarer the observation, the more likely it is that a random uniform split on some feature would put outliers alone in one branch, and the fewer splits it will take to isolate an outlier observation like this.\n\nImportantly, this method operates at the row level, allowing us to identify anomalous records, but not pinpoint specifically which features on which those records may have been outliers.\n\nlibrary(isotree)\nisofor &lt;- isolation.forest(train %&gt;% select(-id), ntrees = 500, nthreads = 4)\niso_preds &lt;- predict(isofor, train %&gt;% select(-id))\ntrain[which.max(iso_preds), ]\n\n# A tibble: 1 × 37\n     id target  O2_1  O2_2  O2_3  O2_4  O2_5  O2_6  O2_7 NH4_1 NH4_2 NH4_3 NH4_4\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2662   15.9  15.9  14.9  8.98  6.17  2.28  8.98  7.15 0.573  0.54 0.208   2.3\n# ℹ 24 more variables: NH4_5 &lt;dbl&gt;, NH4_6 &lt;dbl&gt;, NH4_7 &lt;dbl&gt;, NO2_1 &lt;dbl&gt;,\n#   NO2_2 &lt;dbl&gt;, NO2_3 &lt;dbl&gt;, NO2_4 &lt;dbl&gt;, NO2_5 &lt;dbl&gt;, NO2_6 &lt;dbl&gt;,\n#   NO2_7 &lt;dbl&gt;, NO3_1 &lt;dbl&gt;, NO3_2 &lt;dbl&gt;, NO3_3 &lt;dbl&gt;, NO3_4 &lt;dbl&gt;,\n#   NO3_5 &lt;dbl&gt;, NO3_6 &lt;dbl&gt;, NO3_7 &lt;dbl&gt;, BOD5_1 &lt;dbl&gt;, BOD5_2 &lt;dbl&gt;,\n#   BOD5_3 &lt;dbl&gt;, BOD5_4 &lt;dbl&gt;, BOD5_5 &lt;dbl&gt;, BOD5_6 &lt;dbl&gt;, BOD5_7 &lt;dbl&gt;\n\n\noutForest\nThe outForest package implements a different random forest method of outlier detection in which each variable is regressed onto all others, and outliers are detected based on the difference between the observed value and the out-of-bag predicted value. This has the advantage of identifying outliers on both a row and column basis, providing more flexibility in terms of how outliers can be dealt with.\n\nlibrary(outForest)\nset.seed(42)\noutfor &lt;- outForest(train %&gt;% select(-id), \n                    verbose = 0)\noutForest::outliers(outfor) %&gt;%\n  select(-rmse, -threshold) %&gt;%\n  head()\n\n      row    col observed  predicted    score replacement\n453  2003  NH4_5  3026.00 14.0353304 50.63951      12.175\n811  1021  NO2_3     2.05  0.1152134 25.71619       0.064\n4     237 target    40.78 11.1135703 23.52858       8.100\n1315 2556 BOD5_4    55.40  5.8114438 21.74855       5.800\n340  2149  NH4_1     4.20  0.4161047 21.17629       0.360\n1327  241 BOD5_5    82.45  9.2994377 20.80525       8.400\n\n\nBelow we can see the outliers identified for each variable and how anomalous those outliers were. Using this data, I can then choose a threshold and replace anomalous values by imputation. The outForest package provides different methods of imputation out of the box, defaulting to predictive mean matching.\nI’ll use this method for detection because it’s multivariate, intuitive, and flexible.\n\nplot(outfor, what = \"scores\")"
  },
  {
    "objectID": "blog/2023/09/08/index.html#outlier-removal-or-imputation",
    "href": "blog/2023/09/08/index.html#outlier-removal-or-imputation",
    "title": "Using random forest based outlier detection to clean a training dataset",
    "section": "2. Outlier removal or imputation",
    "text": "2. Outlier removal or imputation\nNow that I’ve decided on an outlier detection method, the next step is to decide what to do about the outliers. There’s two main ways outliers can be handled: Removal or imputation. Removal is often an extreme measure that can lead to information loss, so I tend to prefer imputation over removal.\nSince I’ve decided to use outForest, I can also use its out-of-the-box imputation methods.\nFirst, I’ll go back to the dataframe containing the outliers that it had detected and try to refine the threshold. By default, it was using a score threshold of 3. But I’m not comfortable with imputing so many values. My gut tells me that if I’m identifying more than 3-5% of the records as outliers, then my threshold is too low and I’m catching too many potentially legitimate values.\nHere I can see that a score threshold of 8 yields around 2% outliers on a row-basis. That seems more reasonable\n\nround((nrow(outForest::outliers(outfor) %&gt;%\n        select(-replacement, -rmse, -threshold) %&gt;%\n        filter(abs(score) &gt; 8) %&gt;%\n        distinct(row))/nrow(sample_submission)*100), 1)\n\n[1] 1.9\n\n\nUsing this threshold, I will now impute the values.\n\nset.seed(42)\noutfor2 &lt;- outForest(train %&gt;% select(-id), \n                      verbose = 0, \n                      replace = \"pmm\",\n                      threshold = 8)\ntrain_outlier_adjusted &lt;- outfor2$Data\n\nHere I can see one of the outliers previously identified, and the value that was imputed for it.\n\ntrain[1021,]$NO2_3\n\n[1] 2.05\n\ntrain_outlier_adjusted[1021,]$NO2_3\n\n[1] 0.064\n\n\nAnd now I can quickly run the random forest model again, with the new imputed training dataset, and compare it against the baseline model.\nI see that outlier imputation has improved in-sample performance considerably (which is to be expected since the training dataset on which the cross-validation was performed is now much cleaner!), but it actually had a fairly marginal impact on out-of-sample performance.\n\nranger_fit_1 &lt;- fit_ranger_cv(train_outlier_adjusted, test %&gt;% select(-id), 'outliers imputed')\n\nranger_fit_1 %&gt;%\n  mutate(train_pct_improved = round((baseline_fit$train_perf-train_perf)/baseline_fit$train_perf*100, 2),\n         test_pct_improved = round((baseline_fit$test_perf-test_perf)/baseline_fit$test_perf*100, 2)) %&gt;%\n  bind_rows(baseline_fit) -&gt; ranger_fit_1\n\nranger_fit_1\n\n             model train_perf test_perf train_pct_improved test_pct_improved\n1 outliers imputed      1.080     2.128              12.27              0.28\n2         baseline      1.231     2.134                 NA                NA"
  },
  {
    "objectID": "blog/2023/10/08/index.html",
    "href": "blog/2023/10/08/index.html",
    "title": "Using GPT-4 for classification",
    "section": "",
    "text": "GPT is a powerful new tool in the Data Science toolkit. Used correctly, it can increase productivity while decreasing the “drudgery” of boring tasks like data cleaning. I’ve been trying to find ways to integrate GPT into my data science workflow, and I thought it might be fun to use it with the latest Tidy Tuesday. (US Government Grant Opportunities). So in this blog post, I use GPT to classify US grant-funding agencies into categories using government agency names."
  },
  {
    "objectID": "blog/2023/10/08/index.html#ask-it-to-classify-agencies-using-agency-names-and-categories",
    "href": "blog/2023/10/08/index.html#ask-it-to-classify-agencies-using-agency-names-and-categories",
    "title": "Using GPT-4 for classification",
    "section": "2. Ask it to classify agencies using agency names and categories",
    "text": "2. Ask it to classify agencies using agency names and categories\nNext, using the agency names and categories generated above I’ll ask GPT to do classification. First, I’ll define a function that takes a category as input and runs a query against GPT asking it to put the agencies into the provided category, returning a list of agencies that it put in that category. Although I could ask GPT to classify agencies using all categories in a single query, I expect this would be less reliable than tackling each category separately. One of the trade-offs here is that I won’t get mutual exclusivity of the categories, since GPT won’t know how it’s already classified agencies. For this analysis, let’s assume I don’t want mutual exclusivity.\n\nget_agencies_in_category &lt;- function(category) {\n  query &lt;-paste0(\n    \"I'm going to present a comma-separated list of US agencies,\n    delimited by triple backticks (```), and a single category to \n    which some of the agencies belong, delimited by triple hashtags (###). \n    Your task is to provide a comma-separated list of agency names that \n    belong in the category provided.\n    ```\", agencies_str,  \"```\n    ###\", category, \"###\"\n  )\n  results &lt;- chatGPT(query)\n  \n  # Return as a vector\n  trimws(strsplit(results, \",\")[[1]])\n}\n\nUsing the function above, I’ll query GPT for each of the categories, saving the results into a list called agencies_categorized.\n\nif(!exists('agencies_categorized')){\n  agencies_categorized &lt;- list()\n  for (category in categories){\n    if(category != 'Other'){\n      agencies_categorized[[category]] &lt;- get_agencies_in_category(category) \n    }\n  }\n  \n  # Save to disk\n  save(agencies_categorized, file = 'agencies_categorized.Rdata')\n}\n  \nprint(agencies_categorized)\n\n$`Defense and Military`\n [1] \"69A345 Office of the Under Secretary for Policy\"   \n [2] \"69A350 OSDBU\"                                      \n [3] \"69A355 Research and Technology\"                    \n [4] \"ACC APG - Natick\"                                  \n [5] \"ACC-APG-Aberdeen Division A\"                       \n [6] \"ACC-APG-Belvoir\"                                   \n [7] \"ACC-APG-Detrick\"                                   \n [8] \"ACC-APG-Edgewood\"                                  \n [9] \"ACC-APG-Fort Huachuca\"                             \n[10] \"Air Force -- Materiel Command\"                     \n[11] \"Air Force -- Research Lab\"                         \n[12] \"Air Force Academy\"                                 \n[13] \"Air Force Office of Scientific Research\"           \n[14] \"Alaska District\"                                   \n[15] \"Army Contracting Command - Benet Laboratories\"     \n[16] \"Army Contracting Command - New Jersey\"             \n[17] \"Army Contracting Command Rock Island\"              \n[18] \"Bureau of Diplomatic Security\"                     \n[19] \"Bureau of International Security-Nonproliferation\" \n[20] \"Bureau of Political-Military Affairs - GPI\"        \n[21] \"Bureau of Political-Military Affairs - WRA\"        \n[22] \"DARPA - Biological Technologies Office\"            \n[23] \"DARPA - Defense Sciences Office\"                   \n[24] \"DARPA - Information Innovation Office\"             \n[25] \"DARPA - Information Processing Technology Office\"  \n[26] \"DARPA - Microsystems Technology Office\"            \n[27] \"DARPA - Strategic Technology Office\"               \n[28] \"DARPA - Tactical Technology Office\"                \n[29] \"DARPA - Transformational Convergence Technology\"   \n[30] \"DARPA-MTO-BAA-09-25\"                               \n[31] \"Defense Advanced Research Projects Agency\"         \n[32] \"Defense Health Agency\"                             \n[33] \"Defense Intelligence Agency\"                       \n[34] \"Defense Logistics Agency\"                          \n[35] \"Defense Threat Reduction Agency\"                   \n[36] \"Department of Defense\"                             \n[37] \"Dept of the Army -- Materiel Command\"              \n[38] \"Dept. of the Army  --  Corps of Engineers\"         \n[39] \"Dept. of the Army -- Space & Missle Defense Comman\"\n[40] \"Dept. of the Army -- USAMRAA\"                      \n[41] \"DoD Education Activity\"                            \n[42] \"DOT - FAA Aviation Research Grants\"                \n[43] \"DOT - FAA Centers of Excellence\"                   \n[44] \"DOT Federal Aviation Administration\"               \n[45] \"Engineer Research and Development Center\"          \n[46] \"Fort Worth District\"                               \n[47] \"Kansas City District\"                              \n[48] \"Missile Defense Agency\"                            \n[49] \"Mission and Install. Cmd. JBSA Ft. Sam Houston\"    \n[50] \"NAVAIR\"                                            \n[51] \"Naval Air Warfare Center Aircraft Div. Lakehurst\"  \n[52] \"NAVAL FACILITIES ENGINEERING COMMAND\"              \n[53] \"Naval Facilities Engineering Command Southwest\"    \n[54] \"Naval Information Warfare Center Pacific\"          \n[55] \"NAVAL MEDICAL LOGISTICS COMMAND\"                   \n[56] \"Naval Research Laboratory\"                         \n[57] \"Naval Supply Systems Command\"                      \n[58] \"Naval Surface Warfare Center - Carderock\"          \n[59] \"NAVFAC Atlantic\"                                   \n[60] \"NAVFAC Washington DC\"                              \n[61] \"NCA Contracting\"                                   \n[62] \"NEA Cooperative Agreements Office\"                 \n[63] \"NSWC - CRANE\"                                      \n[64] \"NSWC Indian Head\"                                  \n[65] \"NUWC Division Keyport\"                             \n[66] \"Office of Local Defense Community Cooperation\"     \n[67] \"Office of Naval Research\"                          \n[68] \"Office of the Director of National Intelligence\"   \n[69] \"Omaha District\"                                    \n[70] \"Savannah District\"                                 \n[71] \"Seattle District\"                                  \n[72] \"SPAWAR SYSTEMS CENTER\"                             \n[73] \"U.S. Dept. of Treasury RESTORE Act Program\"        \n[74] \"U.S. Mission to NATO\"                              \n[75] \"United States Coast Guard\"                         \n[76] \"United States Marine Corps\"                        \n[77] \"USAF 347 Contracting Squadron\"                     \n[78] \"Washington Headquarters Services\"                  \n\n$`Science and Technology`\n [1] \"69A345 Office of the Under Secretary for Policy\"   \n [2] \"69A355 Research and Technology\"                    \n [3] \"Advanced Research Projects Agency Energy\"          \n [4] \"Advanced Research Projects Agency for Health\"      \n [5] \"Agency for Health Care Research and Quality\"       \n [6] \"Agricultural Research Service\"                     \n [7] \"Air Force -- Research Lab\"                         \n [8] \"Air Force Office of Scientific Research\"           \n [9] \"Army Contracting Command - Benet Laboratories\"     \n[10] \"Bureau of Oceans - Int. Environmental - Scientific\"\n[11] \"Centers for Disease Control - ATSDR\"               \n[12] \"Centers for Disease Control - CFA\"                 \n[13] \"Centers for Disease Control - CGH\"                 \n[14] \"Centers for Disease Control - CSELS\"               \n[15] \"Centers for Disease Control - NCBDDD\"              \n[16] \"Centers for Disease Control - NCCDPHP\"             \n[17] \"Centers for Disease Control - NCEH\"                \n[18] \"Centers for Disease Control - NCEZID\"              \n[19] \"Centers for Disease Control - NCHHSTP\"             \n[20] \"Centers for Disease Control - NCHS\"                \n[21] \"Centers for Disease Control - NCIPC\"               \n[22] \"Centers for Disease Control - NCIRD\"               \n[23] \"Centers for Disease Control - NIOSH\"               \n[24] \"Centers for Disease Control - OD\"                  \n[25] \"Centers for Disease Control - OPHPR\"               \n[26] \"Centers for Disease Control - OSTLTS\"              \n[27] \"Centers for Disease Control and Prevention\"        \n[28] \"Centers for Disease Control and Prevention - ERA\"  \n[29] \"DARPA - Biological Technologies Office\"            \n[30] \"DARPA - Defense Sciences Office\"                   \n[31] \"DARPA - Information Innovation Office\"             \n[32] \"DARPA - Information Processing Technology Office\"  \n[33] \"DARPA - Microsystems Technology Office\"            \n[34] \"DARPA - Strategic Technology Office\"               \n[35] \"DARPA - Tactical Technology Office\"                \n[36] \"DARPA - Transformational Convergence Technology\"   \n[37] \"Defense Advanced Research Projects Agency\"         \n[38] \"Defense Health Agency\"                             \n[39] \"Defense Intelligence Agency\"                       \n[40] \"Defense Threat Reduction Agency\"                   \n[41] \"Department of Energy\"                              \n[42] \"DoD Education Activity\"                            \n[43] \"DOT - FAA Aviation Research Grants\"                \n[44] \"DOT - FAA Centers of Excellence\"                   \n[45] \"DOT Federal Aviation Administration\"               \n[46] \"Economic Research Service\"                         \n[47] \"Engineer Research and Development Center\"          \n[48] \"Environmental Protection Agency\"                   \n[49] \"FAA - Aviation Next Gen\"                           \n[50] \"FAA-COE-AJFE\"                                      \n[51] \"FAA-COE-GA\"                                        \n[52] \"FAA-COE-TTHP\"                                      \n[53] \"Food and Drug Administration\"                      \n[54] \"Geological Survey\"                                 \n[55] \"Health Resources and Services Administration\"      \n[56] \"Indian Health Service\"                             \n[57] \"Institute of Museum and Library Services\"          \n[58] \"National Aeronautics and Space Administration\"     \n[59] \"National Energy Technology Laboratory\"             \n[60] \"National Geospatial-Intelligence Agency\"           \n[61] \"National Highway Traffic Safety Administration\"    \n[62] \"National Institute of Food and Agriculture\"        \n[63] \"National Institute of Justice\"                     \n[64] \"National Institute of Standards and Technology\"    \n[65] \"National Institutes of Health\"                     \n[66] \"National Oceanic and Atmospheric Administration\"   \n[67] \"National Park Service\"                             \n[68] \"National Science Foundation\"                       \n[69] \"National Telecommunications and Information Admini\"\n[70] \"Naval Research Laboratory\"                         \n[71] \"Naval Supply Systems Command\"                      \n[72] \"Naval Surface Warfare Center - Carderock\"          \n[73] \"NNSA\"                                              \n[74] \"Nuclear Regulatory Commission\"                     \n[75] \"NUWC Division Keyport\"                             \n[76] \"Oak Ridge Office\"                                  \n[77] \"Office of Science\"                                 \n[78] \"Office of the Assistant Secretary for Health\"      \n[79] \"Office of the Director of National Intelligence\"   \n[80] \"Office of the National Coordinator\"                \n[81] \"Pipeline and Hazardous Materials Safety Admin\"     \n[82] \"Risk Management Agency\"                            \n[83] \"Science and Technology Directorate\"                \n[84] \"Substance Abuse and Mental Health Services Admin\"  \n[85] \"Substance Abuse and Mental Health Services Adminis\"\n[86] \"Uniformed Services Univ. of the Health Sciences\"   \n[87] \"USUHS Medical - Non- Research Projects\"            \n[88] \"USUHS Medical Research Projects\"                   \n[89] \"USAID\"                                             \n[90] \"Woodrow Wilson Center\"                             \n\n$`Health and Medicine`\n [1] \"Administration for Children & Families - ACYF/FYSB\"\n [2] \"Administration for Children and Families\"          \n [3] \"Administration for Children and Families - ACYF/CB\"\n [4] \"Administration for Children and Families - ANA\"    \n [5] \"Administration for Children and Families - OCC\"    \n [6] \"Administration for Children and Families - OCS\"    \n [7] \"Administration for Children and Families - OCSE\"   \n [8] \"Administration for Children and Families - OFA\"    \n [9] \"Administration for Children and Families - OHS\"    \n[10] \"Administration for Children and Families - OHSEPR\" \n[11] \"Administration for Children and Families - OPRE\"   \n[12] \"Administration for Children and Families - ORR\"    \n[13] \"Administration for Children and Families-IOAS-OTIP\"\n[14] \"Administration for Community Living\"               \n[15] \"Administration on Aging\"                           \n[16] \"Advanced Research Projects Agency for Health\"      \n[17] \"Agency for Health Care Research and Quality\"       \n[18] \"Centers for Disease Control - ATSDR\"               \n[19] \"Centers for Disease Control - CFA\"                 \n[20] \"Centers for Disease Control - CGH\"                 \n[21] \"Centers for Disease Control - CSELS\"               \n[22] \"Centers for Disease Control - NCBDDD\"              \n[23] \"Centers for Disease Control - NCCDPHP\"             \n[24] \"Centers for Disease Control - NCEH\"                \n[25] \"Centers for Disease Control - NCEZID\"              \n[26] \"Centers for Disease Control - NCHHSTP\"             \n[27] \"Centers for Disease Control - NCHS\"                \n[28] \"Centers for Disease Control - NCIPC\"               \n[29] \"Centers for Disease Control - NCIRD\"               \n[30] \"Centers for Disease Control - NIOSH\"               \n[31] \"Centers for Disease Control - OD\"                  \n[32] \"Centers for Disease Control - OPHPR\"               \n[33] \"Centers for Disease Control - OSTLTS\"              \n[34] \"Centers for Disease Control and Prevention\"        \n[35] \"Centers for Disease Control and Prevention - ERA\"  \n[36] \"Centers for Medicare & Medicaid Services\"          \n[37] \"CMS Consumer Operated and Oriented Plan Program\"   \n[38] \"CMS-Consumer Information & Insurance Oversight\"    \n[39] \"Defense Health Agency\"                             \n[40] \"Department of Health and Human Services\"           \n[41] \"Food and Drug Administration\"                      \n[42] \"Health Resources and Services Administration\"      \n[43] \"Indian Health Service\"                             \n[44] \"National Institute of Food and Agriculture\"        \n[45] \"National Institutes of Health\"                     \n[46] \"Office of the Assistant Secretary for Health\"      \n[47] \"Office of the National Coordinator\"                \n[48] \"Substance Abuse and Mental Health Services Admin\"  \n[49] \"Substance Abuse and Mental Health Services Adminis\"\n[50] \"Uniformed Services Univ. of the Health Sciences\"   \n[51] \"USUHS Medical - Non- Research Projects\"            \n[52] \"USUHS Medical Research Projects\"                   \n[53] \"VA Office of Mental Health\"                        \n[54] \"VHA Member Services-Veterans Transportation\"       \n\n$`International Aid`\n [1] \"Afghanistan USAID-Kabul\"                        \n [2] \"Africa Regional Services\"                       \n [3] \"Agency for International Development\"           \n [4] \"Albania USAID-Tirana\"                           \n [5] \"Armenia USAID-Yerevan\"                          \n [6] \"Azerbaijan USAID-Baku\"                          \n [7] \"Bangladesh USAID-Dhaka\"                         \n [8] \"Benin USAID-Cotonou\"                            \n [9] \"Bolivia USAID-La Paz\"                           \n[10] \"Bosnia USAID-Herzegovina\"                       \n[11] \"Brazil USAID-Brasilia\"                          \n[12] \"Burma USAID - Rangoon\"                          \n[13] \"Burundi USAID-Bujumbura\"                        \n[14] \"Cambodia USAID-Phnom Penh\"                      \n[15] \"Colombia USAID-Bogota\"                          \n[16] \"Democratic Republic of the Congo USAID-Kinshasa\"\n[17] \"Dominican Republic USAID-Santo Domingo\"         \n[18] \"East Africa USAID-Kenya\"                        \n[19] \"Ecuador USAID-Quito\"                            \n[20] \"Egypt USAID-Cairo\"                              \n[21] \"El Salvador USAID-San Salvador\"                 \n[22] \"Ethiopia USAID-Addis Ababa\"                     \n[23] \"Georgia USAID-Tbilisi\"                          \n[24] \"Ghana USAID-Accra\"                              \n[25] \"Guatemala USAID-Guatemala City\"                 \n[26] \"Guinea USAID-Conakry\"                           \n[27] \"Haiti USAID-Port Au Prince\"                     \n[28] \"Honduras USAID-Tegucigalpa\"                     \n[29] \"Hungary USAID-Budapest\"                         \n[30] \"India USAID-New Delhi\"                          \n[31] \"Indonesia USAID-Jakarta\"                        \n[32] \"Iraq Assistance Office\"                         \n[33] \"Iraq USAID-Baghdad\"                             \n[34] \"Jamaica USAID-Kingston\"                         \n[35] \"Jordan USAID-Amman\"                             \n[36] \"Kazakhstan USAID-Almaty\"                        \n[37] \"Kenya USAID-Nairobi\"                            \n[38] \"Kosovo USAID-Pristina\"                          \n[39] \"Lebanon USAID-Beirut\"                           \n[40] \"Liberia USAID-Monrovia\"                         \n[41] \"Macedonia USAID-Skopje\"                         \n[42] \"Madagascar USAID-Antananarivo\"                  \n[43] \"Malawi USAID-Lilongwe\"                          \n[44] \"Mali USAID -Bamako\"                             \n[45] \"Mexico USAID-Mexico City\"                       \n[46] \"Middle East Regional Platform USAID-MERP\"       \n[47] \"Moldova USAID-Chisinau\"                         \n[48] \"Mongolia USAID-Ulaanbaatar\"                     \n[49] \"Morocco USAID-Rabat\"                            \n[50] \"Mozambique USAID-Maputo\"                        \n[51] \"Nepal USAID-Kathmandu\"                          \n[52] \"Nicaragua USAID-Managua\"                        \n[53] \"Nigeria USAID-Abuja\"                            \n[54] \"Pakistan USAID-Islamabad\"                       \n[55] \"Panama USAID-Panama City\"                       \n[56] \"Paraguay USAID-Asuncion\"                        \n[57] \"Peru USAID-Lima\"                                \n[58] \"Philippines USAID-Manila\"                       \n[59] \"Rwanda USAID-Kigali\"                            \n[60] \"Senegal USAID-Dakar\"                            \n[61] \"Serbia USAID-Belgrade\"                          \n[62] \"South Africa USAID-Pretoria\"                    \n[63] \"South Sudan (USAID)-Juba\"                       \n[64] \"Sri Lanka USAID-Colombo\"                        \n[65] \"Sudan USAID-Khartoum\"                           \n[66] \"Tajikistan USAID-Dushanbe\"                      \n[67] \"Tanzania USAID-Dar es Salaam\"                   \n[68] \"Thailand USAID-Bangkok\"                         \n[69] \"Uganda USAID-Kampala\"                           \n[70] \"Ukraine USAID-Kiev\"                             \n[71] \"Uzbekistan USAID-Tashkent\"                      \n[72] \"West Africa USAID-Ghana\"                        \n[73] \"West Bank\"                                      \n[74] \"Gaza USAID-West Bank\"                           \n[75] \"Yemen USAID-Sanaa\"                              \n[76] \"Zambia USAID-Lusaka\"                            \n[77] \"Zimbabwe USAID-Harare\"                          \n[78] \"USAID\"                                          \n[79] \"USAID - Barbados and Eastern Caribbean\"         \n[80] \"USAID-VIETNAM\"                                  \n\n$`Education and Research`\n  [1] \"Advanced Research Projects Agency Energy\"          \n  [2] \"Advanced Research Projects Agency for Health\"      \n  [3] \"Agency for Health Care Research and Quality\"       \n  [4] \"Air Force Academy\"                                 \n  [5] \"Air Force Office of Scientific Research\"           \n  [6] \"Army Contracting Command - Benet Laboratories\"     \n  [7] \"Bureau Of Educational and Cultural Affairs\"        \n  [8] \"Centers for Disease Control - ATSDR\"               \n  [9] \"Centers for Disease Control - CFA\"                 \n [10] \"Centers for Disease Control - CGH\"                 \n [11] \"Centers for Disease Control - CSELS\"               \n [12] \"Centers for Disease Control - NCBDDD\"              \n [13] \"Centers for Disease Control - NCCDPHP\"             \n [14] \"Centers for Disease Control - NCEH\"                \n [15] \"Centers for Disease Control - NCEZID\"              \n [16] \"Centers for Disease Control - NCHHSTP\"             \n [17] \"Centers for Disease Control - NCHS\"                \n [18] \"Centers for Disease Control - NCIPC\"               \n [19] \"Centers for Disease Control - NCIRD\"               \n [20] \"Centers for Disease Control - NIOSH\"               \n [21] \"Centers for Disease Control - OD\"                  \n [22] \"Centers for Disease Control - OPHPR\"               \n [23] \"Centers for Disease Control - OSTLTS\"              \n [24] \"Centers for Disease Control and Prevention\"        \n [25] \"Centers for Disease Control and Prevention - ERA\"  \n [26] \"Centers for Medicare & Medicaid Services\"          \n [27] \"Chief Evaluation Office\"                           \n [28] \"CMS Consumer Operated and Oriented Plan Program\"   \n [29] \"CMS-Consumer Information & Insurance Oversight\"    \n [30] \"DARPA - Biological Technologies Office\"            \n [31] \"DARPA - Defense Sciences Office\"                   \n [32] \"DARPA - Information Innovation Office\"             \n [33] \"DARPA - Information Processing Technology Office\"  \n [34] \"DARPA - Microsystems Technology Office\"            \n [35] \"DARPA - Strategic Technology Office\"               \n [36] \"DARPA - Tactical Technology Office\"                \n [37] \"DARPA - Transformational Convergence Technology\"   \n [38] \"DARPA-MTO-BAA-09-25\"                               \n [39] \"Defense Advanced Research Projects Agency\"         \n [40] \"Defense Health Agency\"                             \n [41] \"Defense Intelligence Agency\"                       \n [42] \"Department of Education\"                           \n [43] \"DoD Education Activity\"                            \n [44] \"DOT - FAA Aviation Research Grants\"                \n [45] \"DOT - FAA Centers of Excellence\"                   \n [46] \"DOT Federal Aviation Administration\"               \n [47] \"Economic Research Service\"                         \n [48] \"Engineer Research and Development Center\"          \n [49] \"FAA - Aviation Next Gen\"                           \n [50] \"FAA-COE-AJFE\"                                      \n [51] \"FAA-COE-GA\"                                        \n [52] \"FAA-COE-TTHP\"                                      \n [53] \"Faculty Exchange Program 10.613\"                   \n [54] \"Fish and Wildlife Service\"                         \n [55] \"Fisheries & Habitat Conservation\"                  \n [56] \"Food and Drug Administration\"                      \n [57] \"Food and Nutrition Service\"                        \n [58] \"Food Safety Inspection Service\"                    \n [59] \"Foreign Agricultural Service\"                      \n [60] \"Forest Service\"                                    \n [61] \"Geological Survey\"                                 \n [62] \"Health Resources and Services Administration\"      \n [63] \"Indian Health Service\"                             \n [64] \"Institute of Museum and Library Services\"          \n [65] \"Internal Revenue Service\"                          \n [66] \"International Agricultural Educ Fellowship 10.619\" \n [67] \"Library of Congress\"                               \n [68] \"NASA Ames Research Center\"                         \n [69] \"NASA Armstrong Flight Research Center\"             \n [70] \"NASA Glenn Research Center\"                        \n [71] \"NASA Goddard Space Flight Center\"                  \n [72] \"NASA Headquarters\"                                 \n [73] \"NASA Johnson Space Center\"                         \n [74] \"NASA Kennedy Space Center\"                         \n [75] \"NASA Langley Research Center\"                      \n [76] \"NASA Marshall Space Flight Center\"                 \n [77] \"NASA Stennis Space Center\"                         \n [78] \"National Aeronautics and Space Administration\"     \n [79] \"National Archives and Records Administration\"      \n [80] \"National Endowment for the Arts\"                   \n [81] \"National Endowment for the Humanities\"             \n [82] \"National Energy Technology Laboratory\"             \n [83] \"National Geospatial-Intelligence Agency\"           \n [84] \"National Highway Traffic Safety Administration\"    \n [85] \"National Institute of Corrections\"                 \n [86] \"National Institute of Food and Agriculture\"        \n [87] \"National Institute of Justice\"                     \n [88] \"National Institute of Standards and Technology\"    \n [89] \"National Institutes of Health\"                     \n [90] \"National Oceanic and Atmospheric Administration\"   \n [91] \"National Park Service\"                             \n [92] \"National Science Foundation\"                       \n [93] \"National Telecommunications and Information Admini\"\n [94] \"National Veterans Sports Programs\"                 \n [95] \"Natural Resources Conservation Service\"            \n [96] \"Naval Research Laboratory\"                         \n [97] \"Nuclear Regulatory Commission\"                     \n [98] \"Office of Science\"                                 \n [99] \"Office of the Assistant Secretary for Health\"      \n[100] \"Office of the Director of National Intelligence\"   \n[101] \"Office of the National Coordinator\"                \n[102] \"Office of the Secretary\"                           \n[103] \"Occupational Safety and Health Administration\"     \n[104] \"Pipeline and Hazardous Materials Safety Admin\"     \n[105] \"Risk Management Agency\"                            \n[106] \"Risk Management Education\"                         \n[107] \"Rural Business-Cooperative Service\"                \n[108] \"Rural Housing Service\"                             \n[109] \"Rural Utilities Service\"                           \n[110] \"Scientific Cooperation and Research 10.961\"        \n[111] \"Social Security Administration\"                    \n[112] \"Substance Abuse and Mental Health Services Admin\"  \n[113] \"Substance Abuse and Mental Health Services Adminis\"\n[114] \"Technical Agricultural Assistance 10.960\"          \n[115] \"Uniformed Services Univ. of the Health Sciences\"   \n[116] \"USUHS Medical - Non- Research Projects\"            \n[117] \"USUHS Medical Research Projects\"                   \n[118] \"Veterans Employment and Training Service\"          \n[119] \"Volunteer Income Tax Assistance\"                   \n[120] \"Woodrow Wilson Center\"                             \n\n$`Agriculture and Environment`\n [1] \"Agricultural Marketing Service\"                    \n [2] \"Agricultural Trade Promotion Program 10.618\"       \n [3] \"Animal and Plant Health Inspection Service\"        \n [4] \"Bureau of Land Management\"                         \n [5] \"Bureau of Ocean Energy Management\"                 \n [6] \"Bureau of Reclamation\"                             \n [7] \"Bureau of Reclamation - Denver Office\"             \n [8] \"Bureau of Reclamation - Great Plains Region\"       \n [9] \"Bureau of Reclamation - Lower Colorado Region\"     \n[10] \"Bureau of Reclamation - Mid-Pacific Region\"        \n[11] \"Bureau of Reclamation - Pacific Northwest Region\"  \n[12] \"Bureau of Reclamation - Upper Colorado Region\"     \n[13] \"Bureau of Reclamation-Upper Columbia Area Office\"  \n[14] \"Bureau of Reclamation\"                             \n[15] \"Denver Office\"                                     \n[16] \"Bureau of Reclamation\"                             \n[17] \"Mid-Pacific Regional Office\"                       \n[18] \"Bureau of Safety and Environmental Enforcement\"    \n[19] \"Department of Agriculture\"                         \n[20] \"Endangered Species\"                                \n[21] \"Environmental Management Consolidated Business Cen\"\n[22] \"Environmental Protection Agency\"                   \n[23] \"Farm Production and Conservation Business Center\"  \n[24] \"Farm Service Agency\"                               \n[25] \"Fish and Wildlife Service\"                         \n[26] \"Fisheries & Habitat Conservation\"                  \n[27] \"Forest Service\"                                    \n[28] \"Migratory Birds\"                                   \n[29] \"National Energy Technology Laboratory\"             \n[30] \"National Oceanic and Atmospheric Administration\"   \n[31] \"Natural Resources Conservation Service\"            \n[32] \"Office of Science\"                                 \n[33] \"Risk Management Agency\"                            \n[34] \"Rural Business-Cooperative Service\"                \n[35] \"Rural Housing Service\"                             \n[36] \"Rural Utilities Service\"                           \n\n$`Infrastructure and Transportation`\n  [1] \"```69A345 Office of the Under Secretary for Policy\"\n  [2] \"69A350 OSDBU\"                                      \n  [3] \"69A355 Research and Technology\"                    \n  [4] \"ACC APG - Natick\"                                  \n  [5] \"ACC-APG-Aberdeen Division A\"                       \n  [6] \"ACC-APG-Belvoir\"                                   \n  [7] \"ACC-APG-Detrick\"                                   \n  [8] \"ACC-APG-Edgewood\"                                  \n  [9] \"ACC-APG-Fort Huachuca\"                             \n [10] \"Air Force -- Materiel Command\"                     \n [11] \"Air Force -- Research Lab\"                         \n [12] \"Air Force Academy\"                                 \n [13] \"Air Force Office of Scientific Research\"           \n [14] \"Airport Improvement Program Discretionary Grants\"  \n [15] \"Alaska District\"                                   \n [16] \"Army Contracting Command - Benet Laboratories\"     \n [17] \"Army Contracting Command - New Jersey\"             \n [18] \"Army Contracting Command Rock Island\"              \n [19] \"Bureau of Land Management\"                         \n [20] \"Bureau of Ocean Energy Management\"                 \n [21] \"Bureau of Reclamation\"                             \n [22] \"Bureau of Reclamation - Denver Office\"             \n [23] \"Bureau of Reclamation - Great Plains Region\"       \n [24] \"Bureau of Reclamation - Lower Colorado Region\"     \n [25] \"Bureau of Reclamation - Mid-Pacific Region\"        \n [26] \"Bureau of Reclamation - Pacific Northwest Region\"  \n [27] \"Bureau of Reclamation - Upper Colorado Region\"     \n [28] \"Bureau of Reclamation-Upper Columbia Area Office\"  \n [29] \"Bureau of Reclamation\"                             \n [30] \"Denver Office\"                                     \n [31] \"Bureau of Reclamation\"                             \n [32] \"Mid-Pacific Regional Office\"                       \n [33] \"Bureau of Safety and Environmental Enforcement\"    \n [34] \"Chicago Service Center\"                            \n [35] \"DARPA - Biological Technologies Office\"            \n [36] \"DARPA - Defense Sciences Office\"                   \n [37] \"DARPA - Information Innovation Office\"             \n [38] \"DARPA - Information Processing Technology Office\"  \n [39] \"DARPA - Microsystems Technology Office\"            \n [40] \"DARPA - Strategic Technology Office\"               \n [41] \"DARPA - Tactical Technology Office\"                \n [42] \"DARPA - Transformational Convergence Technology\"   \n [43] \"Defense Advanced Research Projects Agency\"         \n [44] \"Defense Health Agency\"                             \n [45] \"Defense Intelligence Agency\"                       \n [46] \"Defense Logistics Agency\"                          \n [47] \"Defense Threat Reduction Agency\"                   \n [48] \"Department of Transportation\"                      \n [49] \"Dept of the Army -- Materiel Command\"              \n [50] \"Dept. of the Army  --  Corps of Engineers\"         \n [51] \"Dept. of the Army -- Space & Missle Defense Comman\"\n [52] \"Dept. of the Army -- USAMRAA\"                      \n [53] \"DOT - FAA Aviation Research Grants\"                \n [54] \"DOT - FAA Centers of Excellence\"                   \n [55] \"DOT - Federal Railroad Administration\"             \n [56] \"DOT Federal Aviation Administration\"               \n [57] \"DOT Federal Highway Administration\"                \n [58] \"DOT OSDBU\"                                         \n [59] \"DOT-Federal Motor Carrier Safety Administration\"   \n [60] \"DOT-Federal Transit Administration - Inactive Site\"\n [61] \"DOT/Federal Transit Administration\"                \n [62] \"Economic Development Administration\"               \n [63] \"Engineer Research and Development Center\"          \n [64] \"FAA - Aviation Next Gen\"                           \n [65] \"FAA-COE-AJFE\"                                      \n [66] \"FAA-COE-GA\"                                        \n [67] \"FAA-COE-TTHP\"                                      \n [68] \"Federal Communications Commission\"                 \n [69] \"Federal Mediation and Conciliation Service\"        \n [70] \"Federal Motor Carrier Safety Administration\"       \n [71] \"Federal Transit Administration\"                    \n [72] \"Fort Worth District\"                               \n [73] \"Kansas City District\"                              \n [74] \"Maritime Administration\"                           \n [75] \"NASA Ames Research Center\"                         \n [76] \"NASA Armstrong Flight Research Center\"             \n [77] \"NASA Glenn Research Center\"                        \n [78] \"NASA Goddard Space Flight Center\"                  \n [79] \"NASA Headquarters\"                                 \n [80] \"NASA Johnson Space Center\"                         \n [81] \"NASA Kennedy Space Center\"                         \n [82] \"NASA Langley Research Center\"                      \n [83] \"NASA Marshall Space Flight Center\"                 \n [84] \"NASA Stennis Space Center\"                         \n [85] \"National Aeronautics and Space Administration\"     \n [86] \"National Highway Traffic Safety Administration\"    \n [87] \"National Oceanic and Atmospheric Administration\"   \n [88] \"National Telecommunications and Information Admini\"\n [89] \"Naval Air Warfare Center Aircraft Div. Lakehurst\"  \n [90] \"NAVAL FACILITIES ENGINEERING COMMAND\"              \n [91] \"Naval Facilities Engineering Command Southwest\"    \n [92] \"Naval Information Warfare Center Pacific\"          \n [93] \"NAVAL MEDICAL LOGISTICS COMMAND\"                   \n [94] \"Naval Research Laboratory\"                         \n [95] \"Naval Supply Systems Command\"                      \n [96] \"Naval Surface Warfare Center - Carderock\"          \n [97] \"NAVFAC Atlantic\"                                   \n [98] \"NAVFAC Washington DC\"                              \n [99] \"NCA Contracting\"                                   \n[100] \"Nebraska State Office\"                             \n[101] \"Nuclear Regulatory Commission\"                     \n[102] \"NUWC Division Keyport\"                             \n[103] \"Office of Local Defense Community Cooperation\"     \n[104] \"Office of the Secretary\"                           \n[105] \"Omaha District\"                                    \n[106] \"Pipeline and Hazardous Materials Safety Admin\"     \n[107] \"Region 1\"                                          \n[108] \"Region 10\"                                         \n[109] \"Region 2\"                                          \n[110] \"Region 3\"                                          \n[111] \"Region 4\"                                          \n[112] \"Region 5\"                                          \n[113] \"Region 6\"                                          \n[114] \"Region 7\"                                          \n[115] \"Region 8\"                                          \n[116] \"Region 9\"                                          \n[117] \"Risk Management Agency\"                            \n[118] \"Rural Utilities Service\"                           \n[119] \"Savannah District\"                                 \n[120] \"Seattle District\"                                  \n[121] \"SPAWAR SYSTEMS CENTER\"                             \n[122] \"Transportation Security Administration\"            \n[123] \"U.S. Dept. of Treasury RESTORE Act Program\"        \n[124] \"United States Coast Guard\"                         \n[125] \"United States Marine Corps\"                        \n[126] \"USACE Portland District\"                           \n[127] \"USAF 347 Contracting Squadron\"                     \n[128] \"Walla Walla District\"                              \n[129] \"Washington Headquarters Services```\"               \n\n$`Business and Economic Development`\n [1] \"Economic Development Administration\"               \n [2] \"Small Business Administration\"                     \n [3] \"Community Development Financial Institutions\"      \n [4] \"Rural Business-Cooperative Service\"                \n [5] \"Employment and Training Administration\"            \n [6] \"Office of Local Defense Community Cooperation\"     \n [7] \"Economic Research Service\"                         \n [8] \"Agricultural Marketing Service\"                    \n [9] \"Agricultural Trade Promotion Program 10.618\"       \n[10] \"Bureau of Economic and Business Affairs\"           \n[11] \"Trade Policy and Geographic Affairs\"               \n[12] \"Market Access Program 10.601\"                      \n[13] \"Foreign Agricultural Service\"                      \n[14] \"Foreign Market Development Cooperator Prog 10600\"  \n[15] \"Technical Assistance for Specialty Crops 10.604\"   \n[16] \"Jobs and Innovation Accelerator Challenge\"         \n[17] \"Energy Cluster Program\"                            \n[18] \"DOT - FAA Centers of Excellence\"                   \n[19] \"DOT Federal Highway Administration\"                \n[20] \"DOT OSDBU\"                                         \n[21] \"DOT-Federal Motor Carrier Safety Administration\"   \n[22] \"DOT-Federal Transit Administration - Inactive Site\"\n[23] \"DOT/Federal Transit Administration\"                \n[24] \"Federal Communications Commission\"                 \n[25] \"National Business Center\"                          \n[26] \"National Telecommunications and Information Admini\"\n[27] \"U.S. Dept. of Treasury RESTORE Act Program\"        \n[28] \"Volunteer Income Tax Assistance\"                   \n[29] \"Low Income Taxpayer Clinic\"                        \n[30] \"Tax Counseling for the Elderly\"                    \n[31] \"CMS Consumer Operated and Oriented Plan Program\"   \n[32] \"CMS-Consumer Information & Insurance Oversight\"    \n[33] \"Office of Acquisitions Management\"                 \n[34] \"Office of Procurement Operations - Grants Division\"\n[35] \"Office of the Secretary\"                           \n[36] \"Office on Violence Against Women\"                  \n[37] \"Veterans Employment and Training Service\"          \n[38] \"Veterans Employment Pay for Success\"               \n[39] \"Office of Disability Employment Policy\"            \n[40] \"Office of Partnerships and Public Engagements\"     \n[41] \"Risk Management Agency\"                            \n[42] \"Risk Management Education\"                         \n[43] \"Office of the Assistant Secretary for Health\"      \n[44] \"Office of the National Coordinator\"                \n[45] \"Office of the Director of National Intelligence\"   \n[46] \"Office of the Middle East Partnership Initiative\"  \n[47] \"Office to Monitor-Combat Trafficking in Persons\"   \n[48] \"Office of Global Womens Issues\"                    \n[49] \"Office of Global Criminal Justice\"                 \n[50] \"Office of Justice Programs\"                        \n[51] \"Office of Juvenile Justice Delinquency Prevention\" \n[52] \"Office for Victims of Crime\"                       \n[53] \"Office of Science\"                                 \n[54] \"Office of Surface Mining\"                          \n[55] \"Office of Mental Health and Suicide Prevention\"    \n[56] \"Office of National Drug Control Policy\"            \n[57] \"Office of the Secretary\"                           \n[58] \"Office of Policy ORES\"                             \n[59] \"Office of Acquisitions Management\"                 \n[60] \"Office of Procurement Operations - Grants Division\"\n[61] \"Office of the Secretary\"                           \n[62] \"Office on Violence Against Women\"                  \n[63] \"Veterans Employment and Training Service\"          \n[64] \"Veterans Employment Pay for Success\"               \n[65] \"Office of Disability Employment Policy\"            \n[66] \"Office of Partnerships and Public Engagements\"     \n[67] \"Risk Management Agency\"                            \n[68] \"Risk Management Education\"                         \n[69] \"Office of the Assistant Secretary for Health\"      \n[70] \"Office of the National Coordinator\"                \n[71] \"Office of the Director of National Intelligence\"   \n[72] \"Office of the Middle East Partnership Initiative\"  \n[73] \"Office to Monitor-Combat Trafficking in Persons\"   \n[74] \"Office of Global Womens Issues\"                    \n[75] \"Office of Global Criminal Justice\"                 \n[76] \"Office of Justice Programs\"                        \n[77] \"Office of Juvenile Justice Delinquency Prevention\" \n[78] \"Office for Victims of Crime\"                       \n[79] \"Office of Science\"                                 \n[80] \"Office of Surface Mining\"                          \n[81] \"Office of Mental Health and Suicide Prevention\"    \n[82] \"Office of National Drug Control Policy\"            \n\n$`Arts and Culture`\n[1] \"National Endowment for the Arts\"             \n[2] \"National Endowment for the Humanities\"       \n[3] \"Institute of Museum and Library Services\"    \n[4] \"Library of Congress\"                         \n[5] \"National Archives and Records Administration\"\n[6] \"Smithsonian Institution\"                     \n\n$`Social Services and Community Development`\n [1] \"Administration for Children & Families - ACYF/FYSB\"\n [2] \"Administration for Children and Families\"          \n [3] \"Administration for Children and Families - ACYF/CB\"\n [4] \"Administration for Children and Families - ANA\"    \n [5] \"Administration for Children and Families - OCC\"    \n [6] \"Administration for Children and Families - OCS\"    \n [7] \"Administration for Children and Families - OCSE\"   \n [8] \"Administration for Children and Families - OFA\"    \n [9] \"Administration for Children and Families - OHS\"    \n[10] \"Administration for Children and Families - OHSEPR\" \n[11] \"Administration for Children and Families - OPRE\"   \n[12] \"Administration for Children and Families - ORR\"    \n[13] \"Administration for Children and Families-IOAS-OTIP\"\n[14] \"Administration for Community Living\"               \n[15] \"Administration on Aging\"                           \n[16] \"AmeriCorps\"                                        \n[17] \"Community Capacity Development Office\"             \n[18] \"Community Development Financial Institutions\"      \n[19] \"Community Oriented Policing Services\"              \n[20] \"Corporation for National and Community Service\"    \n[21] \"Department of Housing and Urban Development\"       \n[22] \"Employment and Training Administration\"            \n[23] \"Health Resources and Services Administration\"      \n[24] \"Homeless Providers Grant and Per Diem Program\"     \n[25] \"Housing and Urban Development\"                     \n[26] \"Office of Juvenile Justice Delinquency Prevention\" \n[27] \"Office of Local Defense Community Cooperation\"     \n[28] \"Office of the Assistant Secretary for Health\"      \n[29] \"Office on Violence Against Women\"                  \n[30] \"Rural Business-Cooperative Service\"                \n[31] \"Rural Housing Service\"                             \n[32] \"Rural Utilities Service\"                           \n[33] \"Social Security Administration\"                    \n[34] \"Substance Abuse and Mental Health Services Admin\"  \n[35] \"Substance Abuse and Mental Health Services Adminis\"\n[36] \"Supportive Services for Veteran Families\"          \n[37] \"Veterans Employment and Training Service\"          \n[38] \"Veterans Employment Pay for Success\"               \n[39] \"Veterans Legacy Grants Program\"                    \n[40] \"Volunteer Income Tax Assistance\"                   \n[41] \"Womens Bureau\""
  },
  {
    "objectID": "blog/2023/10/08/index.html#add-categories-to-dataframe",
    "href": "blog/2023/10/08/index.html#add-categories-to-dataframe",
    "title": "Using GPT-4 for classification",
    "section": "Add categories to dataframe",
    "text": "Add categories to dataframe\nFinally, I’ll add the category labels to the grant_opportunity_details dataframe. Since the categories are not mutually exclusive, I’ll encode them as boolean.\n\ncategories_snakecase &lt;- snakecase::to_snake_case(categories)\n\nfor(i in seq(1, length(categories_snakecase))){\n  grant_opportunity_details &lt;- grant_opportunity_details %&gt;%\n    mutate(!!categories_snakecase[i] := agency_name %in% agencies_categorized[i][[1]])\n}"
  },
  {
    "objectID": "blog/2023/12/09/index.html",
    "href": "blog/2023/12/09/index.html",
    "title": "AI Math Tutoring: Using GPT to generate “step-by-step guidance”",
    "section": "",
    "text": "Imagine you’re developing an AI Math Tutor application: You have a set of math questions that student users can answer, along with the correct answers to those questions. Students will sometimes get stuck answering multi-step problems, and in those situations you want them to be able to ask an AI tutor for help. An AI Tutoring interaction that can be helpful is one where a student asks the tutor for step-by-step guidance.\n\n\nSimple AI Math Tutor mock-up (mouse cursor is from kindpng.com)\n\nIn this blog post, I show how a “step-by-step guidance” feature could be developed using GPT and prompt engineering, and how this guidance can be validated. As a demonstration, I use a multi-step math problem that I’ve found GPT-3.5 often struggles to answer. I show how this step-by-step guidance can be validated by asking GPT to solve the problem using its own instructions and then comparing its “stepwise” performance against the performance of a baseline model.\nImports and setup\n\n\n\nR\n\nlibrary(reticulate)\nlibrary(tidyverse)\n# Add OpenAI key to environment\npy_run_string(\"import os\")\npy_run_string(paste0(\"os.environ['OPENAI_API_KEY'] = '\", Sys.getenv('OPENAI_API_KEY'), \"'\"))\n\n\n\n\n\npython\n\nimport pandas as pd\nimport numpy as np\nimport ujson as json\nimport inspect\nimport textwrap\nfrom multiprocessing.pool import ThreadPool\npool = ThreadPool()\n\n\nImport python prompting functions\nIn order to use parallelization within an interactive notebook using the multiprocessing package, it’s necessary to write the python functions to-be-parallelized to disk and import them. I’ll import the functions here, and introduce these functions later in the post as I use them.\n\n\n\npython\n\nfrom prompt_functions import get_response, baseline_solver, step_generator, stepwise_solver\n\n\nTest case\nAs a test case, I’ll use a math problem that I’ve found GPT has some difficulty solving. This problem comes from the GSM8K dataset. Although it’s a grade school math problem which should be very easy, it requires multiple reasoning steps. If any one step is wrong, the final answer is likely to be incorrect.\n\n\n\npython\n\nquestion = 'Gail has two fish tanks. The first tank is twice the size of the second tank. There are 48 gallons of water in the first tank. She follows the rule of one gallon of water per inch of fish. If she keeps two-inch fish in the second tank and three-inch fish in the first tank, how many more fish would Gail have in the first tank than the second tank if one of the first tank fish eats another?'\nprint(textwrap.fill(question, width=80))\n\n\nGail has two fish tanks. The first tank is twice the size of the second tank.\nThere are 48 gallons of water in the first tank. She follows the rule of one\ngallon of water per inch of fish. If she keeps two-inch fish in the second tank\nand three-inch fish in the first tank, how many more fish would Gail have in the\nfirst tank than the second tank if one of the first tank fish eats another?\n\n\nBaseline solver\nIn order to generate steps that an AI Tutor could suggest to a student, I’ll first get GPT to “discover” how to correctly solve the problem on its own. I’ll use the baseline_solver() function, which implements a simple chain-of-thought prompt.\n\n\n\npython\n\nprint(inspect.getsource(baseline_solver))\n\n\ndef baseline_solver(question):\n    instructions = \"Solve the math problem delimited by triple backticks.\"\n    user_content = f\"```{question}```\\nLet's work this out in a step by step way to be sure we have the right answer\"\n    msg = [\n        {\"role\": \"system\", \"content\": instructions},\n        {\"role\": \"user\", \"content\": user_content}\n    ]\n    solution = get_response(msg=msg, temp=0.5)\n    answer = identify_final_answer(solution)\n    return {\n      'is_correct': answer == '3',\n      'solution': solution,\n      'question': question,\n      'answer': answer\n    }\n\n\nI’ll give it 100 attempts to solve the problem.\n\n\n\npython\n\ntry: # Read from disk if I've done this already\n  df_baseline = pd.read_json('baseline_solver_results.ndjson')\nexcept:\n  baseline_results = pool.map(baseline_solver, 100*[question])\n  df_baseline = pd.DataFrame(baseline_results, columns=['is_correct', 'solution', 'question', 'answer'])\n  df_baseline.to_json('baseline_solver_results.ndjson', orient='records')\n\n\nIdentify solution steps\nUsing correct answers from the 100 attempts above, I’ll pick a few examples and ask GPT to synthesize and generalize the steps.\nFor this task I’ll use the step_generator() function, which asks GPT to synthesize the steps from three different solutions provided as input. It also asks GPT to remove any calculations from those steps, which is important for an AI Tutor – after all, we don’t want to give away the answer!\n\n\n\npython\n\nprint(inspect.getsource(step_generator))\n\n\ndef step_generator(solutions):\n    instructions = '''\n      You're given three different solutions to a single math problem, delimited by\n      triple hashtags. Synthesize the solutions into a final set of steps to solve\n      the problem. Remove any calculations from the instructions, leaving only the\n      steps.\n    '''\n    user_content = f'''\n      ###\n      Solution 1: \n      {solutions[0]}\n      ---\n      \n      Solution 2: \n      {solutions[1]}\n      ---\n      \n      Solution 3: \n      {solutions[2]}\n      ###\n      \n      Synthesize the three solutions above. Remove any calculations from the \n      instructions, leaving only the steps.\n    '''\n    msg = [\n        {\"role\": \"system\", \"content\": instructions},\n        {\"role\": \"user\", \"content\": user_content}\n    ]\n    return get_response(msg=msg, temp=0.3)\n\n\nHere’s what the final set of steps looks like:\n\n\n\npython\n\ntry: # Read from disk if I've done this already\n  f = open(\"steps.txt\", \"r\")\n  steps = f.read()\n  print(steps)\nexcept:\n  solutions = df_baseline[df_baseline['is_correct'] == True]['solution'].values[0:3]\n  steps = step_generator(solutions)\n  \n  f = open(\"steps.txt\", \"w\")\n  f.write(steps)\n  f.close()\n  \n  print(steps)\n\n\nTo solve the problem, follow these steps:\n\n1. Find the size of the second tank by dividing the size of the first tank by 2.\n2. Determine the number of fish that can be kept in each tank based on the rule of one gallon of water per inch of fish.\n3. Calculate the number of fish that can be kept in the second tank by dividing the size of the second tank by 2.\n4. Calculate the number of fish that can be kept in the first tank by dividing the size of the first tank by 3.\n5. Subtract 1 from the number of fish in the first tank to account for one fish eating another.\n6. Subtract the number of fish in the second tank from the number of fish in the first tank to find the difference.\n7. Determine the final answer, which is the difference in the number of fish between the two tanks.\n\n\nStepwise solver\nNext, I’ll validate the steps generated above using stepwise_solver(), a function that asks GPT to solve the problem using the steps provided. I expect that providing GPT with the steps above will improve its performance by a significant margin.\n\n\n\npython\n\nprint(inspect.getsource(stepwise_solver))\n\n\ndef stepwise_solver(question, steps):\n    instructions = \"Solve the math problem in triple backticks, using the steps provided in triple hashtags.\"\n    user_content = f\"```{question}```\\n\\n###{steps}###\"\n    msg = [\n        {\"role\": \"system\", \"content\": instructions},\n        {\"role\": \"user\", \"content\": user_content}\n    ]\n    solution = get_response(msg=msg, temp=0.5)\n    answer = identify_final_answer(solution)\n    return {\n      'is_correct': answer == '3',\n      'solution': solution,\n      'question': question,\n      'steps': steps,\n      'answer': answer\n    }\n\n\n\n\n\npython\n\ntry: # Read from disk if I've done this already\n  df_stepwise = pd.read_json('stepwise_solver_results.ndjson')\nexcept:\n  stepwise_results = pool.starmap(stepwise_solver, zip(100*[question], 100*[steps]))\n  df_stepwise = pd.DataFrame(stepwise_results, columns=['is_correct', 'solution', 'question', 'steps', 'answer'])\n  df_stepwise.to_json('stepwise_solver_results.ndjson', orient='records')\n\n\nHere we can see that by providing the GPT solver with steps derived from correct solutions, performance was almost doubled (from 39% to 77% correct responses). This provides important validation for an AI Tutor with “step-by-step guidance” functionality, because now we know that by following these steps the student would be likely to arrive at the correct answer.\n\n\n\npython\n\nprint(f'''\nBaseline solver: {round(np.mean(df_baseline['is_correct'])*100)}% correct\nStepwise solver: {round(np.mean(df_stepwise['is_correct'])*100)}% correct\n''')\n\n\n\nBaseline solver: 39% correct\nStepwise solver: 77% correct\n\n\n\n\n\nR\n\ntibble(`Baseline Solver` = 39, `Stepwise Solver` = 77) %&gt;% \n  pivot_longer(everything()) %&gt;%\n  ggplot(aes(x = name, y = value, fill = name)) + \n    geom_col(width = 0.5) + \n    geom_text(aes(x = name, y = value, label = paste0(round(value), '%')), vjust=-0.5) +\n    labs(\n      x = '',\n      y = '% correct (out of 100)',\n      title = 'Validating the step-by-step guidance, performance of the\\nStepwise Solver was roughly double the Baseline Solver.'\n    ) +\n    ylim(0, 100) +\n    theme(legend.position = \"none\",\n          text = element_text(size=14),\n          axis.text.x = element_text(size=12),\n          axis.text.y = element_text(size=12),  \n          axis.title.x = element_text(size=12),\n          axis.title.y = element_text(size=12))\n\n\n\n\n\n\n\n\nAI Tutor interaction\nHaving generated and validated the step-by-step guidance, we can now begin to imagine what the user interaction would look like. It could take the form of a single interaction, where a student asks for help and the AI Tutor provides all of the steps at once.\n\n\n\npython\n\nprint(steps)\n\n\nTo solve the problem, follow these steps:\n\n1. Find the size of the second tank by dividing the size of the first tank by 2.\n2. Determine the number of fish that can be kept in each tank based on the rule of one gallon of water per inch of fish.\n3. Calculate the number of fish that can be kept in the second tank by dividing the size of the second tank by 2.\n4. Calculate the number of fish that can be kept in the first tank by dividing the size of the first tank by 3.\n5. Subtract 1 from the number of fish in the first tank to account for one fish eating another.\n6. Subtract the number of fish in the second tank from the number of fish in the first tank to find the difference.\n7. Determine the final answer, which is the difference in the number of fish between the two tanks.\n\n\nOr the AI Tutor could provide the steps one at a time – i.e., one step is given each time the student asks for help.\n\n\n\npython\n\n# Print step 1\nprint(steps.split('\\n')[2])\n\n\n1. Find the size of the second tank by dividing the size of the first tank by 2.\n\n\n\n\n\npython\n\n# Print step 2\nprint(steps.split('\\n')[3])\n\n\n2. Determine the number of fish that can be kept in each tank based on the rule of one gallon of water per inch of fish.\n\n\nProviding steps one-at-a-time could be implemented as part of a “progressive” hint system. In an educational math game, for example, the student could receive full points for giving a correct answer with zero assistance, and the number of points earned could be reduced every time they ask for a step hint.\nIn this implementation I’ve deliberately removed the results from each step. But we could imagine an interaction where the AI first gives a step, and then optionally provides the result obtained at that step. This could help the student figure out at which step they’re making a mistake."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "January 12, 2025\n        \n        \n            A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices\n\n            \n            \n                \n                \n                    prompt-engineering\n                \n                \n                \n                    python\n                \n                \n                \n                    LLM-as-judge\n                \n                \n                \n                    LLM-evals\n                \n                \n            \n            \n\n            In this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "",
    "text": "January 12, 2025\n        \n        \n            A test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices\n\n            \n            \n                \n                \n                    prompt-engineering\n                \n                \n                \n                    python\n                \n                \n                \n                    LLM-as-judge\n                \n                \n                \n                    LLM-evals\n                \n                \n            \n            \n\n            In this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2023",
    "text": "2023\n\n\n    \n    \n                  \n            December 9, 2023\n        \n        \n            AI Math Tutoring: Using GPT to generate \"step-by-step guidance\"\n\n            \n            \n                \n                \n                    GPT\n                \n                \n                \n                    prompt-engineering\n                \n                \n                \n                    python\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            In this post, I show how an AI Tutor feature like \"step-by-step guidance\" can be built to help students on multi-step math problems, and how this guidance can be validated.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            December 4, 2023\n        \n        \n            Tackling the GSM8K (grade school math) with GPT-3.5 and self-consistency prompting\n\n            \n            \n                \n                \n                    GPT\n                \n                \n                \n                    prompt-engineering\n                \n                \n                \n                    python\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            In this post, I use the \"Self-Consistency\" prompt engineering strategy to improve the performance of a GPT-3.5 based model tasked with solving problems from the GSM8K (grade school math) dataset. I explore two implementations of this strategy, finding that one is more effective than the other. Overall, I find that this strategy is effective, leading to an increase in the percentage of correct answers from 75% at baseline to 93% with the strongest implementation of the strategy.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            October 8, 2023\n        \n        \n            Using GPT-4 for classification\n\n            \n            \n                \n                \n                    GPT\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            In this post, I use GPT-4 to classify US grant-funding agencies into 10 categories using government agency names. Then I summarize funding by category.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            September 19, 2023\n        \n        \n            Encoding high cardinality features with \"embeddings\"\n\n            \n            \n                \n                \n                    machine-learning\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            In this post I show how the performance of an ML model can be improved by encoding high cardinality features using \"embeddings\", a method that uses deep learning to represent categorical features as vectors. I compare the performance of embedding encoding with other common categorical encoding methods: one-hot, label, frequency, and target encoding.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            September 8, 2023\n        \n        \n            Using random forest based outlier detection to clean a training dataset\n\n            \n            \n                \n                \n                    machine-learning\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            In this post, I explore whether a random forest model can be improved by using random forest based multivariate outlier detection and imputation methods, and by reducing feature multicollinearity. Supporting the common wisdom that random forest models are robust to outliers and multicollinearity, these data cleaning steps led to only marginal improvements in out-of-sample model performance.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 31, 2023\n        \n        \n            Joining messy dataframes using fuzzy joining, string cleaning, and column binding\n\n            \n            \n                \n                \n                    R\n                \n                \n            \n            \n\n            Tidy Tuesday this week presented a challenge: \"There are two datasets this week for which the rows align, but the values might not precisely line up for a clean join.\" In this post I walkthrough my solution that uses a combination of fuzzy joining, string cleaning, and column binding.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 25, 2023\n        \n        \n            Using data normalization to better compare change over time in regions with different population sizes\n\n            \n            \n                \n                \n                    R\n                \n                \n            \n            \n\n            I use data normalization to better compare the changes in refugee outflows in different regions from 2010 to 2022. Four regions are identified with large increases over their 2010 baseline.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 19, 2023\n        \n        \n            Building a prediction model to detect spam email\n\n            \n            \n                \n                \n                    machine-learning\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            Using the spam email dataset from Tidy Tuesday Week 33, I walk through the process of building and evaluating a prediction model using decision tree and random forest machine learning algorithms.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2020",
    "text": "2020\n\n\n    \n    \n                  \n            May 12, 2020\n        \n        \n            Modeling cognitive impairment using NHANES data\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            I build a machine learning model to predict possible cases of cognitive impairment / dementia in a population of individuals over the age of 60. My data for this model comes from the 2013-2014 NHANES (National Health and Nutrition Examination Survey) study cohort, which is a nationally representative, longitudinal study of health in the US.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            April 11, 2020\n        \n        \n            Estimating how many people live near a landmark / point-of-interest\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            In this post, I start with a point-of-interest, \"Times Square, NYC\", and using the Census API I find out how many people live within the census tract that contains this POI (a tract is one of the smallest sub-divisions for which the Census provides population estimates).\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            April 4, 2020\n        \n        \n            Identifying pneumonia from chest x-rays using EfficientNet\n\n            \n            \n                \n                \n                    machine-learning\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            I was interested in trying tensorflow + EfficientNet on another image classification task. This time, I used it to predict pneumonia on chest x-ray images. Using this model, I achieved 97% out of sample accuracy.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            April 1, 2020\n        \n        \n            Using tensorflow with EfficientNet to predict plant diseases\n\n            \n            \n                \n                \n                    machine-learning\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            I use tensorflow with an EfficientNet base model (via transfer learning) to predict plant diseases for the Plant Pathology 2020 Kaggle challenge. Using this model, I achieved 94% out of sample accuracy.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 21, 2020\n        \n        \n            COVID-19 case growth and the Big 5 Personality traits\n\n            \n            \n                \n                \n                    python\n                \n                \n            \n            \n\n            Does the growth in COVID-19 cases have anything to do with Big 5 Personality traits? To answer this question, I compute country-level aggregates on the Big 5 test, and a country-level aggregate that represents for \"growth\" over time in coronavirus cases, using data current as of March 20, 2020.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 20, 2020\n        \n        \n            Using a logistic regression model to predict heart disease\n\n            \n            \n                \n                \n                    machine-learning\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            I trained a logistic regression model to predict heart disease, using 14 attributes and 303 observations (e.g., age, sex, chest pain, resting ECG). Then I evaluated its performance.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n    \n    \n                  \n            September 27, 2019\n        \n        \n            Predicting t-shirt size from height and weight\n\n            \n            \n                \n                \n                    machine-learning\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            Using body measurement data from the National Health and Nutrition Examination Survey (NHANES), I created a model that predicts Gildan t-shirt sizes from height and weight.\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "Hi! I’m an AI engineer with a unique background in human psychology and data science. I currently work at Khan Academy where I develop AI systems and user experiences using Large Language Models with a focus on rigorous evaluation."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Hello there!",
    "section": "",
    "text": "Hi! I’m an AI engineer with a unique background in human psychology and data science. I currently work at Khan Academy where I develop AI systems and user experiences using Large Language Models with a focus on rigorous evaluation."
  },
  {
    "objectID": "index.html#my-latest-blog-posts",
    "href": "index.html#my-latest-blog-posts",
    "title": "Hello there!",
    "section": "My latest blog posts",
    "text": "My latest blog posts\n\n\n\n\n\n\n\n\n\n\nA test of the Single Advocate Multi-Round Evaluation (SAMRE) method for LLM evaluation, and the importance of using a baseline model that implements standard best practices\n\n\n\n\n\nIn this post, I re-evaluate a method that was recently published in arXiv, critiquing the baseline model used in the paper and then implementing a new baseline model that implements standard best practices and similar multi-round aggregation. I find that the SAMRE method does not perform better than the new baseline model. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as the being skeptical of claims in research papers that compare new methods to a baseline.\n\n\n\n\n\nJan 12, 2025\n\n\nTyler Burleigh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/articles/burleigh-et-al-2017/index.html",
    "href": "research/articles/burleigh-et-al-2017/index.html",
    "title": "Wanting ‘the whole loaf’: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists",
    "section": "",
    "text": "Consensual non-monogamy (CNM) is a relationship in which individuals agree that romantic or sexual relationships with others are permissible or desirable (e.g. polyamory or open relationships). Although anti-CNM prejudice is prevalent, it is not well understood. We propose that one of the bases of anti-CNM prejudice is zero-sum thinking about love – the perception that one person’s love gained is another’s love lost. We outline our theory and then present three studies that test our predictions. In these studies, participants read a vignette that depicted characters who were in a CNM or monogamous relationship, and then judged aspects of the characters and their relationship. In Study 1, participants who read the CNM vignette judged the protagonist’s love for their initial romantic partner before and after they became involved with a second partner. Zero-sum thinking was operationally defined as the within-subject change in love ratings. In Studies 2 and 3, participants rated their agreement with items from a new preliminary measure of zero-sum romantic beliefs. We measured CNM devaluation by asking for ratings of the relationships and of individuals in the relationships. Supporting our predictions, in all three studies we found that zero-sum thinking about love was associated with increased CNM devaluation. We end by briefly discussing the implications of our findings."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2017/index.html#abstract",
    "href": "research/articles/burleigh-et-al-2017/index.html#abstract",
    "title": "Wanting ‘the whole loaf’: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists",
    "section": "",
    "text": "Consensual non-monogamy (CNM) is a relationship in which individuals agree that romantic or sexual relationships with others are permissible or desirable (e.g. polyamory or open relationships). Although anti-CNM prejudice is prevalent, it is not well understood. We propose that one of the bases of anti-CNM prejudice is zero-sum thinking about love – the perception that one person’s love gained is another’s love lost. We outline our theory and then present three studies that test our predictions. In these studies, participants read a vignette that depicted characters who were in a CNM or monogamous relationship, and then judged aspects of the characters and their relationship. In Study 1, participants who read the CNM vignette judged the protagonist’s love for their initial romantic partner before and after they became involved with a second partner. Zero-sum thinking was operationally defined as the within-subject change in love ratings. In Studies 2 and 3, participants rated their agreement with items from a new preliminary measure of zero-sum romantic beliefs. We measured CNM devaluation by asking for ratings of the relationships and of individuals in the relationships. Supporting our predictions, in all three studies we found that zero-sum thinking about love was associated with increased CNM devaluation. We end by briefly discussing the implications of our findings."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2017/index.html#important-figure",
    "href": "research/articles/burleigh-et-al-2017/index.html#important-figure",
    "title": "Wanting ‘the whole loaf’: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 2. This figure shows that increased zero-sum romantic beliefs were associated with greater devaluation of CNM (steeper slopes) in Study 2. It represents a simple slopes analysis of relationship vignette (monogamous, CNM) predicting evaluative judgment scores at 1 SD above the mean of zero-sum romantic beliefs, at the mean of zero-sum romantic beliefs, and 1 SD below the mean of zero-sum romantic beliefs."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2013/index.html",
    "href": "research/articles/burleigh-meegan-2013/index.html",
    "title": "Keeping up with the Joneses affects perceptions of distributive justice",
    "section": "",
    "text": "An experimental field study investigated why people of higher social standing might jump to the conclusion that an injustice has occurred when an authority implements a program that benefits some constituents but not others. High-status individuals are uniquely vulnerable to downward mobility, especially in the event that a situation does not benefit them, but does benefit their high-status peers. In our study, students in a university course were asked to judge a bonus program by which the grades for some would increase and the grades for others would remain the same. Two framing conditions were used, each providing an example in which only one of two students would benefit from the program. In the peer-gets-ahead condition, the two students were of equal status before the program acted to differentiate them, and in the inferior-catches-up condition, the two students differed in status before the program acted to equate them. A majority of students responded favorably to the program, although this number was affected strongly by framing, with almost unanimous approval in the inferior-catches-up condition and comparatively modest approval in the peer-gets-ahead condition. Objections in the latter condition were most frequent among high-status students, who were implicitly uncomfortable with the possibility that their status could decrease relative to some of their high-status peers. Explicitly, their objections used the language of social injustice, especially claims of distributive unfairness. We argue that these perceptions of injustice are a cognitive manifestation of an aversion to any situation that could result in downward mobility."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2013/index.html#abstract",
    "href": "research/articles/burleigh-meegan-2013/index.html#abstract",
    "title": "Keeping up with the Joneses affects perceptions of distributive justice",
    "section": "",
    "text": "An experimental field study investigated why people of higher social standing might jump to the conclusion that an injustice has occurred when an authority implements a program that benefits some constituents but not others. High-status individuals are uniquely vulnerable to downward mobility, especially in the event that a situation does not benefit them, but does benefit their high-status peers. In our study, students in a university course were asked to judge a bonus program by which the grades for some would increase and the grades for others would remain the same. Two framing conditions were used, each providing an example in which only one of two students would benefit from the program. In the peer-gets-ahead condition, the two students were of equal status before the program acted to differentiate them, and in the inferior-catches-up condition, the two students differed in status before the program acted to equate them. A majority of students responded favorably to the program, although this number was affected strongly by framing, with almost unanimous approval in the inferior-catches-up condition and comparatively modest approval in the peer-gets-ahead condition. Objections in the latter condition were most frequent among high-status students, who were implicitly uncomfortable with the possibility that their status could decrease relative to some of their high-status peers. Explicitly, their objections used the language of social injustice, especially claims of distributive unfairness. We argue that these perceptions of injustice are a cognitive manifestation of an aversion to any situation that could result in downward mobility."
  },
  {
    "objectID": "research/articles/burleigh-schoenherr-2015/index.html",
    "href": "research/articles/burleigh-schoenherr-2015/index.html",
    "title": "A reappraisal of the Uncanny Valley: Categorical perception or frequency-based sensitization?",
    "section": "",
    "text": "The uncanny valley (UCV) hypothesis describes a non-linear relationship between perceived human-likeness and affective response. The “uncanny valley” refers to an intermediate level of human-likeness that is associated with strong negative affect. Recent studies have suggested that the uncanny valley might result from the categorical perception of human-like stimuli during identification. When presented with stimuli sharing human-like traits, participants attempt to segment the continuum in “human” and “non-human” categories. Due to the ambiguity of stimuli located at a category boundary, categorization difficulty gives rise to a strong, negative affective response. Importantly, researchers who have studied the UCV in terms of categorical perception have focused on categorization responses rather than affective ratings. In the present study, we examined whether the negative affect associated with the UCV might be explained in terms of an individual’s degree of exposure to stimuli. In two experiments, we tested a frequency-based model against a categorical perception model using a category-learning paradigm. We manipulated the frequency of exemplars that were presented to participants from two categories during a training phase. We then examined categorization and affective responses functions, as well as the relationship between categorization and affective responses. Supporting previous findings, categorization responses suggested that participants acquired novel category structures that reflected a category boundary. These category structures appeared to influence affective ratings of eeriness. Crucially, participants’ ratings of eeriness were additionally affected by exemplar frequency. Taken together, these findings suggest that the UCV is determined by both categorical properties as well as the frequency of individual exemplars retained in memory."
  },
  {
    "objectID": "research/articles/burleigh-schoenherr-2015/index.html#abstract",
    "href": "research/articles/burleigh-schoenherr-2015/index.html#abstract",
    "title": "A reappraisal of the Uncanny Valley: Categorical perception or frequency-based sensitization?",
    "section": "",
    "text": "The uncanny valley (UCV) hypothesis describes a non-linear relationship between perceived human-likeness and affective response. The “uncanny valley” refers to an intermediate level of human-likeness that is associated with strong negative affect. Recent studies have suggested that the uncanny valley might result from the categorical perception of human-like stimuli during identification. When presented with stimuli sharing human-like traits, participants attempt to segment the continuum in “human” and “non-human” categories. Due to the ambiguity of stimuli located at a category boundary, categorization difficulty gives rise to a strong, negative affective response. Importantly, researchers who have studied the UCV in terms of categorical perception have focused on categorization responses rather than affective ratings. In the present study, we examined whether the negative affect associated with the UCV might be explained in terms of an individual’s degree of exposure to stimuli. In two experiments, we tested a frequency-based model against a categorical perception model using a category-learning paradigm. We manipulated the frequency of exemplars that were presented to participants from two categories during a training phase. We then examined categorization and affective responses functions, as well as the relationship between categorization and affective responses. Supporting previous findings, categorization responses suggested that participants acquired novel category structures that reflected a category boundary. These category structures appeared to influence affective ratings of eeriness. Crucially, participants’ ratings of eeriness were additionally affected by exemplar frequency. Taken together, these findings suggest that the UCV is determined by both categorical properties as well as the frequency of individual exemplars retained in memory."
  },
  {
    "objectID": "research/articles/burleigh-schoenherr-2015/index.html#important-figure",
    "href": "research/articles/burleigh-schoenherr-2015/index.html#important-figure",
    "title": "A reappraisal of the Uncanny Valley: Categorical perception or frequency-based sensitization?",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFIGURE 12: Test categorization response accuracy in the unequal frequency, unequal distribution condition. Stimulus values correspond to stimuli selected from the training range (i.e., stimuli 3–13). Error bars represent 1 standard error of the mean (N = 60)."
  },
  {
    "objectID": "research/articles/kennedy-et-al-2020/index.html",
    "href": "research/articles/kennedy-et-al-2020/index.html",
    "title": "The shape of and solutions to the MTurk quality crisis",
    "section": "",
    "text": "Amazon’s Mechanical Turk is widely used for data collection; however, data quality may be declining due to the use of virtual private servers to fraudulently gain access to studies. Unfortunately, we know little about the scale and consequence of this fraud, and tools for social scientists to detect and prevent this fraud are underdeveloped. We first analyze 38 studies and show that this fraud is not new, but has increased recently. We then show that these fraudulent respondents provide particularly low-quality data and can weaken treatment effects. Finally, we provide two solutions: an easy-to-use application for identifying fraud in the existing datasets and a method for blocking fraudulent respondents in Qualtrics surveys."
  },
  {
    "objectID": "research/articles/kennedy-et-al-2020/index.html#abstract",
    "href": "research/articles/kennedy-et-al-2020/index.html#abstract",
    "title": "The shape of and solutions to the MTurk quality crisis",
    "section": "",
    "text": "Amazon’s Mechanical Turk is widely used for data collection; however, data quality may be declining due to the use of virtual private servers to fraudulently gain access to studies. Unfortunately, we know little about the scale and consequence of this fraud, and tools for social scientists to detect and prevent this fraud are underdeveloped. We first analyze 38 studies and show that this fraud is not new, but has increased recently. We then show that these fraudulent respondents provide particularly low-quality data and can weaken treatment effects. Finally, we provide two solutions: an easy-to-use application for identifying fraud in the existing datasets and a method for blocking fraudulent respondents in Qualtrics surveys."
  },
  {
    "objectID": "research/articles/kennedy-et-al-2020/index.html#important-figure",
    "href": "research/articles/kennedy-et-al-2020/index.html#important-figure",
    "title": "The shape of and solutions to the MTurk quality crisis",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 5. Path diagram of screening protocol."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2015/index.html",
    "href": "research/articles/schoenherr-burleigh-2015/index.html",
    "title": "Uncanny sociocultural categories",
    "section": "",
    "text": "Considered individually, folkbiological categories, biological anomalies and monsters, as well as human categories represent individual cultural products of human categorization. Instead, we suggest that the uncanny valley might reflect a primary response to unfamiliar or covert categories. In the absence of having prior knowledge of an individual or group, the relative distinctiveness of a category, due to a lower frequency of exposure, will produce negative affect—an inversion of the mere-exposure effect. The deceptive simplicity of learning mechanisms can lead to important individual and social consequences."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2015/index.html#abstract",
    "href": "research/articles/schoenherr-burleigh-2015/index.html#abstract",
    "title": "Uncanny sociocultural categories",
    "section": "",
    "text": "Considered individually, folkbiological categories, biological anomalies and monsters, as well as human categories represent individual cultural products of human categorization. Instead, we suggest that the uncanny valley might reflect a primary response to unfamiliar or covert categories. In the absence of having prior knowledge of an individual or group, the relative distinctiveness of a category, due to a lower frequency of exposure, will produce negative affect—an inversion of the mere-exposure effect. The deceptive simplicity of learning mechanisms can lead to important individual and social consequences."
  },
  {
    "objectID": "research/articles/sparks-et-al-2016/index.html",
    "href": "research/articles/sparks-et-al-2016/index.html",
    "title": "We can see inside: Accurate prediction of Prisoner’s Dilemma decisions in announced games following a face-to-face interaction",
    "section": "",
    "text": "Humans form impressions and make social judgments about others based on information that is quickly and easily available, such as facial and vocal traits. The evolutionary function of impression formation and social judgment mechanisms have received limited attention in psychology research; we argue that their function is to accurately forecast the behavior of others. There is some evidence for the predictive accuracy of social judgments, but much of it comes from situations where there is little incentive to deceive, which limits applicability to questions of the function of such mechanisms. A classic experiment that avoids this problem was conducted by R. H. Frank, T. Gilovich, and D. T. Regan (1993); their participants predicted each other’s Prisoner’s Dilemma Game decisions with above-chance accuracy after a short interaction period, knowing the game would follow. We report three original studies that replicate these aspects of the methods of Frank et al. (1993) and reanalyze data from all known replications. Our meta-analysis of these studies confirms the original report: humans can predict each other’s Prisoner’s Dilemma decisions after a brief interaction with people who have incentive to deceive."
  },
  {
    "objectID": "research/articles/sparks-et-al-2016/index.html#abstract",
    "href": "research/articles/sparks-et-al-2016/index.html#abstract",
    "title": "We can see inside: Accurate prediction of Prisoner’s Dilemma decisions in announced games following a face-to-face interaction",
    "section": "",
    "text": "Humans form impressions and make social judgments about others based on information that is quickly and easily available, such as facial and vocal traits. The evolutionary function of impression formation and social judgment mechanisms have received limited attention in psychology research; we argue that their function is to accurately forecast the behavior of others. There is some evidence for the predictive accuracy of social judgments, but much of it comes from situations where there is little incentive to deceive, which limits applicability to questions of the function of such mechanisms. A classic experiment that avoids this problem was conducted by R. H. Frank, T. Gilovich, and D. T. Regan (1993); their participants predicted each other’s Prisoner’s Dilemma Game decisions with above-chance accuracy after a short interaction period, knowing the game would follow. We report three original studies that replicate these aspects of the methods of Frank et al. (1993) and reanalyze data from all known replications. Our meta-analysis of these studies confirms the original report: humans can predict each other’s Prisoner’s Dilemma decisions after a brief interaction with people who have incentive to deceive."
  },
  {
    "objectID": "research/articles/sparks-et-al-2016/index.html#important-figure",
    "href": "research/articles/sparks-et-al-2016/index.html#important-figure",
    "title": "We can see inside: Accurate prediction of Prisoner’s Dilemma decisions in announced games following a face-to-face interaction",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFig. 1. Forest plot—marker size indicates a study’s weight in the combined estimate."
  },
  {
    "objectID": "research/articles/wood-et-al-2018/index.html",
    "href": "research/articles/wood-et-al-2018/index.html",
    "title": "Reasons for sex and relational outcomes in consensually nonmonogamous and monogamous relationships: A self-determination theory approach",
    "section": "",
    "text": "Approximately 4% of individuals in North America participate in consensually nonmonogamous (CNM) relationships, wherein all partners have agreed to additional sexual and/or emotional partnerships. The CNM relationships are stigmatized and viewed as less stable and satisfying than monogamous relationships, a perception that persists despite research evidence. In our study, we assess the legitimacy of this negative perception by using a self-determination theory (SDT) framework to explore how sexual motivation impacts relational and sexual satisfaction among CNM and monogamous participants in romantic relationships. A total of 348 CNM (n = 142) and monogamous participants (n = 206) were recruited from Amazon’s Mechanical Turk (MTurk. (2016). www.mturk.com) to complete a cross-sectional survey. Participants reported on their sexual motivations during their most recent sexual event, their level of sexual need fulfillment, and measures of sexual and relational satisfaction with their current (primary) partner. The CNM and monogamous participants reported similar reasons for engaging in sex, though CNM participants were significantly more likely to have sex for personal intrinsic motives. No differences in mean levels of relationship and sexual satisfaction were found between CNM and monogamous individuals. Participants who engaged in sex for more self-determined reasons reported increased relational and sexual satisfaction. This relationship was mediated by sexual need fulfillment; participants who reported more self-determined motives reported higher levels of need fulfillment and, in turn, greater relationship and sexual satisfaction. This study indicates that CNM and monogamous individuals report similar levels of satisfaction within their relationship(s) and that the mechanisms that affect relational and sexual satisfaction are similar for both CNM and monogamous individuals. Our research extends theoretical understandings of motivation within romantic relationships and suggests that SDT is a useful framework for considering the impact of sexual motivation on relational outcomes."
  },
  {
    "objectID": "research/articles/wood-et-al-2018/index.html#abstract",
    "href": "research/articles/wood-et-al-2018/index.html#abstract",
    "title": "Reasons for sex and relational outcomes in consensually nonmonogamous and monogamous relationships: A self-determination theory approach",
    "section": "",
    "text": "Approximately 4% of individuals in North America participate in consensually nonmonogamous (CNM) relationships, wherein all partners have agreed to additional sexual and/or emotional partnerships. The CNM relationships are stigmatized and viewed as less stable and satisfying than monogamous relationships, a perception that persists despite research evidence. In our study, we assess the legitimacy of this negative perception by using a self-determination theory (SDT) framework to explore how sexual motivation impacts relational and sexual satisfaction among CNM and monogamous participants in romantic relationships. A total of 348 CNM (n = 142) and monogamous participants (n = 206) were recruited from Amazon’s Mechanical Turk (MTurk. (2016). www.mturk.com) to complete a cross-sectional survey. Participants reported on their sexual motivations during their most recent sexual event, their level of sexual need fulfillment, and measures of sexual and relational satisfaction with their current (primary) partner. The CNM and monogamous participants reported similar reasons for engaging in sex, though CNM participants were significantly more likely to have sex for personal intrinsic motives. No differences in mean levels of relationship and sexual satisfaction were found between CNM and monogamous individuals. Participants who engaged in sex for more self-determined reasons reported increased relational and sexual satisfaction. This relationship was mediated by sexual need fulfillment; participants who reported more self-determined motives reported higher levels of need fulfillment and, in turn, greater relationship and sexual satisfaction. This study indicates that CNM and monogamous individuals report similar levels of satisfaction within their relationship(s) and that the mechanisms that affect relational and sexual satisfaction are similar for both CNM and monogamous individuals. Our research extends theoretical understandings of motivation within romantic relationships and suggests that SDT is a useful framework for considering the impact of sexual motivation on relational outcomes."
  }
]