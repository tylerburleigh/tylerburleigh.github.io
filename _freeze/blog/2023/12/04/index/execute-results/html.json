{
  "hash": "7b29c8b5d0b98b705e62c53793ff9570",
  "result": {
    "markdown": "---\ntitle: 'Tackling the GSM8K (grade school math) with GPT-3.5 and self-consistency prompting'\ndate: 2023-12-04\ndescription: \"In this post, I use the \\\"Self-Consistency\\\" prompt engineering strategy to improve the performance of a GPT-3.5 based model tasked with solving problems from the GSM8K (grade school math) dataset. I explore two implementations of this strategy, finding that one is more effective than the other. Overall, I find that this strategy is effective, leading to an increase in the percentage of correct answers from 75% at baseline to 93% with the strongest implementation of the strategy.\"\nimage: social-image.png\ntwitter-card:\n  image: \"social-image.png\"\nopen-graph:\n  image: \"social-image.png\"\ncategories:\n  - GPT\n  - prompt-engineering\n  - python\n  - R\nfreeze: true\n---\n\n\nI've been interested in AI tutoring applications for education for a long time, and with today's Large Language Models like GPT, which have been shown to [perform extremely well on standardized tests](https://openai.com/research/gpt-4) like the SAT and GRE, it seems that building these applications is now achievable. In fact, some companies have already started building these applications, like Khan Academy's own [Khanmigo](https://www.khanacademy.org/khan-labs).\n\nThe current generation of LLMs perform pretty well at many tasks, and research (like [this paper on chain-of-thought prompting](https://arxiv.org/abs/2201.11903)) has shown that there are a variety of \"prompt engineering\" techniques that can be used to boost performance even further. Basically, prompt engineering refers to the process of writing effective instructions for the model, as well as the process of breaking a complex task into sub-tasks and \"chaining\" prompts together.\n\nIn this post, I use the `Self-Consistency` prompt engineering strategy to improve the performance of a GPT-3.5 based model tasked with solving problems from the [GSM8K (grade school math)](https://github.com/openai/grade-school-math) benchmark dataset. Conceptually, the `Self-Consistency` strategy involves asking the LLM to follow multiple reasoning paths to generate multiple answers, and then taking a majority vote of its answers.\n\nI explore implementing the `Self-Consistency` strategy first by using a single prompt that instructs the model to generate multiple reasoning paths and answers, followed by identifying the majority vote of its own answers -- all within a single response. In addition, I explore an implementation in which the LLM is asked to generate an answer multiple times using independent requests to the API, followed by using a simple frequency count to obtain the majority vote of its answers.\n\nUsing the `Self-Consistency` strategy, model performance was increased from 75% correct answers at baseline to 93% with the multiple-attempts implementation. Overall, this analysis suggests that `Self-Consistency` is an effective strategy to improve LLM performance on cognitive tasks like answering grade school math questions.\n\n(Note: This blog post uses a mix of `python` and `R` code.)\n\n# Sample test cases\n\nFirst, I'll sample 100 test cases at random from the [GSM8K](https://github.com/openai/grade-school-math) training dataset. The GSM8K has 8000 cases in total, but every API request takes time and costs money, and I want to keep this reasonable. :)\n\n\n::: {.cell filename='python'}\n\n```{.python .cell-code}\nimport pandas as pd\ntrain = pd.read_json(path_or_buf='train.jsonl', lines=True)\nsample = train.sample(n=100, random_state=1) # Random sample of 100 items\n```\n:::\n\n\n# OpenAI client\n\nNext, I'll define a client function for interfacing with GPT.\n\n\n::: {.cell filename='python'}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef get_response(msg, mod=\"gpt-3.5-turbo\", temp=0):\n    response = client.chat.completions.create(\n      model=mod,\n      messages=msg,\n      temperature=temp\n    )\n    return response.choices[0].message.content\n```\n:::\n\n\n# Prompt functions\n\nNext, I'll define some custom prompt functions.\n\n`self_consistency_solver()` is an implementation of the `Self-Consistency` prompting strategy within a single prompt. This prompt asks the LLM to \"imagine N completely independent experts who reason differently\" and also asking it to take the majority vote across these imagined experts. The `n_experts` parameter identifies how many experts the LLM will be instructed to imagine.\n\n`identify_final_answer()` is a prompt that takes an LLM's answer and basically extracts and refines it, because in many cases the answer given will contain lots of extra text.\n\n\n::: {.cell filename='python'}\n\n```{.python .cell-code}\nimport re\n\ndef self_consistency_solver(queston, n_experts):\n    instructions = f'''\n    Imagine {n_experts} completely independent experts who reason differently \n    are answering a question. The question is delimited by triple backticks.\n    The final answer is obtained by majority vote.\n    \n    Step 1. For each of the experts, give their step-by-step \n    reasoning and answer\n    Step 2. Determine the final answer by majority vote\n    Step 3. Return the final answer, obtained by majority vote, \n    prefixed by 'Final answer:'\n    '''    \n    user_content = f'```{queston}```'\n    \n    msg = [\n        {\"role\": \"system\", \"content\": instructions},\n        {\"role\": \"user\", \"content\": user_content}\n    ]\n    return get_response(msg=msg, temp=0.5)\n\n\ndef identify_final_answer(question, solution):\n    instructions = '''\n    You will be provided with the answer to a math question. \n    The question is delimited by triple backticks, \n    and the answer is delimited by triple hashtags.\n\n    Step 1. Determine if the answer is expressed as a single number\n    Step 2. If the answer is not expressed as a single number, \n    find a way to express it as a single number\n\n    Return the final answer, expressed as a single number, \n    prefixed by 'Final answer:'\n    '''\n    try:\n        answer = solution.split('Final answer: ')[1]\n        answer = f'###{answer}###'\n        user_content = f'```{question}```' + answer\n\n        msg = [\n            {\"role\": \"system\", \"content\": instructions},\n            {\"role\": \"user\", \"content\": user_content}\n        ]\n        return get_response(msg=msg, temp=0)\n    except:\n        return 'NA'\n```\n:::\n\n\nNext, I'll define several other functions. Some of these are used for data wrangling. The most important functions here are `multi_step_solver()` and `get_best_answer`, which underlie the second implementation of the `Self-Consistency` strategy. \n\nThis implementation involves asking the LLM to generate a response several times using independent requests to the API, and then using a simple frequency count to obtain the majority vote across its responses. The multiple-attempts implementation is simple: Given a parameter, `n_attempts`, an API request is sent that many times and the responses are collected into an array, then `get_best_answer()` is used to find the \"best answer\" which is the answer most frequently given.\n\n\n::: {.cell filename='python'}\n\n```{.python .cell-code}\nfrom multiprocessing.pool import ThreadPool \n\ndef clean_answer(answer):\n    # Remove everything but numbers in integer or decimal form\n    answer_clean = re.sub('[^\\d\\.]', '', answer)\n    # If the last character is a decimal, remove it, it was probably presented as a sentence\n    if answer_clean[-1] == '.':\n        answer_clean = answer_clean[:-1]    \n    # If number contains decimal, decide if it should be removed\n    if '.' in answer_clean:\n        # If the number contains only trailing zeroes, strip them and remove it\n        if answer_clean[-1] == '0' and answer_clean[-2] == '0':\n            answer_clean = answer_clean.rstrip('0')\n            answer_clean = answer_clean[:-1]\n        # If the decimal is now the last character, remove it\n        if answer_clean[-1] == '.':\n            answer_clean = answer_clean[:-1]\n    return answer_clean\n\n\ndef parse_final_answer(evaluation):\n    try:\n        answer = evaluation.split('Final answer: ')[1]\n        return clean_answer(answer)\n    except:\n        return 'NA' # Return NA if a final answer could not be found\n\n\ndef get_best_answer(options):\n    # The best answer is determined by a majority vote;\n    # in other words, the one with the highest frequency\n    answer_count = [[x, options.count(x)] for x in set(options) if x not in ['', 'NA']]\n    answer_count_sorted = sorted(answer_count, key=lambda x: x[1], reverse=True)\n    if len(answer_count_sorted) > 0:\n        return answer_count_sorted[0][0]\n    else:\n        return 'NA' # Return NA if there were no correct answers\n\n\ndef get_true_answer(answer):\n    answer = answer.split('### ')[1]\n    # Remove anything other than a decimal form number (e.g., commas)\n    answer = re.sub('[^\\d\\.]', '', answer)\n    return answer\n\n\ndef multi_step_solver(question, n_experts, n_attempts):\n    # Pool for parallelization\n    pool = ThreadPool(n_attempts)\n    \n    # Generate attempts\n    attempts = pool.starmap(self_consistency_solver, zip([question]*n_attempts, [n_experts]*n_attempts))\n\n    # Identify the final answers\n    answers = pool.starmap(identify_final_answer, zip([question]*n_attempts, attempts))\n    \n    # Parse the final answers\n    answers_parsed = [parse_final_answer(i) for i in answers]\n    \n    # Identify the best answer\n    best_answer = get_best_answer(answers_parsed)\n\n    # Results\n    return {\n        \"best_answer\": best_answer,\n        \"attempts\": attempts,\n        \"answers\": answers,\n        \"answers_parsed\": answers_parsed\n    }\n```\n:::\n\n\n# Run test cases\n\nI'll use a few loops to run through the test cases, and save the results to a newline-delimited JSON after each one is processed.\n\n\n::: {.cell filename='python'}\n\n```{.python .cell-code}\nimport ujson as json\n\n# Read previous answers from disk if they exist\ntry:\n    df = pd.read_json('answers.ndjson', lines=True)        \n    start = len(df[(df['n_experts'] == 1) & (df['n_attempts'] == 1)]) # Identify where we left off\nexcept:\n    start = 0\n\nfor i in range(start, len(sample)):\n    for n_attempts in [1, 3, 5, 10]:\n        for n_experts in [1, 3, 5]:\n            question = sample.iloc[i].question\n            true_answer = get_true_answer(sample.iloc[i].answer)\n            results = multi_step_solver(question, n_experts=n_experts, n_attempts=n_attempts)\n\n            with open(f'answers.ndjson', 'a+') as f:\n                json.dump({\n                    \"n_experts\": n_experts,\n                    \"n_attempts\": n_attempts,\n                    \"question\": question,\n                    \"true_answer\": true_answer,\n                    \"best_answer\": results['best_answer'],\n                    \"attempts\": results['attempts'],\n                    \"answers\": results['answers'],\n                    \"answers_parsed\": results['answers_parsed']\n                }, f)\n                f.write('\\n')\n```\n:::\n\n\n# Analysis of performance\n\nAlright! Now that all of the responses has been generated, it's time for the analysis.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf = jsonlite::stream_in(file('answers.ndjson'), verbose=F) %>%\n  mutate(is_correct = best_answer == true_answer,\n         n_experts = as.factor(n_experts),\n         n_attempts = as.factor(n_attempts))\n```\n:::\n\n\n## Validity checks\n\nCheck that there are 1200 total records.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\nnrow(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1200\n```\n:::\n:::\n\n\nCheck that each of the 12 levels has 100 questions each.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\ndf %>%\n  group_by(n_experts, n_attempts) %>%\n  count() %>%\n  filter(n == 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 Ã— 3\n# Groups:   n_experts, n_attempts [12]\n   n_experts n_attempts     n\n   <fct>     <fct>      <int>\n 1 1         1            100\n 2 1         3            100\n 3 1         5            100\n 4 1         10           100\n 5 3         1            100\n 6 3         3            100\n 7 3         5            100\n 8 3         10           100\n 9 5         1            100\n10 5         3            100\n11 5         5            100\n12 5         10           100\n```\n:::\n:::\n\n\n## Baseline performance\n\nAs a baseline, I'll use performance when the model is given 1 attempt using 1 expert. \n\nBaseline performance is 75%.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\ndf %>%\n  filter(n_experts == 1, n_attempts == 1) %>%\n  summarize(pct_correct = mean(is_correct)*100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  pct_correct\n1          75\n```\n:::\n:::\n\n\n## Single-prompt \"imagined experts\"\n\nIf the single-prompt \"imagined experts\" implementation improves performance, then I would expect that prompting the model to imagine 3 (or 5) experts would perform better than asking it to imagine only 1 expert. Contrary to this expectation, I see no improvement.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\ndf %>%\n  group_by(n_experts) %>%\n  summarize(pct_correct = mean(is_correct)*100) %>%\n  ggplot(aes(x = n_experts, y = pct_correct, group = 1)) +\n    geom_line() +\n    labs(x=\"# of Imagined Experts\",\n         y='% Correct Answers',\n         title=\"Imagined experts had minimal benefits when averaged over\\ndifferent numbers of attempts\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_experts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nHowever, it's possible that the improvement is obscured by the fact that I'm taking the average across number of attempts. It's possible that the two implementations are redundant, and that the single-prompt \"imagined experts\" implementation is only beneficial when the model is given a single attempt.\n\nBelow we can see that if the model is given only 1 attempt, the prompt with 3 experts performed better than the prompt with only 1 imagined expert. There's also a non-linear pattern, which may suggest that asking the model to imagine too many experts with different reasoning leads to some experts engaging sub-optimal reasoning, which then leads to poorer performance.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\ndf %>%\n  filter(n_attempts == 1) %>%\n  group_by(n_experts) %>%\n  summarize(pct_correct = mean(is_correct)*100) %>%\n  ggplot(aes(x = n_experts, y = pct_correct, group = 1)) +\n    geom_line() +\n    labs(x=\"# of Imagined Experts\",\n         y='% Correct Answers',\n         title=\"In the absence of multiple attempts, imagining 3 experts was best\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_experts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Multiple attempts\n\nI expect that when I take the most frequent answer across attempts, performance would improve with the number of attempts (at least, up to a certain point). This would support the multiple-attempts implementation. Indeed this does seem to be the case: Model performance increased linearly with number of attempts, with 10 attempts performing better than 5, 5 better than 3, and 3 better than 1.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\ndf %>%\n  group_by(n_attempts) %>%\n  summarize(pct_correct = mean(is_correct)*100) %>%\n  ggplot(aes(x = n_attempts, y = pct_correct, group = 1)) +\n    geom_line() +\n    labs(x=\"# of Attempts\",\n         y='% Correct Answers',\n         title=\"More attempts means more accuracy\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_attempts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nThe strongest model was one that was given 10 attempts, with only 1 imagined expert per attempt, which achieved 93% correct answers. This suggests that giving the model multiple attempts, while generating only 1 answer per attempt may be the best implementation of the `Self-Consistency` strategy.\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\ndf %>%\n  group_by(n_attempts, n_experts) %>%\n  summarize(pct_correct = mean(is_correct)*100, .groups = 'drop') %>%\n  ggplot(aes(x = n_attempts, y = pct_correct, group = n_experts, color = n_experts)) +\n    geom_line() +\n    labs(x=\"# of Attempts\",\n         y='% Correct Answers',\n         color=\"# of Imagined Experts\",\n         title=\"The best implementation: 10 attempts with 1 imagined expert\") +\n    ylim(65, 100) +\n    geom_text(aes(x = n_attempts, y = pct_correct, label = paste0(round(pct_correct), '%')), vjust=-0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nOverall, this analysis provides strong evidence in favor of the multiple-attempts implementation of `Self-Consistency`, and weaker evidence in favor of the single-prompt \"imagined experts\" implementation. The best implementation involved asking the LLM to generate only 1 answer per attempt, while giving it 10 attempts in total. Using this implementation of the strategy, performance was increased from 75% correct answers at baseline to 93% correct answers.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}