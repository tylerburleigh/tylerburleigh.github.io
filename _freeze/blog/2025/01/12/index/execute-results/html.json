{
  "hash": "7dbbfbdb64e9582054201f20e84dcee5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Challenging SAMRE: Comparing multi-round debate-style LLM evaluation to a robust (and much simpler) baseline\"\ndate: 2025-01-12\ndescription: \"In this post, I re-evaluate a method that was recently published in arXiv, critiquing their baseline model and then designing a new baseline model that implements standard best practices for comparison with the new method. I find that the new evaluation method proposed in the paper does not perform better than this robust baseline. This serves to highlight the importance of implementing best practices in baseline models for comparison with new methods, as well as being skeptical of claims in research papers that compare new methods to baseline.\"\ncategories:\n  - prompt-engineering\n  - python\n  - LLM-as-judge\n  - LLM-evals\nfreeze: true\n---\n\n\nI've been doing a lot of work with LLM-based evaluations lately, and I've been thinking about how to improve the quality of these evaluations.\n\nI like to read research papers from arXiv for inspiration, and I recently came across a paper called [Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates](https://arxiv.org/abs/2410.04663), which introduces a new method inspired by judicial process called Single Advocate Multi-Round Evaluation (SAMRE). Briefly, the SAMRE method evaluates the quality of different LLM outputs through an iterative debate process.\n\nI was initially impressed by the results, which reported a gain of ~6-8% over baseline. Below I've reproduced an excerpt from one of the tables in the paper showing their results.\n\n\n| Model | SAMRE w/o Juries | SAMRE w/o Juries (%) |\n|-------|------------------|---------------------|\n| Llama-3-8B | 0.05 | 6.3% |\n| Qwen | 0.06 | 7.3% |\n| Gemini | 0.06 | 7.2% |\n| GPT-4-o | 0.07 | 8.3% |\n| GPT-4-turbo | 0.07 | 8.2% |\n| GPT-3.5-turbo | 0.05 | 6.2% |\n\n: Excerpt from \"Table 2: Performance Gains Compared to Baseline\"\n\n_Note that the authors had tested versions of SAMRE with and without the addition of \"juries\". In the table I've included only the version without juries, as it was both simpler and more performant. It is also this more performant version without juries that I am interested in testing. So with that said, in this blog post when I mention \"SAMRE\", I will be referring to the version without juries._\n\nDespite the impressive results reported in the paper, I am often skeptical when researchers claim to have found that new methods outperform \"baseline\" models. I have observed that researchers often fail to implement standard best practices in their baseline models, and so their results are therefore not represenative of true gains over baseline. It is as if they are knocking down a straw man.\n\nGiven this skepticism of mine, I decided that it might be interesting to put it this skepticism the test: What if I implemented the SAMRE method (again, note that I am referring to the version without juries), and compared it to a baseline model that does implement standard best practices for prompt engineering? Would I find that the SAMRE method is indeed an improvement over the baseline? Or would I find that SAMRE is inferior to a properly implemented baseline?\n\n## TL;DR: What I did and what I found\n\nI tested three model variants:\n\n1. SAMRE, as implemented in the paper (without juries)\n2. Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)\n3. Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.\n\nI evaluated each of these models using a sample of 300 conversations from MT-Bench for testing and evaluation. (MT-Bench was used in the original paper as well.)\n\nAfter running the evaluations and calculating Krippendorff alpha agreement with human judge ground truth, I found that although SAMRE did yield better agreement than Baseline-Weak more importantly it was inferior to Baseline-Strong -- and by a fair margin. A similar result was found when examining binary classification accuracy using Matthews Correlation Coefficient (MCC).\n\nThese results serve to highlight the importance of implementing standard best practices in baseline models, as well as being skeptical of claims in research papers that compare new methods to a \"baseline model\". Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.\n\n# Baseline model prompt inadequacies\n\nHere I will consider some of the inadequacies in the Baseline model's prompt reported in the paper, and share a version of the prompt that addresses these inadequacies and implements standard best practices. \n\nThe \"baseline\" prompt used by the authors of the paper was as follows:\n\n```prompt\nYou are a fair, impartial judge scoring a debate on the following question:\nquestion.\nAnswer 1: answer_1\nAnswer 2: answer_2\nScore each answer on a scale of 1-20 for each of the following criteria:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nProvide scores as [answer_1_score, answer_2_score] for each criterion in a list format, then sum for final scores. Please keep an eye on the slightest difference that should make a difference in the scoring. Don’t overthink!\nRelevance:\nAccuracy:\nDepth:\nClarity:\nLogic and Factuality:\nAddressing opponent’s points:\nFinal Scores (sum of above) as a tuple (example: (18, 9)):\nExplain your scoring, focusing on why one answer is better than the other based on the criteria above. Keep your explanation concise but informative.\nFinally, return the final score tuple (score1, score2) as a tuple (in parentheses).\nExample: (18, 9)\nYour scores and explanation:\n```\n\nHere are the issues I see with this prompt:\n\n1. The prompt does not use delimiters for most of the inputs. I would enclose the inputs inside XML tags like `<Question></Question>`, `<Answer1></Answer1>`, and `<Answer2></Answer2>`, but in a pinch delimiters like triple backticks can be used.\n\n2. The prompt instructs the model to first generate scores in list format, and then to sum them. But as we know, language models models often make arithmetic mistakes. It would be better to ask the model to generate scores for each criterion, and then to programmatically extract and summarize them in python (or another programming language) from which the routine is run.\n\n3. Although the prompt asks the model to \"explain your scoring\", it is not clear if the model should be reasoning about each criterion before it scores them, or if it should provide reasoning at the end when giving its final score. I would ask the model to provide reasoning for each criterion that it is asked to score, and ask it to reason before scoring.\n\n4. It's unclear why a scale of 1-20 is used. This is not a standard scale for scoring. I would use a scale of 1-10 which is likely more familiar to the model and can be expected to be used more consistently.\n\n5. Although the prompt does suggest that the model provide its scores in tuple format, it would be better to provide more explicit format instructions.\n\n6. The prompt includes an \"Effectiveness in addressing opponent's points\" criterion, but this is almost certainly irrelevant given that the answers to the question were not generated with the goal of addressing an opponent.\n\n7. Finally, although this goes beyond the prompt itself, the authors of the paper are comparing a multi-round method to a single-round method. This is obviously an unfair comparison. Instead, it would be better to compare the SAMRE method to a baseline that uses the same number of rounds and then similarly averages its scores.\n\nWith all of that in mind, here's how I would rewrite the prompt:\n\n```prompt\nYou are a fair, impartial judge scoring a debate on Question.\n\n<Question>\n{question}\n</Question>\n\nTwo Answers have been given to the Question.\n\n<Answer1>\n{answer_1}\n</Answer1>\n\n<Answer2>\n{answer_2}\n</Answer2>\n\nThe Answers are being judged on the following Criteria:\n\n<Criteria>\n<Criterion1>Relevance to their task</Criterion1>\n<Criterion2>Accuracy and credible sources</Criterion2>\n<Criterion3>Depth and completeness</Criterion3>\n<Criterion4>Clarity and logical flow</Criterion4>\n<Criterion5>Reasoning and factual support</Criterion5>\n</Criteria>\n\nFor each Criterion, briefly analyze the performance of \nthe two Answers, then give a score between 1 and 10.\n\nRespond as follows:\n<Criterion1>\n<CriterionName>Relevance to their task</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion1>\n<Criterion2>\n<CriterionName>Accuracy and credible sources</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion2>\n...\n```\n\nNotice that the prompt now uses XML tags to structure the instructions, that it asks the model to provide reasoning for each criterion before scoring, and that it gives the model a clear format for its response that reinforces analysis before scoring for each criterion.\n\nI've also changed the scale from 1-20 to 1-10, removed the unnecessary \"Effectiveness in addressing opponent's points\" criterion, and removed the instruction to summarize the scores, as I would handle this within the code.\n\n_Note the baseline could be improved even further by requesting the structured output using a mode like OpenAI's Structured Outputs. This would increase the likelihood of the model responding in the desired format. For this test, I will not be using structured outputs._\n\n# Hypothesis and predictions\n\nI hypothesize that SAMRE will NOT perform better than a baseline model that implements standard best practices for prompt engineering.\n\nMy predictions are as follows:\n\n1. SAMRE will perform better than Baseline-Weak, as this was what the authors of the paper found and by implementing these methods faithfully from the paper, I can expect to replicate their results.\n2. Baseline-Strong will perform better than Baseline-Weak, since the \"Strong\" variant implements best practices and can be expected to perform better than the \"Weak\" variant.\n3. Baseline-Strong will perform equal to or better than SAMRE, as the best practices implemented in Baseline-Strong will close the gap that exists between SAMRE and Baseline-Weak due to the inadequacies in the Baseline-Weak prompt.\n\n# My implementation of SAMRE and Baseline\n\nOkay, so with those criticisms out of the way, let's design evaluators to implement three methods:\n\n1. SAMRE, as implemented in the paper (without juries)\n2. Baseline-Weak: The baseline model used in the paper (which does not implement standard best practices for prompt engineering)\n3. Baseline-Strong: A baseline model that implements standard best practices for prompt engineering as I understand them.\n\nBelow is my python implementation of these evaluators. To the best of my ability, I have implemented the SAMRE and Baseline methods as described in the paper (I call the paper's Baseline method \"Baseline-Weak\"). And I have implemented a Baseline-Strong evaluator that implements what I would consider to be standard best practices for prompt engineering, as discussed in an earlier section.\n\n::: {#a0e049f9 .cell code-fold-show='false' execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the implementation code\"}\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Dict\nimport os\nfrom openai import AsyncOpenAI\nimport logging\nimport re\nfrom contextlib import asynccontextmanager\n\n# Define the prompts\nPROMPTS = {\n# Prompt for advocates to defend their assigned answer, used in SAMRE\n\"defend_prompt\": \"\"\"\nYou’re an eloquent advocate (Advocate {advocate_id}) in a group defending this answer:\n{answer} to the question: {question}. Your opponent group defends: {opponent_answer}.\nCollaborate with your fellow advocates to convince the audience your argument is\nbetter. Use the latest feedback, your opponent’s last argument, and your team’s previous\narguments to improve your case.\nLatest feedback: {feedback}\nOpponent’s last argument: {opponent_argument}\nYour team’s previous arguments: team_arguments\nRespond in under 80 words.\nYour defense:\n\"\"\",\n# Prompt for judge to provide feedback on debate progress, used in SAMRE\n\"judge_prompt\": \"\"\"\nYou’re a fair, impartial judge in a debate on: \"{question}\". Answer 1: \"{answer_1}\".\nAnswer 2: \"{answer_2}\". Your goal is to provide feedback that will help advocate groups\nimprove and differentiate their arguments more clearly.\nCurrent round: {current_round}\nTotal rounds: {total_rounds}\nPrevious scores: {previous_scores}\nDefense for 1st answer: {defense_1}\nDefense for 2nd answer: {defense_2}\nProvide specific, constructive feedback to help each advocate group strengthen their\nunique position. Encourage them to address weaknesses and highlight distinctions. Aim\nfor your feedback to lead to more divergent scores in future rounds.\nGive your feedback in under 50 words:\n\"\"\",\n# Prompt for SAMRE method scoring\n\"score_prompt_samre\": \"\"\"\nYou’re a critical, impartial judge in a high-stakes debate on: \"{question}\". Answer\n1: \"{answer_1}\". Answer 2: \"{answer_2}\". Your goal is to provide detailed, constructive\nfeedback that will push advocates to significantly improve their arguments.\nTotal rounds: {total_rounds}\nPrevious scores: {previous_scores}\nDefense for 1st answer: {defense_1}\nDefense for 2nd answer: {defense_2}\nAnalyze each argument meticulously. Be thorough and unbiased in your assessment of:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nFor each criterion, provide a score on a scale of 1-20 and detailed justification.\nScores should be given as [answer_1_score, answer_2_score] for each criterion.\nYour comprehensive feedback for each advocate (50 words each):\nFeedback for Advocate 1:\nFeedback for Advocate 2:\nSum up the scores and return the final score tuple (score1, score2). Example: (95, 87)\nYour detailed scores and final tally:\n\"\"\",\n# Prompt for Baseline-Weak method scoring, which represents the baseline model used in the paper\n\"score_prompt_baseline_weak\": \"\"\"\nYou are a fair, impartial judge scoring a debate on the following question:\nquestion.\nAnswer 1: {answer_1}\nAnswer 2: {answer_2}\nScore each answer on a scale of 1-20 for each of the following criteria:\n1. Relevance to the question\n2. Accuracy of information and use of credible sources\n3. Depth of analysis and completeness of argument\n4. Clarity of expression and logical flow\n5. Strength of reasoning and factual support\n6. Effectiveness in addressing opponent’s points\nProvide scores as [Answer1_score, Answer2_score] for each criterion in a list format,\nthen sum for final scores. Please keep an eye on the slightest difference that should\nmake a difference in the scoring. Don’t overthink!\nRelevance:\nAccuracy:\nDepth:\nClarity:\nLogic and Factuality:\nAddressing opponent’s points:\nFinal Scores (sum of above) as a tuple (example: (18, 9)):\nExplain your scoring, focusing on why one answer is better than the other based on the\ncriteria above. Keep your explanation concise but informative.\nFinally, return the final score tuple (score1, score2) as a tuple (in parentheses).\nExample: (18, 9)\nYour scores and explanation:\n\"\"\",\n# Prompt for Baseline-Strong method scoring, which implements what I consider to be standard best practices for prompt engineering\n\"score_prompt_baseline_strong\": \"\"\"\nYou are a fair, impartial judge scoring a debate on Question.\n\n<Question>\n{question}\n</Question>\n\nTwo Answers have been given to the Question.\n\n<Answer1>\n{answer_1}\n</Answer1>\n\n<Answer2>\n{answer_2}\n</Answer2>\n\nThe Answers are being judged on the following Criteria:\n\n<Criteria>\n<Criterion1>Relevance to their task</Criterion1>\n<Criterion2>Accuracy and credible sources</Criterion2>\n<Criterion3>Depth and completeness</Criterion3>\n<Criterion4>Clarity and logical flow</Criterion4>\n<Criterion5>Reasoning and factual support</Criterion5>\n</Criteria>\n\nFor each Criterion, briefly analyze the performance of \nthe two Answers, then give a score between 1 and 10.\n\nRespond as follows:\n<Criterion1>\n<CriterionName>Relevance to their task</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion1>\n<Criterion2>\n<CriterionName>Accuracy and credible sources</CriterionName>\n<Analysis>\nAnswer 1: [Analysis of Answer 1 performance on the Criterion]\nAnswer 2: [Analysis of Answer 2 performance on the Criterion]\n</Analysis>\n<Scores>\n<Answer1Score>[score between 1 and 10]</Answer1Score>\n<Answer2Score>[score between 1 and 10]</Answer2Score>\n</Scores>\n</Criterion2>\n...\n\"\"\"\n}\n\n@dataclass\nclass Memory:\n    \"\"\"Stores debate history including arguments, scores, and feedback for each round, used in SAMRE\"\"\"\n    arguments: List[Tuple[str, str]] = field(default_factory=list)\n    scores: List[Tuple[float, float]] = field(default_factory=list)\n    feedback: List[str] = field(default_factory=list)\n\nclass ModelEvaluator:\n    @classmethod\n    @asynccontextmanager\n    async def create(cls, mode=\"samre\", model=\"gpt-4o-mini\", logging_level=logging.WARNING):\n        \"\"\"Factory method to create evaluator instance with proper async context management\"\"\"\n        instance = cls(mode=mode, model=model, logging_level=logging_level)\n        instance.client = AsyncOpenAI()\n        try:\n            yield instance\n        finally:\n            await instance.client.close()\n\n    def _setup_logger(self, logging_level):\n        \"\"\"Setup logger with word wrapping.\"\"\"\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging_level)\n        if not logger.handlers:\n            handler = logging.StreamHandler()\n            class WrapFormatter(logging.Formatter):\n                def format(self, record):\n                    import textwrap\n                    message = super().format(record)\n                    return '\\n'.join(textwrap.fill(line, width=80) \n                                for line in message.split('\\n'))\n            \n            formatter = WrapFormatter('%(message)s')\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n        return logger\n\n    def __init__(self, mode=\"samre\", model=\"gpt-4o-mini\", logging_level=logging.WARNING):\n        self.mode = mode\n        self.model = model\n        # Modify to handle both baseline modes\n        self.max_rounds = 1 if mode.startswith(\"baseline\") else 4\n        self.logger = self._setup_logger(logging_level)\n        \n        # Initialize all prompts\n        self.defend_prompt = PROMPTS[\"defend_prompt\"]\n        self.judge_prompt = PROMPTS[\"judge_prompt\"]\n\n\n    async def get_completion(self, prompt: str) -> str:\n        \"\"\"Get a completion from the OpenAI API.\"\"\"\n        if not self.client:\n            raise RuntimeError(\"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'\")\n            \n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"system\", \"content\": prompt}],\n            temperature=0\n        )\n        return response.choices[0].message.content\n\n    def _extract_final_scores(self, score_response: str) -> Tuple[float, float]:\n        \"\"\"Extracts final scores from model response based on evaluation mode\"\"\"\n        if self.mode == \"samre\":\n            # Look for final tuple in format (score1, score2)\n            tuple_pattern = r'\\((\\d+\\.?\\d*),\\s*(\\d+\\.?\\d*)\\)'\n            match = re.search(tuple_pattern, score_response)\n            if match:\n                return (float(match.group(1)), float(match.group(2)))\n            raise ValueError(\"Could not find score tuple in SAMRE response\")\n        \n        elif self.mode == \"baseline_weak\":\n            # Look for final tuple in format (score1, score2)\n            tuple_pattern = r'\\((\\d+\\.?\\d*),\\s*(\\d+\\.?\\d*)\\)'\n            match = re.search(tuple_pattern, score_response)\n            if match:\n                return (float(match.group(1)), float(match.group(2)))\n            raise ValueError(\"Could not find score tuple in weak baseline response\")\n        \n        elif self.mode == \"baseline_strong\":\n            # Use XML parsing for strong baseline\n            score_a_pattern = r'<Answer1Score>\\s*(\\d+\\.?\\d*)\\s*</Answer1Score>'\n            score_b_pattern = r'<Answer2Score>\\s*(\\d+\\.?\\d*)\\s*</Answer2Score>'\n            \n            scores_a = [float(match.group(1)) for match in re.finditer(score_a_pattern, score_response)]\n            scores_b = [float(match.group(1)) for match in re.finditer(score_b_pattern, score_response)]\n            \n            if not scores_a or not scores_b:\n                raise ValueError(\"Could not find scores for both candidates\")\n            \n            if len(scores_a) != len(scores_b):\n                raise ValueError(f\"Mismatched number of scores: A={len(scores_a)}, B={len(scores_b)}\")\n            \n            final_score_a = sum(scores_a) / len(scores_a)\n            final_score_b = sum(scores_b) / len(scores_b)\n            \n            return (final_score_a, final_score_b)\n        \n        else:\n            raise ValueError(f\"Unknown mode: {self.mode}\")\n\n    async def evaluate(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -> Dict:\n        \"\"\"Main evaluation entry point that routes to appropriate evaluation method based on mode\"\"\"\n        if not self.client:\n            raise RuntimeError(\"Evaluator must be created using 'async with ModelEvaluator.create() as evaluator:'\")\n            \n        if self.mode.startswith(\"baseline\"):\n            self.logger.info(f\"\\n=== Starting {self.mode.title()} Evaluation ===\\n\")\n            return await self._evaluate_baseline(question, answer_1, answer_2, num_rounds)\n        else:\n            self.logger.info(\"\\n=== Starting SAMRE Evaluation ===\\n\")\n            return await self._evaluate_samre(question, answer_1, answer_2)\n\n    async def _evaluate_baseline(self, question: str, answer_1: str, answer_2: str, num_rounds: int = 1) -> Dict:\n        \"\"\"Implements baseline evaluation methods (both weak and strong)\"\"\"\n        score_history = []\n        \n        num_rounds = 1 if self.mode == \"baseline_weak\" else num_rounds\n        for _ in range(num_rounds):\n            # Select appropriate prompt based on mode\n            prompt_key = \"score_prompt_\" + self.mode\n            score_prompt = PROMPTS[prompt_key].format(\n                question=question,\n                answer_1=answer_1,\n                answer_2=answer_2\n            )\n            score_response = await self.get_completion(score_prompt)\n            self.logger.info(f\"Score response: {score_response}\")\n            \n            try:\n                round_scores = self._extract_final_scores(score_response)\n                score_history.append(list(round_scores))\n            except Exception as e:\n                self.logger.error(f\"Score parsing error: {e}\")\n                self.logger.error(f\"Raw score response: {score_response}\")\n                score_history.append([10.0, 10.0])\n\n        # Calculate average scores across all rounds\n        avg_scores = [\n            sum(scores[i] for scores in score_history) / len(score_history)\n            for i in range(2)\n        ]\n\n        # Determine winner based on average scores\n        winner = (\n            'model_a' if avg_scores[0] > avg_scores[1]\n            else 'model_b' if avg_scores[0] < avg_scores[1]\n            else 'tie'\n        )\n\n        return {\n            \"winner\": winner,\n            \"average_scores\": [round(score, 2) for score in avg_scores] ,\n            \"rounds\": len(score_history),\n            \"score_history\": score_history,\n            \"full_response\": score_response  # Include the final response for analysis\n        }\n        \n    async def _evaluate_samre(self, question: str, answer_1: str, answer_2: str) -> Dict:\n        \"\"\"Implements SAMRE evaluation with multi-round debate process\n        \n        Flow:\n        1. Get defenses from both advocates\n        2. Judge provides feedback and scores\n        3. Repeat until max rounds or convergence\n        4. Return averaged results\n        \"\"\"\n        local_memory = Memory()\n        \n        self.logger.info(\"\\n=== Starting SAMRE Evaluation ===\\n\")\n        \n        for round_num in range(self.max_rounds):\n            self.logger.info(f\"\\n--- Round {round_num + 1} ---\")\n            \n            scores = await self._run_debate_round(\n                question,\n                answer_1, \n                answer_2, \n                round_num,\n                local_memory\n            )\n            \n            if self._has_scores_converged(round_num, local_memory):\n                self.logger.info(\"\\nScores have converged - ending debate early.\")\n                break\n        \n        return self._prepare_results(local_memory)\n\n    async def defend_answer(self, question: str, answer_1: str, answer_2: str, \n                        advocate_id: int, feedback: str = \"\", \n                        opponent_argument: str = \"\",\n                        team_arguments: List[str] = None) -> str:\n        \"\"\"Get defense from an advocate.\n        \n        Args:\n            question: The question being debated\n            answer_1: First answer in the debate\n            answer_2: Second answer in the debate\n            advocate_id: Which advocate (1 or 2) is defending\n            feedback: Previous feedback from judge\n            opponent_argument: Last argument from opponent\n            team_arguments: List of previous arguments from this advocate's team\n        \"\"\"\n        if team_arguments is None:\n            team_arguments = []\n            \n        # Map answers based on advocate_id\n        answer = answer_1 if advocate_id == 1 else answer_2\n        opponent_answer = answer_2 if advocate_id == 1 else answer_1\n            \n        prompt = self.defend_prompt.format(\n            question=question,\n            advocate_id=advocate_id,\n            answer=answer,  # The answer this advocate is defending\n            opponent_answer=opponent_answer,  # The opposing answer\n            feedback=feedback,\n            opponent_argument=opponent_argument,\n            team_arguments=\"\\n\".join(team_arguments)\n        )\n        return await self.get_completion(prompt)\n\n    async def judge_debate(self, question: str, answer_1: str, answer_2: str,\n                          defense_1: str, defense_2: str, \n                          current_round: int,\n                          memory: Memory) -> Tuple[str, Tuple[float, float]]:\n        \"\"\"Judge the debate between two answers.\"\"\"\n        feedback_prompt = self.judge_prompt.format(\n            question=question,\n            answer_1=answer_1,\n            answer_2=answer_2,\n            current_round=current_round,\n            total_rounds=self.max_rounds,\n            previous_scores=memory.scores,\n            defense_1=defense_1,\n            defense_2=defense_2\n        )\n        feedback = await self.get_completion(feedback_prompt)\n        \n        score_prompt = PROMPTS[\"score_prompt_samre\"].format(\n            question=question,\n            answer_1=answer_1,\n            answer_2=answer_2,\n            defense_1=defense_1,\n            defense_2=defense_2,\n            total_rounds=self.max_rounds,\n            previous_scores=memory.scores,\n            feedback=feedback\n        )\n        score_response = await self.get_completion(score_prompt)    \n        self.logger.info(f\"Score response: {score_response}\")\n        \n        try:\n            scores = self._extract_final_scores(score_response)\n        except Exception as e:\n            self.logger.error(f\"Score parsing error: {e}\")\n            self.logger.error(f\"Raw score response: {score_response}\")\n            scores = (10.0, 10.0)\n        \n        return feedback, scores\n\n    async def _run_debate_round(self, question: str, answer_1: str, answer_2: str, \n                               round_num: int, memory: Memory) -> Tuple[float, float]:\n        \"\"\"Executes single debate round in SAMRE evaluation\"\"\"\n        defenses = await self._get_advocate_defenses(question, answer_1, answer_2, memory)\n        memory.arguments.append(defenses)\n        \n        feedback, scores = await self.judge_debate(\n            question, answer_1, answer_2, defenses[0], defenses[1], round_num + 1, memory\n        )\n        \n        self._store_round_results(feedback, scores, memory)\n        self._display_round_results(defenses, feedback, scores)\n        \n        return scores\n\n    async def _get_advocate_defenses(self, question: str, answer_1: str, answer_2: str,\n                                   memory: Memory) -> Tuple[str, str]:\n        \"\"\"Get defenses from both advocates.\"\"\"\n        defense_1 = await self.defend_answer(\n            question, answer_1, answer_2, 1,\n            feedback=memory.feedback[-1] if memory.feedback else \"\",\n            opponent_argument=memory.arguments[-1][1] if memory.arguments else \"\",\n            team_arguments=[args[0] for args in memory.arguments]\n        )\n        \n        defense_2 = await self.defend_answer(\n            question, answer_1, answer_2, 2,\n            feedback=memory.feedback[-1] if memory.feedback else \"\",\n            opponent_argument=memory.arguments[-1][0] if memory.arguments else \"\",\n            team_arguments=[args[1] for args in memory.arguments]\n        )\n        \n        return (defense_1, defense_2)\n\n    def _store_round_results(self, feedback: str, scores: Tuple[float, float],\n                           memory: Memory) -> None:\n        \"\"\"Store feedback and scores from the round.\"\"\"\n        memory.feedback.append(feedback)\n        memory.scores.append(scores)\n\n    def _display_round_results(self, defenses: Tuple[str, str], \n                             feedback: str, scores: Tuple[float, float]) -> None:\n        \"\"\"Display the results of the current round.\"\"\"\n        self.logger.info(f\"\\nAdvocate 1's defense:\\n{defenses[0]}\")\n        self.logger.info(f\"\\nAdvocate 2's defense:\\n{defenses[1]}\")\n        self.logger.info(f\"\\nJudge's feedback:\\n{feedback}\")\n        self.logger.info(f\"Scores for this round: Answer 1 = {round(scores[0], 2)}, Answer 2 = {round(scores[1], 2)}\")\n\n    def _has_scores_converged(self, round_num: int, memory: Memory) -> bool:\n        \"\"\"Checks if debate scores have converged by comparing last two rounds\"\"\"\n        if round_num > 0:\n            prev_diff = memory.scores[-2][0] - memory.scores[-2][1]\n            curr_diff = memory.scores[-1][0] - memory.scores[-1][1]\n            return (prev_diff * curr_diff) > 0\n        return False\n\n    def _prepare_results(self, memory: Memory) -> Dict:\n        \"\"\"Prepare the final results dictionary.\"\"\"\n        avg_scores = [\n            round(sum(scores[i] for scores in memory.scores) / len(memory.scores), 2)\n            for i in range(2)\n        ]\n        \n        winner = (\n            'model_a' if avg_scores[0] > avg_scores[1]\n            else 'model_b' if avg_scores[0] < avg_scores[1]\n            else 'tie'\n        )\n        \n        return {\n            \"winner\": winner,\n            \"average_scores\": avg_scores,\n            \"rounds\": len(memory.scores),\n            \"score_history\": [[round(s[0], 2), round(s[1], 2)] for s in memory.scores],\n            \"argument_history\": memory.arguments,\n            \"feedback_history\": memory.feedback\n        }\n```\n:::\n\n\n# Load the MT-Bench dataset\n\nFor evaluation, I'll use MT-Bench which is the dataset used in the paper. MT-Bench is a dataset that contains human annotator judgments of preference between two alternative LLM responses.\n\nI'll read the dataset from Llamahub [MtBenchHumanJudgementDataset](https://llamahub.ai/l/llama_datasets/MT%20Bench%20Human%20Judgement%20Dataset?from=), which has simplified the dataset by aggregating human judgments for repeated observations of the same model competitions.\n\n> In the original version, there can be more than one human evaluator for a given example (query, two model responses). In this adapted version however, we aggregate these 'repeated' entries entries and convert the 'winner' column of the original schema to instead represent the proportion of times 'model_a' wins across all of the human evaluators. To adapt this to a llama-dataset, and to better consider ties (albeit with small samples) we set an uncertainty threshold for this proportion in that if it is between [0.4, 0.6] then we consider there to be no winner between the two models.\n\nAlthough it's not entirely clear from this datacard description, the human evaluator judgments were encoded as \"1\" (model_a wins), \"0\" (model_b wins), or \"0.5\" (tie). Essentially, they were aggregated to represent the majority winner across repeated observations.\n\n::: {#71391835 .cell execution_count=2}\n``` {.python .cell-code}\n# Commented out since the dataset is already downloaded\n#!llamaindex-cli download-llamadataset MtBenchHumanJudgementDataset --download-dir ./data\n```\n:::\n\n\n::: {#b6e8f171 .cell code-fold-show='false' execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code to load the dataset\"}\nimport json\nimport pandas as pd\nfrom llama_index.core.llama_dataset import LabelledPairwiseEvaluatorDataset\n\ndf = LabelledPairwiseEvaluatorDataset.from_json(\n    \"./data/pairwise_evaluator_dataset.json\"\n).to_pandas()\n\n# Print the shape of the dataset\nprint(f'Dataset shape: {df.shape}')\n\n# Print the reference_score value counts, just to confirm that this column is encoding the winner as I expect\nprint(f'\\nReference score (winner) value counts: {df[\"reference_score\"].value_counts()}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset shape: (1204, 12)\n\nReference score (winner) value counts: reference_score\n0.0    709\n1.0    433\n0.5     62\nName: count, dtype: int64\n```\n:::\n:::\n\n\nI'll rename some of the columns, and also encode a \"human_winner\" column to indicate whether model_a was preferred, model_b, or if there was a tie. (Note: This is just my own preference for how to represent the data).\n\n::: {#c22b4194 .cell code-fold-show='false' execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code to rename variables and encode a winner column\"}\ndf = df[['query', 'answer', 'second_answer', 'answer_by', 'second_answer_by', 'reference_score']]\n\n# Rename as follows: query => question, answer => model_a_answer, second_answer => model_b_answer, answer_by => model_a, second_answer_by => model_b, reference_score => human_winner\ndf.rename(columns={'query': 'question', 'answer': 'model_a_answer', 'second_answer': 'model_b_answer', 'answer_by': 'model_a', 'second_answer_by': 'model_b', 'reference_score': 'human_winner'}, inplace=True)\n\n# Reencode human winner as \"model_a\" if 1, \"model_b\" if 0, and \"tie\" if 0.5\ndf['human_winner'] = df['human_winner'].apply(lambda x: 'model_a' if x == 1 else 'model_b' if x == 0 else 'tie')\n```\n:::\n\n\nThe original dataset contains ~1200 rows. I'll take a random sample of 300 rows for my testing to save on time and API costs.\n\n::: {#d2f9cc35 .cell code-fold-show='false' execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code to take a random sample of 300 rows\"}\n# Take a random sample of 300 rows\ndf = df.sample(n=300, random_state=42)\n\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>model_a_answer</th>\n      <th>model_b_answer</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>human_winner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>76</th>\n      <td>Write a descriptive paragraph about a bustling...</td>\n      <td>The bustling marketplace teemed with life, a v...</td>\n      <td>Here is a descriptive paragraph about a bustli...</td>\n      <td>ai (gpt-4)</td>\n      <td>ai (claude-v1)</td>\n      <td>model_a</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>Embrace the role of Sheldon from \"The Big Bang...</td>\n      <td>As Sheldon, I am a staunch proponent of hand d...</td>\n      <td>Well, I must say, I find hand dryers to be a r...</td>\n      <td>ai (alpaca-13b)</td>\n      <td>ai (vicuna-13b-v1.2)</td>\n      <td>model_b</td>\n    </tr>\n    <tr>\n      <th>717</th>\n      <td>You are given two sorted lists of size m and n...</td>\n      <td>One approach to solve this problem with linear...</td>\n      <td>To find the kth smallest element in the union ...</td>\n      <td>ai (gpt-3.5-turbo)</td>\n      <td>ai (vicuna-13b-v1.2)</td>\n      <td>model_a</td>\n    </tr>\n    <tr>\n      <th>602</th>\n      <td>Develop a Python program that reads all the te...</td>\n      <td>Here is a Python program that reads all text f...</td>\n      <td>Here's a Python program that reads all the tex...</td>\n      <td>ai (claude-v1)</td>\n      <td>ai (vicuna-13b-v1.2)</td>\n      <td>model_b</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>Act as a math teacher. I will provide some mat...</td>\n      <td>Probability is a branch of mathematics that de...</td>\n      <td>Probability is the likelihood that an event wi...</td>\n      <td>ai (vicuna-13b-v1.2)</td>\n      <td>ai (alpaca-13b)</td>\n      <td>tie</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Use methods to evaluate MT-Bench dataset\n\nUsing this sample of 300 rows from the MT-Bench dataset, I will run the three LLM models (Baseline-Weak, Baseline-Strong, and SAMRE) on each set of question and answers.\n\nThe code below is the main evaluation loop, designed to run multiple evaluations asynchronously (to save time). It will evaluate each item in the dataset, and save the results to disk as a checkpoint. If the evaluation is interrupted, the code can be resumed from the last checkpoint.\n\nI'll use `gpt-4o-mini` for the evaluations. In the paper they had tested models like `gpt-4o` and `gpt-3.5-turbo`, and I would not expect `gpt-4o-mini` to be an exception.\n\n::: {#858797b7 .cell code-fold-show='false' execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code that runs the evaluations\"}\nimport asyncio\nfrom asyncio import Semaphore\nimport logging\nimport os\nimport hashlib\nimport json\nlogging.basicConfig(level=logging.WARNING)\n\nasync def evaluate_conversation_pair(row, evaluators, semaphore, idx, total):\n    \"\"\"Evaluate a single conversation pair with all evaluators\"\"\"\n    async with semaphore:\n        # Add delay between API calls\n        #await asyncio.sleep(1)  # Add small delay between conversations\n        \n        # Generate pair_id from conversation hash\n        pair_id = f\"{row['model_a']}_{row['model_b']}_{hashlib.sha256(str(row['question']).encode()).hexdigest()[:12]}\"\n        checkpoint_file = f'checkpoints/{pair_id}.json'\n        \n        # Return existing checkpoint if available\n        if os.path.exists(checkpoint_file):\n            logging.info(f\"Found existing checkpoint file for {pair_id}\")\n            return json.load(open(checkpoint_file))\n        \n        logging.info(f\"No checkpoint file found for {pair_id}\")\n        result = {\n            'model_a': row['model_a'],\n            'model_b': row['model_b'],\n            'human_winner': row['human_winner'],\n            'pair_id': pair_id\n        }\n        \n        try:\n            # First run SAMRE evaluation with retries\n            for attempt in range(3):  # Try up to 3 times\n                try:\n                    samre_evaluator = evaluators['samre']\n                    samre_result = await samre_evaluator.evaluate(\n                        row['question'], \n                        row['model_a_answer'], \n                        row['model_b_answer']\n                    )\n                    result['samre_winner'] = samre_result['winner']\n                    result.update({f'samre_{k}': samre_result[k] for k in ['average_scores', 'rounds', 'score_history']})\n                    result.update({\n                        'samre_argument_history': samre_result['argument_history'],\n                        'samre_feedback_history': samre_result['feedback_history']\n                    })\n                    break  # If successful, break retry loop\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1  # Exponential backoff\n                        print(f\"Rate limit hit on SAMRE, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:  # Last attempt failed\n                            raise\n                    else:\n                        raise  # Re-raise non-rate-limit errors\n\n            await asyncio.sleep(0.5)  # Add small delay between evaluator calls\n            \n            # Run baseline strong with same number of rounds as SAMRE\n            for attempt in range(3):\n                try:\n                    baseline_strong_evaluator = evaluators['baseline_strong']\n                    baseline_strong_result = await baseline_strong_evaluator.evaluate(\n                        row['question'],\n                        row['model_a_answer'],\n                        row['model_b_answer'],\n                        num_rounds=result['samre_rounds']\n                    )\n                    result['baseline_strong_winner'] = baseline_strong_result['winner']\n                    result.update({f'baseline_strong_{k}': baseline_strong_result[k] \n                                 for k in ['average_scores', 'rounds', 'score_history']})\n                    result['baseline_strong_full_response'] = baseline_strong_result['full_response']\n                    break\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1\n                        print(f\"Rate limit hit on baseline strong, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:\n                            raise\n                    else:\n                        raise\n\n            await asyncio.sleep(0.5)  # Add small delay between evaluator calls\n\n            # Run baseline weak with 1 round\n            for attempt in range(3):\n                try:\n                    baseline_weak_evaluator = evaluators['baseline_weak']\n                    baseline_weak_result = await baseline_weak_evaluator.evaluate(\n                        row['question'],\n                        row['model_a_answer'],\n                        row['model_b_answer'],\n                        num_rounds=1\n                    )\n                    result['baseline_weak_winner'] = baseline_weak_result['winner']\n                    result.update({f'baseline_weak_{k}': baseline_weak_result[k] \n                                 for k in ['average_scores', 'rounds', 'score_history']})\n                    result['baseline_weak_full_response'] = baseline_weak_result['full_response']\n                    break\n                except Exception as e:\n                    if \"rate limit\" in str(e).lower():\n                        wait_time = (2 ** attempt) * 1\n                        print(f\"Rate limit hit on baseline weak, waiting {wait_time} seconds...\")\n                        await asyncio.sleep(wait_time)\n                        if attempt == 2:\n                            raise\n                    else:\n                        raise\n                        \n        except Exception as e:\n            print(f\"Error evaluating row {idx}: {str(e)}\")\n            result['samre_winner'] = None\n            result['baseline_strong_winner'] = None\n            result['baseline_weak_winner'] = None\n            result['error'] = str(e)\n        \n        # Save checkpoint after each evaluation\n        os.makedirs('checkpoints', exist_ok=True)\n        json.dump(result, open(checkpoint_file, 'w'))\n        \n        if (idx + 1) % 10 == 0:\n            print(f\"Processed {idx + 1}/{total} conversations\")\n            \n        return result\n\nasync def evaluate_conversations_async(df, evaluators, semaphore_limit=3):\n    \"\"\"Evaluate conversations asynchronously\"\"\"\n    # Reduce semaphore limit\n    semaphore_limit = 1  # Process one at a time to avoid rate limits\n    \n    # Process in smaller batches\n    batch_size = 10\n    results = []\n    \n    for i in range(0, len(df), batch_size):\n        batch = df.iloc[i:i+batch_size]\n        tasks = [\n            evaluate_conversation_pair(row[1], evaluators, Semaphore(semaphore_limit), idx, len(df))\n            for idx, row in enumerate(batch.iterrows(), start=i)\n        ]\n        batch_results = await asyncio.gather(*tasks)\n        results.extend(batch_results)\n        \n        # Add delay between batches\n        if i + batch_size < len(df):\n            print(f\"Completed batch {i//batch_size + 1}, waiting before next batch...\")\n            #await asyncio.sleep(5)  # 5 second delay between batches\n            \n    return pd.DataFrame(results)\n\nasync def main():\n    async with ModelEvaluator.create(mode=\"samre\") as samre_evaluator, \\\n               ModelEvaluator.create(mode=\"baseline_strong\") as baseline_strong_evaluator, \\\n               ModelEvaluator.create(mode=\"baseline_weak\") as baseline_weak_evaluator:\n        return await evaluate_conversations_async(\n            df,\n            {\n                'samre': samre_evaluator, \n                'baseline_strong': baseline_strong_evaluator,\n                'baseline_weak': baseline_weak_evaluator\n            },\n            semaphore_limit=1\n        )\n\n# Run evaluation with checkpoint recovery\ntry:\n    eval_df = await main()\nexcept Exception as e:\n    print(f\"Error during evaluation: {str(e)}\\nRecovering from checkpoints...\")\n    eval_df = pd.DataFrame([json.load(open(f'checkpoints/{f}')) \n                           for f in os.listdir('checkpoints') \n                           if f.endswith('.json')])\nfinally:\n    eval_df.to_csv('eval_df.csv', index=False)\n    eval_df.head()\n\n# Drop rows with any null values on the model winner columns\neval_df = eval_df.dropna(subset=['baseline_strong_winner', 'baseline_weak_winner', 'samre_winner'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCompleted batch 1, waiting before next batch...\nCompleted batch 2, waiting before next batch...\nCompleted batch 3, waiting before next batch...\nCompleted batch 4, waiting before next batch...\nCompleted batch 5, waiting before next batch...\nCompleted batch 6, waiting before next batch...\nCompleted batch 7, waiting before next batch...\nCompleted batch 8, waiting before next batch...\nCompleted batch 9, waiting before next batch...\nCompleted batch 10, waiting before next batch...\nCompleted batch 11, waiting before next batch...\nCompleted batch 12, waiting before next batch...\nCompleted batch 13, waiting before next batch...\nCompleted batch 14, waiting before next batch...\nCompleted batch 15, waiting before next batch...\nCompleted batch 16, waiting before next batch...\nCompleted batch 17, waiting before next batch...\nCompleted batch 18, waiting before next batch...\nCompleted batch 19, waiting before next batch...\nCompleted batch 20, waiting before next batch...\nCompleted batch 21, waiting before next batch...\nCompleted batch 22, waiting before next batch...\nCompleted batch 23, waiting before next batch...\nCompleted batch 24, waiting before next batch...\nCompleted batch 25, waiting before next batch...\nCompleted batch 26, waiting before next batch...\nCompleted batch 27, waiting before next batch...\nCompleted batch 28, waiting before next batch...\nCompleted batch 29, waiting before next batch...\n```\n:::\n:::\n\n\n# Performance evaluation\n\nNow that the evaluation is complete, I will evaluate the performance of each of the three methods by first looking at how well each method agreed with the human judgments. \n\nI'll use Krippendorff's alpha to measure agreement, since it is a robust measure of agreement that can handle non-binary ratings (among other things).\n\n::: {#7f293d8e .cell code-fold-show='false' execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code that calculates agreement\"}\nfrom krippendorff import alpha\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef calculate_agreement(df, rater1_col, rater2_col):\n    \"\"\"\n    Calculate Krippendorff's alpha between two raters.\n    \n    Args:\n        df: DataFrame containing the ratings\n        rater1_col: Name of first rater's column\n        rater2_col: Name of second rater's column\n    \n    Returns:\n        float: Krippendorff's alpha score\n    \"\"\"\n    # Create label encoder\n    le = LabelEncoder()\n    \n    # Combine all unique values from both columns\n    all_values = pd.concat([df[rater1_col], df[rater2_col]]).unique()\n    le.fit(all_values)\n    \n    # Transform the ratings to numeric values\n    ratings1 = le.transform(df[rater1_col].fillna('missing'))\n    ratings2 = le.transform(df[rater2_col].fillna('missing'))\n    \n    # Reshape data for krippendorff alpha calculation\n    # Each row represents one item, each column represents one rater\n    reliability_data = np.vstack([ratings1, ratings2])\n    \n    return alpha(reliability_data=reliability_data, level_of_measurement='nominal')\n\n# Calculate agreement scores for all methods\nhuman_baseline_strong_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_strong_winner')\nhuman_baseline_weak_agreement = calculate_agreement(eval_df, 'human_winner', 'baseline_weak_winner')\nhuman_samre_agreement = calculate_agreement(eval_df, 'human_winner', 'samre_winner')\n\n# Create a DataFrame with the agreement scores\nagreement_df = pd.DataFrame({\n    'Evaluator Pair': ['Baseline-Strong Agreement with Humans', 'Baseline-Weak Agreement with Humans', 'SAMRE Agreement with Humans'],\n    'Krippendorff Alpha': [human_baseline_strong_agreement, human_baseline_weak_agreement, human_samre_agreement]\n})\n\n# Round the scores to 3 decimal places\nagreement_df['Krippendorff Alpha'] = agreement_df['Krippendorff Alpha'].round(3)\n\n# Calculate the percent difference between Baseline-Strong and Baseline-Weak, and SAMRE and Baseline-Strong\nbaseline_strong_baseline_weak_diff = (human_baseline_strong_agreement - human_baseline_weak_agreement) / human_baseline_strong_agreement\nbaseline_strong_samre_diff = (human_baseline_strong_agreement - human_samre_agreement) / human_baseline_strong_agreement\nsamre_baseline_weak_diff = (human_samre_agreement - human_baseline_weak_agreement) / human_samre_agreement\n\n# Print raw values\nprint(agreement_df)\n\n# Display the percent difference\nprint(\"\\nKrippendorff Alpha Improvements:\")\nprint(f\"SAMRE vs. Baseline-Weak: {samre_baseline_weak_diff:.0%}\")\nprint(f\"Baseline-Strong vs. Baseline-Weak: {baseline_strong_baseline_weak_diff:.0%}\")\nprint(f\"Baseline-Strong vs. SAMRE: {baseline_strong_samre_diff:.0%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          Evaluator Pair  Krippendorff Alpha\n0  Baseline-Strong Agreement with Humans               0.411\n1    Baseline-Weak Agreement with Humans               0.321\n2            SAMRE Agreement with Humans               0.369\n\nKrippendorff Alpha Improvements:\nSAMRE vs. Baseline-Weak: 13%\nBaseline-Strong vs. Baseline-Weak: 22%\nBaseline-Strong vs. SAMRE: 10%\n```\n:::\n:::\n\n\nAlthough none of the methods yielded particularly strong agreement with the human judges in an absolute sense, their relative performance is in line with my predictions:\n\n1. As reported in the paper, SAMRE yielded significantly better agreement than Baseline-Weak (0.369 vs. 0.321, an increase of ~13%).\n2. Baseline-Strong yielded significantly better agreement than Baseline-Weak (0.411 vs. 0.321, an increase of ~22%).\n3. Importantly, Baseline-Strong also yielded significantly better agreement than SAMRE (0.411 vs. 0.321, an increase of ~10%)!\n\nNext, we can also measure performance in terms of binary classification accuracy using Matthews Correlation Coefficient (MCC) as a balanced accuracy metric, while re-encoding the \"winner\" columns to indicate whether model_a was selected as better (1) or not better (0) in each case.\n\n::: {#981d302a .cell code-fold-show='false' execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code that calculates Matthews Correlation Coefficient (MCC)\"}\n# Encode winner as binary\ndef encode_winner_as_binary(winner):\n    return 1 if winner == 'model_a' else 0\n\n# Create binary columns for each evaluator\neval_df['human_model_a_better'] = eval_df['human_winner'].apply(encode_winner_as_binary)\neval_df['baseline_strong_model_a_better'] = eval_df['baseline_strong_winner'].apply(encode_winner_as_binary)\neval_df['baseline_weak_model_a_better'] = eval_df['baseline_weak_winner'].apply(encode_winner_as_binary)\neval_df['samre_model_a_better'] = eval_df['samre_winner'].apply(encode_winner_as_binary)\n\nfrom sklearn.metrics import matthews_corrcoef\n\n# Calculate MCC for each method\nmetrics_df = pd.DataFrame({\n    'Method': ['Baseline-Strong', 'Baseline-Weak', 'SAMRE'],\n    'MCC': [\n        matthews_corrcoef(\n            eval_df['human_model_a_better'], \n            eval_df['baseline_strong_model_a_better']\n        ),\n        matthews_corrcoef(\n            eval_df['human_model_a_better'], \n            eval_df['baseline_weak_model_a_better']\n        ),\n        matthews_corrcoef(\n            eval_df['human_model_a_better'], \n            eval_df['samre_model_a_better']\n        )\n    ]\n})\n\n# Round the scores to 3 decimal places\nmetrics_df['MCC'] = metrics_df['MCC'].round(3)\n\n# Calculate the percent differences\ndef calc_percent_diff(new, old):\n    return (new - old) / old * 100\n\n# MCC differences\nsamre_baseline_weak_mcc_diff = calc_percent_diff(\n    metrics_df.loc[metrics_df['Method'] == 'SAMRE', 'MCC'].iloc[0],\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Weak', 'MCC'].iloc[0]\n)\nbaseline_strong_baseline_weak_mcc_diff = calc_percent_diff(\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Strong', 'MCC'].iloc[0],\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Weak', 'MCC'].iloc[0]\n)\nbaseline_strong_samre_mcc_diff = calc_percent_diff(\n    metrics_df.loc[metrics_df['Method'] == 'Baseline-Strong', 'MCC'].iloc[0],\n    metrics_df.loc[metrics_df['Method'] == 'SAMRE', 'MCC'].iloc[0]\n)\n\n# Print raw values\nprint(metrics_df)\n\nprint(\"\\nMCC Improvements:\")\nprint(f\"SAMRE vs. Baseline-Weak: {samre_baseline_weak_mcc_diff:.0f}%\")\nprint(f\"Baseline-Strong vs. Baseline-Weak: {baseline_strong_baseline_weak_mcc_diff:.0f}%\")\nprint(f\"Baseline-Strong vs. SAMRE: {baseline_strong_samre_mcc_diff:.0f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Method    MCC\n0  Baseline-Strong  0.482\n1    Baseline-Weak  0.417\n2            SAMRE  0.401\n\nMCC Improvements:\nSAMRE vs. Baseline-Weak: -4%\nBaseline-Strong vs. Baseline-Weak: 16%\nBaseline-Strong vs. SAMRE: 20%\n```\n:::\n:::\n\n\nLooking at MCC values, we observe a similar pattern of findings to the Krippendorff alphas:\n\n1. SAMRE did not perform better than Baseline-Weak, in fact it performed slightly worse (0.401 vs. 0.417, a decrease of 4%). This is a bit different than what we saw with Krippendorff alpha.\n2. Baseline-Strong performed better than Baseline-Weak (0.482 vs. 0.401, an increase of 16%).\n3. Baseline-Strong performed better than SAMRE (0.464 vs. 0.401, an increase of 20%).\n\n_Side-note: Why does MCC disagree with the Krippendorff alpha on the SAMRE vs. Baseline-Weak comparison? I would guess this is due to how ties were resolved when encoding the winner as binary._\n\nFinally, we can look at accuracy in terms of percentage agreement. Percentage agreement is not a \"balanced\" accuracy metric and therefore needs to be used with caution (for example, if the classes are imbalanced, then percentage agreement accuracy can be misleading). But it is the metric used in the paper.\n\n::: {#41f74433 .cell code-fold-show='false' execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code that calculates percentage agreement\"}\n# Calculate percentage agreement for each method\ndef calculate_percent_agreement(df, rater1_col, rater2_col):\n    \"\"\"Calculate percentage agreement between two raters\"\"\"\n    return (df[rater1_col] == df[rater2_col]).mean()\n\n# Calculate agreement percentages\nagreement_percentages = pd.DataFrame({\n    'Method': ['Baseline-Strong', 'Baseline-Weak', 'SAMRE'],\n    'Agreement': [\n        calculate_percent_agreement(eval_df, 'human_winner', 'baseline_strong_winner'),\n        calculate_percent_agreement(eval_df, 'human_winner', 'baseline_weak_winner'),\n        calculate_percent_agreement(eval_df, 'human_winner', 'samre_winner')\n    ]\n})\n\n# Round to 3 decimal places and convert to percentage\nagreement_percentages['Agreement'] = (agreement_percentages['Agreement'] * 100).round(1)\n\n# Calculate the percentage point differences\nsamre_baseline_weak_diff = (\n    agreement_percentages.loc[agreement_percentages['Method'] == 'SAMRE', 'Agreement'].iloc[0] -\n    agreement_percentages.loc[agreement_percentages['Method'] == 'Baseline-Weak', 'Agreement'].iloc[0]\n)\nbaseline_strong_baseline_weak_diff = (\n    agreement_percentages.loc[agreement_percentages['Method'] == 'Baseline-Strong', 'Agreement'].iloc[0] -\n    agreement_percentages.loc[agreement_percentages['Method'] == 'Baseline-Weak', 'Agreement'].iloc[0]\n)\nbaseline_strong_samre_diff = (\n    agreement_percentages.loc[agreement_percentages['Method'] == 'Baseline-Strong', 'Agreement'].iloc[0] -\n    agreement_percentages.loc[agreement_percentages['Method'] == 'SAMRE', 'Agreement'].iloc[0]\n)\n\n# Print raw values\nprint(\"Percentage Agreement with Human Judgments:\")\nprint(agreement_percentages)\n\nprint(\"\\nPercentage Point Differences:\")\nprint(f\"SAMRE vs. Baseline-Weak: {samre_baseline_weak_diff:+.1f}\")\nprint(f\"Baseline-Strong vs. Baseline-Weak: {baseline_strong_baseline_weak_diff:+.1f}\")\nprint(f\"Baseline-Strong vs. SAMRE: {baseline_strong_samre_diff:+.1f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPercentage Agreement with Human Judgments:\n            Method  Agreement\n0  Baseline-Strong       68.4\n1    Baseline-Weak       62.8\n2            SAMRE       67.0\n\nPercentage Point Differences:\nSAMRE vs. Baseline-Weak: +4.2\nBaseline-Strong vs. Baseline-Weak: +5.6\nBaseline-Strong vs. SAMRE: +1.4\n```\n:::\n:::\n\n\nOverall across these three metrics, the story is the same: SAMRE did not perform better than a baseline that is designed with best practices.\n\n# Conclusion\n\nIn this post, I have shown that SAMRE does not perform better than a well-engineered baseline method. Prompt engineers need to remain cautious and resist the urge to use complex methods that may seem more sophisticated than standard best practices, without first testing them against a well-engineered baseline.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}