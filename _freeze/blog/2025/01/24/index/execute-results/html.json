{
  "hash": "8cc78d69d9003bf8c27ef999eb02bd25",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Analysis of the GenAI/LLM job market in January 2025\"\ndate: 2025-01-24\ndescription: \"A data-driven exploration of the  GenAI/LLM job market for science and engineering roles in January 2025. I scrape ~1000 job postings from ai-jobs.net, perform data extraction and classification using LLMs, and ultimately analyze the data to identify patterns and insights about the GenAI/LLM job market, including salary ranges, skill requirements, and role distribution.\"\ncategories:\n  - prompt-engineering\n  - python\n  - job-market\n  - GenAI\n  - LLM\nfreeze: true\n---\n\n\n# Introduction\n\nAs someone currently working in a \"Prompt Engineering\" role, I've been thinking a lot about how this title basically doesn't exist outside of a handful of companies, how the title communicates a narrow range of skills and responsibilities,and how the work that I do day-to-day is much larger in scope than just writing prompts. I identify more as an AI Engineer or AI Research Scientist, and so I was interested to see what I could learn about other similar roles that work with GenAI and LLMs.\n\nSo with that motivation, I set about to collect some data and look at the job market for these sorts of roles. What responsibilities and skills are being advertised most often, what kinds of titles are being used for these roles, and what kind of compensation is being offered at different levels of seniority?\n\nTo accomplish this, I built a custom web scraper to collect ~1000 job postings from ai-jobs.net. My code gathers job details like the title, company, location, salary, and posting date -- from a range of U.S. and Canadian cities, covering entry-level, mid-level, and senior positions. I then use a Large Language Model (LLM) to extract each job's key responsibilities, required skills, and qualifications. Afterwards, I classify each position to see whether it involves working with Generative AI or large language models, and if so, categorize it further into four major AI roles: (1) AI Research Scientist, (2) AI/ML Engineer, (3) MLOps/AI Infrastructure Engineer, and (4) AI Solution Architect. I believe this set of roles is a good representation of different areas of focus.\n\nFinally, I integrate the various metadata and classifications into a comprehensive dataset. I observe that GenAI/LLM positions command consistently high salary ranges across the four different roles, particularly at more senior levels. Senior-level roles tended to offer median salaries in the $195K–$210K range, while mid-level roles generally clustered around $165K–$180K. Entry-level salaries showed greater variation (likely due to the small sample size) but still landed in competitive ranges of roughly $155K–$205K in many postings. These roles often share common technical demands—like proficiency with large-scale model training, distributed computing, and LLM-specific knowledge—though each role emphasizes distinct priorities (research vs. production, for example).\n\n# Scraping job postings\n\nI'll scrape job postings from ai-jobs.net.\n\nThe code below implements a web scraper for job postings from ai-jobs.net. It collects job postings from major cities in the US and Canada, searching across entry-level, mid-level, and senior positions. For each job, it gets the title, company, location, salary, description, and posting date. The scraper saves each job as a JSON file and keeps track of what it has already scraped to avoid duplicates. It includes error handling and logging to track any problems that occur during scraping.\n\n::: {#fac2c9f9 .cell code-fold-show='false' execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the Job Posting Scraper code\"}\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional, Set\nfrom datetime import datetime, timedelta\nimport logging\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\nfrom pathlib import Path\nimport re\nimport time\n\n@dataclass\nclass JobData:\n    \"\"\"Structured container for job posting data\"\"\"\n    title: str\n    company: str\n    location: str\n    level: Optional[str]\n    salary: Optional[str]\n    url: str\n    description: str\n    scraped_date: str\n    posted_date: Optional[str]\n    raw_data: str\n\nclass ScrapeConfig:\n    \"\"\"Configuration settings for the scraper\"\"\"\n    EXPERIENCE_LEVELS = ['EN', 'MI', 'SE']  # Entry, Mid, Senior\n    CITIES = {\n        '5391959': 'San Francisco',\n        '5128581': 'New York City',\n        '6167865': 'Toronto',\n        '6173331': 'Vancouver',\n        '5809844': 'Seattle',\n        '4671654': 'Austin',\n        '4930956': 'Boston',\n        '5': 'Region'\n    }\n    CATEGORIES = {\n        '1': 'Research',\n        '2': 'Engineering',\n        '18': 'GenerativeAI'\n    }\n    BASE_URL = \"https://ai-jobs.net\"\n\nclass SalaryExtractor:\n    \"\"\"Handles salary extraction logic\"\"\"\n    @staticmethod\n    def extract_from_schema(schema_data: Dict) -> Optional[str]:\n        \"\"\"Extract salary from schema.org data\"\"\"\n        try:\n            if schema_data and 'baseSalary' in schema_data:\n                base_salary = schema_data.get('baseSalary', {}).get('value', {})\n                if base_salary:\n                    min_value = base_salary.get('minValue')\n                    max_value = base_salary.get('maxValue')\n                    currency = base_salary.get('currency', 'USD')  # Default to USD if not specified\n                    if min_value and max_value:\n                        return f\"{currency} {float(min_value)/1000:.0f}K - {float(max_value)/1000:.0f}K\"\n                    elif min_value:\n                        return f\"{currency} {float(min_value)/1000:.0f}K+\"\n        except Exception as e:\n            logging.debug(f\"Error extracting schema salary: {e}\")\n        return None\n\n    @staticmethod\n    def extract_from_text(description: str) -> Optional[str]:\n        \"\"\"Extract salary from description text\"\"\"\n        patterns = [\n            r'(?:salary range.*?)(?:CAD|\\$)?([\\d,]+)\\s*-\\s*(?:CAD|\\$)?([\\d,]+)',\n            r'(?:salary.*?)(?:CAD|\\$)?([\\d,]+)(?:\\s*-\\s*(?:CAD|\\$)?([\\d,]+))?'\n        ]\n        \n        for pattern in patterns:\n            if match := re.search(pattern, description, re.IGNORECASE):\n                try:\n                    groups = match.groups()\n                    # Check if salary is in CAD\n                    currency = 'CAD' if 'CAD' in description.upper() else 'USD'\n                    if len(groups) == 2 and groups[1]:  # Range format\n                        min_sal = float(groups[0].replace(',', ''))\n                        max_sal = float(groups[1].replace(',', ''))\n                        return f\"{currency} {min_sal/1000:.0f}K - {max_sal/1000:.0f}K\"\n                    elif groups[0]:  # Single value format\n                        base_sal = float(groups[0].replace(',', ''))\n                        return f\"{currency} {base_sal/1000:.0f}K+\"\n                except ValueError:\n                    continue\n        return None\n\n    @classmethod\n    def extract(cls, soup: BeautifulSoup, description: str, schema_data: Dict) -> Optional[str]:\n        \"\"\"Main salary extraction method\"\"\"\n        # Try schema data first\n        if salary := cls.extract_from_schema(schema_data):\n            return salary\n            \n        # Try description text\n        if salary := cls.extract_from_text(description):\n            return salary\n            \n        # Try salary badge\n        if salary_badge := soup.find('span', class_='badge rounded-pill text-bg-success'):\n            salary_text = salary_badge.text.strip()\n            if re.search(r'(USD|\\$|\\d)', salary_text):\n                return salary_text\n                \n        return None\n\nclass JobPageParser:\n    \"\"\"Handles parsing of individual job pages\"\"\"\n    def __init__(self, html: str):\n        self.soup = BeautifulSoup(html, 'html.parser')\n        self.raw_html = html\n        \n    def parse_schema_data(self) -> Dict:\n        \"\"\"Parse schema.org JSON-LD data\"\"\"\n        if script := self.soup.find('script', type='application/ld+json'):\n            try:\n                cleaned_script = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', script.string)\n                return json.loads(cleaned_script)\n            except Exception as e:\n                logging.warning(f\"Could not parse JSON-LD data: {e}\")\n        return {}\n        \n    def parse_job_data(self, url: str) -> JobData:\n        \"\"\"Extract all job data from the page\"\"\"\n        schema_data = self.parse_schema_data()\n        scraped_date = datetime.now().isoformat()\n        \n        # Get company name from schema or fallback to page element\n        company = schema_data.get('hiringOrganization', {}).get('name')\n        if not company:\n            if company_elem := self.soup.find('a', class_='company-name'):\n                company = company_elem.text.strip()\n                \n        # Get job level from meta description\n        level = None\n        if meta_desc := self.soup.find('meta', {'name': 'description'}):\n            if level_match := re.search(r'a ([\\w-]+level)', meta_desc['content']):\n                level = level_match.group(1).replace('-', '-level / ').title()\n        \n        return JobData(\n            title=self.soup.find('h1', class_='display-5').text.strip(),\n            company=company,\n            location=self.soup.find('h3', class_='lead').text.strip(),\n            level=level,\n            salary=SalaryExtractor.extract(\n                self.soup,\n                self.soup.find('div', class_='job-description-text').text.strip(),\n                schema_data\n            ),\n            url=url,\n            description=self.soup.find('div', class_='job-description-text').text.strip(),\n            scraped_date=scraped_date,\n            posted_date=self.calculate_posted_date(scraped_date),\n            raw_data=self.raw_html\n        )\n        \n    def calculate_posted_date(self, scraped_date_str: str) -> Optional[str]:\n        \"\"\"Calculate posting date from 'posted X time ago' text\"\"\"\n        if match := re.search(r'Posted (\\d+) (hours?|days?|weeks?|months?) ago', self.raw_html):\n            number = int(match.group(1))\n            unit = match.group(2)\n            \n            scraped_date = datetime.fromisoformat(scraped_date_str)\n            delta = {\n                'hour': timedelta(hours=number),\n                'day': timedelta(days=number),\n                'week': timedelta(weeks=number),\n                'month': timedelta(days=number * 30)  # Approximate\n            }.get(unit.rstrip('s'))\n            \n            if delta:\n                return (scraped_date - delta).isoformat()\n        return None\n\nclass AIJobScraper:\n    \"\"\"Main scraper orchestration class\"\"\"\n    def __init__(self, output_dir: str = 'json_data'):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n        self.session = requests.Session()\n        self.config = ScrapeConfig()\n        self.existing_jobs = self._load_existing_jobs()\n        \n    def _load_existing_jobs(self) -> Set[str]:\n        \"\"\"Load set of existing job URLs\"\"\"\n        existing_jobs = set()\n        for f in self.output_dir.glob('*.json'):\n            try:\n                data = json.loads(f.read_text())\n                existing_jobs.add(data['url'])\n            except Exception as e:\n                logging.warning(f\"Error reading {f}: {e}\")\n        return existing_jobs\n        \n    def _generate_search_urls(self) -> List[str]:\n        \"\"\"Generate all search URL combinations\"\"\"\n        urls = []\n        for exp in self.config.EXPERIENCE_LEVELS:\n            for cat in self.config.CATEGORIES:\n                for city in self.config.CITIES:\n                    url = f\"{self.config.BASE_URL}/?\"\n                    url += f\"cat={cat}\"\n                    url += f\"&{'reg' if city == '5' else 'cit'}={city}\"\n                    url += f\"&typ=1&key=&exp={exp}&sal=\"\n                    urls.append(url)\n        return urls\n        \n    def get_job_urls(self, search_url: str) -> List[str]:\n        \"\"\"Get all job URLs from a search page\"\"\"\n        try:\n            response = self.session.get(search_url)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            if job_list := soup.find('ul', id='job-list'):\n                return [\n                    f\"{self.config.BASE_URL}{link['href']}\"\n                    for link in job_list.find_all('a', href=lambda h: h and '/job/' in h)\n                ]\n        except Exception as e:\n            logging.error(f\"Error fetching job URLs: {e}\")\n        return []\n        \n    def scrape_job_details(self, url: str) -> Optional[JobData]:\n        \"\"\"Scrape details from a job page\"\"\"\n        try:\n            response = self.session.get(url)\n            response.raise_for_status()\n            parser = JobPageParser(response.text)\n            return parser.parse_job_data(url)\n        except Exception as e:\n            logging.error(f\"Error scraping job {url}: {e}\")\n        return None\n        \n    def save_job(self, job: JobData) -> None:\n        \"\"\"Save job data to JSON file\"\"\"\n        try:\n            safe_title = re.sub(r'[^\\w\\s-]', '', job.title)\n            safe_company = re.sub(r'[^\\w\\s-]', '', job.company or 'unknown')\n            filename = f\"{safe_company}_{safe_title}_{hash(job.url)}.json\"\n            \n            self.output_dir.joinpath(filename).write_text(\n                json.dumps(vars(job), indent=2, ensure_ascii=False)\n            )\n            logging.info(f\"Saved job: {job.title} at {job.company}\")\n            \n        except Exception as e:\n            logging.error(f\"Error saving job: {e}\")\n            raise\n            \n    def scrape_jobs(self, test: bool = False) -> int:\n        \"\"\"Main scraping method\"\"\"\n        jobs_found = jobs_skipped = 0\n        \n        for search_url in self._generate_search_urls():\n            logging.debug(f\"Processing search URL: {search_url}\")\n            \n            job_urls = self.get_job_urls(search_url)\n            if test and job_urls:\n                job_urls = job_urls[:1]\n                \n            for url in job_urls:\n                if url in self.existing_jobs:\n                    jobs_skipped += 1\n                    continue\n                    \n                if job_data := self.scrape_job_details(url):\n                    self.save_job(job_data)\n                    self.existing_jobs.add(url)\n                    jobs_found += 1\n                    \n                time.sleep(1)  # Rate limiting\n                \n        logging.info(f\"Found {jobs_found} new jobs. Skipped {jobs_skipped} existing jobs.\")\n        return {'jobs_found': jobs_found, 'jobs_skipped': jobs_skipped}\n\ndef scrape_jobs(test: bool = False, verbose: bool = False) -> int:\n    \"\"\"Convenience function to run the scraper\"\"\"\n    if not verbose:\n        logging.getLogger().setLevel(logging.WARNING)\n        \n    try:\n        scraper = AIJobScraper(output_dir=Path.cwd() / 'json_data')\n        return scraper.scrape_jobs(test=test)\n    except Exception as e:\n        logging.error(f\"Scraper failed: {e}\")\n        raise\n```\n:::\n\n\n::: {#b7014a82 .cell execution_count=2}\n``` {.python .cell-code}\n#logging.basicConfig(level=logging.INFO)\n#scraper = AIJobScraper()\n#scrape_results = scraper.scrape_jobs(test=False)\n#print(f\"Found {scrape_results['jobs_found']} new jobs, skipped {scrape_results['jobs_skipped']} existing jobs\")\n```\n:::\n\n\n# General-purpose LLM completion function\n\nFor the classification tasks, I'll write a general-purpose LLM completion function. This function takes a prompt and a model as parameters, and returns the completion from the OpenAI API.\n\n::: {#3211a1e2 .cell execution_count=3}\n``` {.python .cell-code}\nfrom openai import AsyncOpenAI\nimport os\n\nasync def get_completion(prompt: str, model: str = \"gpt-4o-2024-08-06\") -> str:\n    \"\"\"Get a completion from the OpenAI API.\"\"\"\n    client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))  # Initialize with API key from env vars\n    response = await client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"system\", \"content\": prompt}],\n        temperature=0\n    )\n    return response.choices[0].message.content\n```\n:::\n\n\n# Extract Responsibilities, Skills, and Qualifications\n\nThe code below uses an LLM to extract key information from job postings that will be used to classify them into different role categories. It uses OpenAI's GPT-4o model to analyze job descriptions and extract three key components: responsibilities, skills, and qualifications. The system processes multiple jobs concurrently with rate limiting, saves the extracted data as JSON files, and includes retry logic for handling API rate limits. The code uses `asyncio` for concurrent processing and includes error handling and logging. It also checks for previously processed jobs to avoid duplicate work.\n\nAlso included is code that implements data preprocessing steps. This code has functions to load JSON files into a pandas DataFrame, uses a list of keywords to filter out certain jobs that are not relevant to the analysis, and removes duplicate or highly similar job descriptions using TF-IDF vectorization and cosine similarity. The main function `process_job_listings()` combines these steps, taking a directory path, similarity threshold, and filter keywords as parameters. It returns a dictionary containing the processed DataFrame along with counts of the original and filtered entries.\n\n::: {#8044692d .cell code-fold-show='false' execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code for LLM extraction\"}\nimport os\nimport json\nimport logging\nimport asyncio\nimport hashlib\nimport nest_asyncio\nfrom typing import Dict, List, Optional\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom openai import AsyncOpenAI, RateLimitError\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\n\n# Enable nested event loops for Jupyter notebooks\nnest_asyncio.apply()\n\n# Set up logging configuration\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\nPROMPTS = {\n    \"prompt\": \"\"\"\n    You will be given a Job Description.\n\n    Your task is to extract a list of Responsibilities,\n    Skills, and Qualifications. \n\n    - Responsibilities are the tasks and activities that the job requires the employee to perform.\n    - Skills are the abilities and knowledge that the employee needs to have to perform the responsibilities.\n    - Qualifications are the requirements that the employee needs to meet to be considered for the job.\n\n    <JobDescription>\n    {job_description}\n    </JobDescription>\n\n    Return a list of Responsibilities, Skills, and Qualifications as follows:\n    <Responsibilities>\n    [Bullet point list of responsibilities]\n    </Responsibilities>\n    <Skills>\n    [Bullet point list of skills]\n    </Skills>\n    <Qualifications>\n    [Bullet point list of qualifications]\n    </Qualifications>\n    \"\"\"\n}\n\nclass JobProcessor:\n    def __init__(self, output_dir='json_extracted_data'):\n        self.output_dir = output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        self.semaphore = asyncio.Semaphore(3)\n        self.processed = 0\n        self.skipped = 0\n\n    def _parse_llm_response(self, response: str) -> Dict[str, List[str]]:\n        \"\"\"Parse LLM response into structured data.\"\"\"\n        result = {\n            \"responsibilities\": [],\n            \"skills\": [],\n            \"qualifications\": []\n        }\n        \n        sections = {\n            \"responsibilities\": r'<Responsibilities>\\n(.*?)\\n</Responsibilities>',\n            \"skills\": r'<Skills>\\n(.*?)\\n</Skills>',\n            \"qualifications\": r'<Qualifications>\\n(.*?)\\n</Qualifications>'\n        }\n        \n        for section, pattern in sections.items():\n            if match := re.search(pattern, response, re.DOTALL):\n                result[section] = [\n                    item.strip('- ') \n                    for item in match.group(1).strip().split('\\n')\n                    if item.strip('- ')  # Filter out empty items\n                ]\n        \n        return result\n\n    async def extract_job_details(self, job_id: str, job_description: str) -> Dict[str, List[str]]:\n        \"\"\"Extract structured information from job description using LLM.\"\"\"\n        try:\n            prompt = PROMPTS[\"prompt\"].format(job_description=job_description)\n            response = await get_completion(prompt)\n            result = self._parse_llm_response(response)\n            \n            # Save results\n            json_path = os.path.join(self.output_dir, f'{job_id}.json')\n            with open(json_path, 'w', encoding='utf-8') as f:\n                json.dump(result, f, indent=2, ensure_ascii=False)\n            \n            return result\n            \n        except Exception as e:\n            logging.error(f\"Error extracting job details: {str(e)}\")\n            return {\"responsibilities\": [], \"skills\": [], \"qualifications\": []}\n\n    async def process_single_job(self, job_id: str, description: str) -> Dict[str, List[str]]:\n        \"\"\"Process a single job with caching.\"\"\"\n        json_path = os.path.join(self.output_dir, f'{job_id}.json')\n        \n        if os.path.exists(json_path):\n            self.skipped += 1\n            logging.debug(f\"Skipping job ID {job_id} - already processed\")\n            with open(json_path, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        \n        async with self.semaphore:\n            self.processed += 1\n            logging.info(f\"Processing job ID: {job_id}\")\n            return await self.extract_job_details(job_id, description)\n\n    async def process_jobs(self, df: pd.DataFrame) -> List[Dict[str, List[str]]]:\n        \"\"\"Process multiple jobs concurrently.\"\"\"\n        tasks = [\n            self.process_single_job(str(row['id']), row['description'])\n            for _, row in df.iterrows()\n        ]\n        \n        results = await asyncio.gather(*tasks)\n        \n        print(f\"\\nProcessing complete:\")\n        print(f\"- New jobs processed: {self.processed}\")\n        print(f\"- Skipped jobs (already processed): {self.skipped}\")\n        \n        return results\n\nclass DataPreprocessor:\n    def __init__(self, similarity_threshold=0.8):\n        self.similarity_threshold = similarity_threshold\n        self.default_filter_keywords = [\n            'Dir ', 'Director', 'Intern', 'Data Scientist', 'Data Science',\n            'Content Writer', 'Faculty', 'Product Owner', 'Manager',\n            'Analyst', 'Postdoctoral', 'Postdoc', 'Summer'\n        ]\n\n    @staticmethod\n    def load_json_files(directory='json_data') -> pd.DataFrame:\n        \"\"\"Load JSON files into DataFrame with error handling.\"\"\"\n        df = pd.DataFrame()\n        \n        if not os.path.exists(directory):\n            logging.error(f\"Directory {directory} does not exist\")\n            return df\n        \n        for file in [f for f in os.listdir(directory) if f.endswith('.json')]:\n            try:\n                with open(os.path.join(directory, file), 'r') as f:\n                    data = json.load(f)\n                    df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)\n            except Exception as e:\n                logging.error(f\"Error loading {file}: {e}\")\n        \n        if not df.empty:\n            df['id'] = df['url'].apply(\n                lambda x: f\"j{hashlib.md5(x.encode()).hexdigest()[:5]}\"\n            )\n        \n        return df\n\n    def filter_by_title(self, df: pd.DataFrame, \n                       filter_keywords: Optional[List[str]] = None) -> pd.DataFrame:\n        \"\"\"Filter DataFrame by job titles.\"\"\"\n        if df.empty:\n            return df\n            \n        keywords = filter_keywords or self.default_filter_keywords\n        return df[~df['title'].str.contains(\n            '|'.join(keywords),\n            case=False,\n            na=False\n        )]\n\n    def remove_similar_descriptions(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Remove similar job descriptions using TF-IDF and cosine similarity.\"\"\"\n        if df.empty:\n            return df\n            \n        tfidf = TfidfVectorizer(stop_words='english')\n        tfidf_matrix = tfidf.fit_transform(df['description'].fillna(''))\n        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n        \n        indices_to_remove = {\n            j for i in range(len(df))\n            for j in range(i + 1, len(df))\n            if cosine_sim[i][j] > self.similarity_threshold\n        }\n        \n        return df.iloc[~df.index.isin(list(indices_to_remove))]\n\ndef extract_jobs_from_dataframe(df: pd.DataFrame) -> List[Dict[str, List[str]]]:\n    \"\"\"Wrapper function to process jobs from a DataFrame.\"\"\"\n    processor = JobProcessor()\n    \n    loop = asyncio.get_event_loop()\n    if loop.is_running():\n        return loop.create_task(processor.process_jobs(df))\n    else:\n        return asyncio.run(processor.process_jobs(df))\n\ndef process_job_listings(directory='json_data', similarity_threshold=0.8, \n                        filter_keywords=None) -> Dict:\n    \"\"\"Main function to process job listings.\"\"\"\n    preprocessor = DataPreprocessor(similarity_threshold)\n    \n    # Load and preprocess data\n    df = preprocessor.load_json_files(directory)\n    initial_count = len(df)\n    \n    if df.empty:\n        return {\n            'processed_data': df,\n            'original_count': 0,\n            'filtered_count': 0\n        }\n    \n    df = preprocessor.filter_by_title(df, filter_keywords)\n    df_filtered = preprocessor.remove_similar_descriptions(df)\n    \n    return {\n        'processed_data': df_filtered,\n        'original_count': initial_count,\n        'filtered_count': len(df_filtered)\n    }\n```\n:::\n\n\n::: {#d74548e7 .cell execution_count=5}\n``` {.python .cell-code}\nresults = process_job_listings()\ndf = results['processed_data']\nprint(f'Original count of listings: {results[\"original_count\"]}')\nprint(f'Filtered count of listings: {results[\"filtered_count\"]}')\nextract_jobs_from_dataframe(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal count of listings: 999\nFiltered count of listings: 680\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<Task pending name='Task-1' coro=<JobProcessor.process_jobs() running at /tmp/ipykernel_1334300/2751672584.py:118>>\n```\n:::\n:::\n\n\n# Classification\n\n## Classify jobs as relevant to GenAI/LLM work or not\n\n::: {#4dfd7c79 .cell code-fold-show='false' execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code for job classification as GenAI/LLM relevant or not\"}\nimport nest_asyncio\nimport re\nimport asyncio\nimport logging\nfrom typing import List, Dict, Set\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom openai import AsyncOpenAI\nimport os\nfrom openai import RateLimitError\nimport json\n\nclass JobClassifierGenAI:\n    \"\"\"Handles classification of jobs for GenAI/LLM work\"\"\"\n    \n    PROMPT = \"\"\"\n    You will be given a list of Responsibilities and\n    Skills listed for a job. Your task is to determine\n    if the job involves working with Generative AI (GenAI)\n    or language models (a.k.a. Large Language Models (LLMs)).\n\n    <Job>\n    <Responsibilities>\n    {responsibilities}\n    </Responsibilities>\n    <Skills>\n    {skills}\n    </Skills>\n    </Job>\n\n    Start by thinking step-by-step about the Job and its \n    Responsibilities and Skills, and whether it involves\n    working with Generative AI (GenAI) or language models\n    (a.k.a. Large Language Models (LLMs)).\n\n    Return your response in the following format:\n    <Analysis>\n    [Your analysis of the job and its Responsibilities and Skills]\n    </Analysis>\n    <FinalAnswer>\n    true|false\n    </FinalAnswer>\n    \"\"\"\n\n    def __init__(self, output_dir='role_genai_classifications', batch_size=3):\n        self.output_dir = output_dir\n        self.batch_size = batch_size\n        self.semaphore = asyncio.Semaphore(batch_size)\n        os.makedirs(output_dir, exist_ok=True)\n\n    async def classify_job(self, job: Dict) -> Dict:\n        \"\"\"Classify a single job listing\"\"\"\n        logging.info(f\"Classifying job '{job['filename']}'\")\n        \n        prompt = self.PROMPT.format(\n            responsibilities=job['responsibilities'],\n            skills=job.get('skills', '')\n        )\n        \n        try:\n            response = await get_completion(prompt)\n            await asyncio.sleep(2)  # Rate limiting\n        except Exception as e:\n            logging.error(f\"Failed to classify job '{job['filename']}': {str(e)}\")\n            return {\n                **job,\n                'analysis': f'Failed to process: {str(e)}',\n                'is_genai_role': None\n            }\n        \n        return self._parse_response(job, response)\n\n    def _parse_response(self, job: Dict, response: str) -> Dict:\n        \"\"\"Parse LLM response into structured format\"\"\"\n        soup = BeautifulSoup(f\"<root>{response}</root>\", 'lxml-xml')\n        \n        analysis = soup.find('Analysis')\n        final_answer = soup.find('FinalAnswer')\n        is_genai_role = None\n        \n        if final_answer:\n            answer_text = final_answer.text.strip().lower()\n            is_genai_role = True if answer_text == 'true' else False if answer_text == 'false' else None\n        \n        return {\n            **job,\n            'analysis': analysis.text.strip() if analysis else '',\n            'is_genai_role': is_genai_role\n        }\n\n    def save_classification(self, job_id: str, result: Dict) -> None:\n        \"\"\"Save classification results to file\"\"\"\n        filename = os.path.join(self.output_dir, f\"{job_id}.json\")\n        with open(filename, 'w') as f:\n            json.dump(result, f, indent=2)\n        logging.info(f\"Saved classification for job {job_id}\")\n\n    def get_classified_jobs(self) -> Set[str]:\n        \"\"\"Get set of already classified job IDs\"\"\"\n        if not os.path.exists(self.output_dir):\n            return set()\n        return {f[:-5] for f in os.listdir(self.output_dir) if f.endswith('.json')}\n\n    async def process_jobs_batch(self, jobs: List[Dict]) -> None:\n        \"\"\"Process a batch of jobs concurrently\"\"\"\n        async def process_with_semaphore(job: Dict) -> None:\n            async with self.semaphore:\n                result = await self.classify_job(job)\n                job_id = str(result['filename'])\n                self.save_classification(job_id, result)\n        \n        await asyncio.gather(*[process_with_semaphore(job) for job in jobs])\n\n    async def classify_jobs_async(self, df: pd.DataFrame) -> None:\n        \"\"\"Process all unclassified jobs in the DataFrame\"\"\"\n        total_jobs = len(df)\n        logging.info(f\"Starting classification of {total_jobs} jobs\")\n        \n        classified_jobs = self.get_classified_jobs()\n        jobs_to_process = [\n            job.to_dict() for idx, job in df.iterrows() \n            if str(job['filename']) not in classified_jobs\n        ]\n        \n        await self.process_jobs_batch(jobs_to_process)\n        logging.info(f\"Completed classification of all {total_jobs} jobs\")\n\n    def classify_jobs(self, df: pd.DataFrame) -> None:\n        \"\"\"Main entry point for job classification\"\"\"\n        if df.empty:\n            logging.warning(\"Empty DataFrame provided\")\n            return\n            \n        if 'filename' not in df.columns:\n            logging.error(\"DataFrame missing required 'filename' column\")\n            return\n        \n        classified_jobs = self.get_classified_jobs()\n        logging.info(f\"Found {len(classified_jobs)} previously classified jobs\")\n        \n        new_jobs = df[~df['filename'].isin(classified_jobs)]\n        if new_jobs.empty:\n            logging.info(\"No new jobs to classify\")\n            return\n        \n        logging.info(f\"Processing {len(new_jobs)} new jobs\")\n        logging.info(f\"Skipping {len(df) - len(new_jobs)} existing jobs\")\n        \n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self.classify_jobs_async(new_jobs))\n\nclass JobDataLoader:\n    \"\"\"Handles loading and preprocessing of job data\"\"\"\n    \n    @staticmethod\n    def read_json_files(json_dir='json_extracted_data') -> List[Dict]:\n        \"\"\"Read job data from JSON files\"\"\"\n        result = []\n        \n        for filename in os.listdir(json_dir):\n            if filename.endswith('.json'):\n                file_path = os.path.join(json_dir, filename)\n                try:\n                    with open(file_path, 'r') as f:\n                        data = json.load(f)\n                        name = filename[:-5]\n                        if 'responsibilities' in data and 'skills' in data:\n                            result.append({\n                                'filename': name,\n                                'responsibilities': data['responsibilities'],\n                                'skills': data['skills']\n                            })\n                except json.JSONDecodeError:\n                    logging.error(f\"Invalid JSON in {filename}\")\n                except Exception as e:\n                    logging.error(f\"Error processing {filename}: {str(e)}\")\n        \n        return result\n\n    @staticmethod\n    def load_classifications(input_dir='role_genai_classifications') -> pd.DataFrame:\n        \"\"\"Load classification results into DataFrame\"\"\"\n        if not os.path.exists(input_dir):\n            return pd.DataFrame()\n            \n        all_results = []\n        for filename in os.listdir(input_dir):\n            if filename.endswith('.json'):\n                with open(os.path.join(input_dir, filename), 'r') as f:\n                    classification = json.load(f)\n                    all_results.append(classification)\n        \n        return pd.DataFrame(all_results)\n```\n:::\n\n\n::: {#e01894db .cell execution_count=7}\n``` {.python .cell-code}\nloader = JobDataLoader()\njobs = loader.read_json_files()\ndf = pd.DataFrame(jobs)\n\n#classifier = JobClassifierGenAI()\n#classifier.classify_jobs(df)\n\n# Load results\n#results_df = loader.load_classifications()\n```\n:::\n\n\n## Classify GenAI/LLM jobs into pre-defined categories\n\nFor this next classification task, I'll make the assumption that there are four types of AI engineering and science roles that are relevant to work with GenAI systems. These are:\n\n1. AI Research Scientist\n2. AI/ML Engineer\n3. MLOps / AI Infrastructure Engineer\n4. AI Solution Architect\n\nI'll also include other categories that are not of interest, but may improve classification accuracy. These are:\n\n5. Data Scientist\n6. Data Engineer\n7. Product Manager\n8. Software Engineer\n\nThe code below implements an automated job classification system that uses an LLM to categorize job postings into the eight predefined roles listed above (four GenAI-focused and four related roles). It consists of several classes that work together: JobClassifier handles the core classification logic by comparing job descriptions against detailed role templates, JobData and ClassificationResult provide structured data containers, and JobProcessor manages the overall pipeline from loading jobs to saving results. The system processes jobs concurrently using asyncio, includes error handling and rate limiting, and outputs both an analysis explaining the classification and a final numerical category (0-8) for each job, with all results saved as JSON files for further analysis.\n\nDefinitions of the roles can be found in the `JOB_DESCRIPTIONS` variable.\n\n::: {#872d4f90 .cell code-fold-show='false' execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code for job classification into pre-defined roles\"}\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Set, Optional\nimport logging\nimport json\nimport asyncio\nimport os\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport nest_asyncio\n\nJOB_DESCRIPTIONS = \"\"\"\n<Option title=\"AI Research Scientist\" number=\"1\">\n  <PrimaryFocus>\n    Investigate and adapt cutting-edge AI methodologies (e.g., generative models, advanced prompt engineering) for applications.\n  </PrimaryFocus>\n  <KeyResponsibilities>\n    Conduct experiments to evaluate the performance (e.g., quality, accuracy) of new AI approaches and refine existing models.\n    Collaborate with AI/ML Engineers to transition successful prototypes into production.\n    Stay current with the latest AI research and emerging trends in generative AI.\n    Develop human-annotated datasets for training and evaluation of AI models.\n  </KeyResponsibilities>\n  <SkillsAndTools>\n    Deep understanding of LLMs and prompt engineering.\n    Strong background in statistics, optimization, or related fields.\n    Knowledge of experimental methods (e.g., A/B testing) and hypothesis testing.\n    Knowledge of LLM evaluation methods, including algorithmic evals, human evals, or LLM-as-a-judge evals.\n  </SkillsAndTools>\n</Option>\n<Option title=\"AI/ML Engineer\" number=\"2\">\n  <PrimaryFocus>\n    Transform research output into robust, scalable AI solutions for the product or internal use.\n  </PrimaryFocus>\n  <KeyResponsibilities>\n    Productionize AI models, ensuring they meet performance and reliability requirements.\n    Develop and maintain data pipelines for model training, inference, and monitoring.\n    Collaborate closely with Research Scientists to optimize and refine model implementations.\n  </KeyResponsibilities>\n  <SkillsAndTools>\n    Proficiency in Python, Go, or similar languages.\n    Experience with API development and integration (REST, GraphQL).\n    Working knowledge of software engineering best practices (version control, testing, CI/CD).\n  </SkillsAndTools>\n</Option>\n<Option title=\"MLOps / AI Infrastructure Engineer\" number=\"3\">\n  <PrimaryFocus>\n    Ensure reliable deployment, scaling, and monitoring of AI systems in production.\n  </PrimaryFocus>\n  <KeyResponsibilities>\n    Set up CI/CD pipelines tailored for AI workflows, including model versioning and data governance.\n    Monitor production models for performance, latency, and data drift, implementing necessary updates.\n    Manage infrastructure for scalable AI deployments (Docker, Kubernetes, cloud services).\n  </KeyResponsibilities>\n  <SkillsAndTools>\n    Strong DevOps background, with tools like Docker, Kubernetes, and Terraform.\n    Familiarity with ML orchestration/monitoring tools (MLflow, Airflow, Prometheus).\n    Experience optimizing compute usage (GPU/TPU) for cost-effective scaling.\n  </SkillsAndTools>\n</Option>\n<Option title=\"AI Solution Architect\" number=\"4\">\n  <PrimaryFocus>\n    Design and orchestrate AI solutions leveraging generative models and LLM technologies to create impactful experiences and solutions that align with business objectives.\n  </PrimaryFocus>\n  <KeyResponsibilities>\n    Collaborate with subject matter experts (SMEs) to identify and refine opportunities for generative AI/LLM-based use cases.\n    Assess feasibility and define high-level solution architectures, ensuring they address core business and user requirements.\n    Develop technical proposals and roadmaps, translating complex requirements into actionable plans.\n    Provide thought leadership on conversational design, user experience flow, and model interaction strategies.\n    Ensure solutions comply with relevant data governance, privacy, and security considerations.\n    Facilitate cross-functional collaboration, guiding teams through solution conceptualization and implementation phases.\n  </KeyResponsibilities>\n  <SkillsAndTools>\n    Strong understanding of LLM capabilities and prompt engineering principles.\n    Experience with conversational experience design (e.g., chatbots, voice interfaces) and user journey mapping.\n    Ability to analyze business needs and translate them into feasible AI solution proposals.\n    Familiarity with data privacy and security best practices, especially as they pertain to AI solutions.\n    Excellent communication and stakeholder management skills to align technical and non-technical teams.\n  </SkillsAndTools>\n</Option>\n<Option title=\"Data Scientist\" number=\"5\"> \n  <PrimaryFocus>\n    Leverage statistical analysis, machine learning, and data visualization to derive actionable insights and guide data-informed decisions.\n  </PrimaryFocus> \n  <KeyResponsibilities> \n    Perform exploratory data analysis (EDA) to identify trends and patterns in large, complex datasets. \n    Develop and validate predictive and prescriptive models, collaborating with cross-functional teams to implement these solutions. \n    Design and execute experiments to test hypotheses, measure impact, and inform business strategies. \n    Present findings and recommendations to stakeholders in a clear, concise manner using visualizations and dashboards. \n    Work with data engineers to ensure data quality, governance, and availability. \n  </KeyResponsibilities> \n  <SkillsAndTools> \n    Proficiency in Python, R, or SQL for data manipulation and analysis. \n    Experience with common ML libraries (e.g., scikit-learn, XGBoost) and deep learning frameworks (e.g., PyTorch, TensorFlow). \n    Solid grounding in statistics, probability, and experimental design. \n    Familiarity with data visualization tools (e.g., Tableau, Power BI) for communicating insights. \n    Strong analytical thinking and ability to translate complex data problems into business solutions. \n  </SkillsAndTools> \n</Option>\n<Option title=\"Data Engineer\" number=\"6\">\n  <PrimaryFocus>\n    Design, build, and maintain scalable data pipelines and architectures that enable efficient data collection, storage, and analysis.\n  </PrimaryFocus>\n  <KeyResponsibilities>\n    Develop and optimize data ingestion and transformation processes (ETL/ELT), ensuring high performance and reliability.\n    Implement and manage data workflows, integrating internal and external data sources.\n    Collaborate with Data Scientists, AI/ML Engineers, and other stakeholders to ensure data readiness for analytics and model training.\n    Monitor data pipelines for performance, reliability, and cost-effectiveness, taking corrective actions when needed.\n    Maintain data quality and governance standards, including metadata management and data cataloging.\n  </KeyResponsibilities>\n  <SkillsAndTools>\n    Proficiency in Python, SQL, and distributed data processing frameworks (e.g., Spark, Kafka).\n    Experience with cloud-based data ecosystems (AWS, GCP, or Azure), and related storage/processing services (e.g., S3, BigQuery, Dataflow).\n    Familiarity with infrastructure-as-code and DevOps tools (Terraform, Docker, Kubernetes) for automating data platform deployments.\n    Strong understanding of database systems (relational, NoSQL) and data modeling principles.\n    Knowledge of data orchestration and workflow management tools (Airflow, Luigi, Dagster).\n  </SkillsAndTools>\n</Option>\n<Option title=\"Product Manager\" number=\"7\">\n  <PrimaryFocus>\n    Drive the product vision and strategy, ensuring alignment with business goals and user needs while delivering impactful AI-driven solutions.\n  </PrimaryFocus>\n  <KeyResponsibilities>\n    Conduct user and market research to identify opportunities, define product requirements, and set success metrics.\n    Collaborate with cross-functional teams (Engineering, Data Science, Design) to prioritize features and plan releases.\n    Develop and communicate product roadmaps, ensuring stakeholders are aligned on goals and timelines.\n    Monitor product performance through data analysis and user feedback, iterating on improvements and new feature ideas.\n    Facilitate agile development practices, writing clear user stories and acceptance criteria.\n  </KeyResponsibilities>\n  <SkillsAndTools>\n    Strong understanding of product lifecycle management and agile methodologies (Scrum/Kanban).\n    Excellent communication, negotiation, and stakeholder management skills.\n    Experience with product management and collaboration tools (e.g., Jira, Confluence, Trello).\n    Analytical mindset for leveraging metrics, A/B testing, and user feedback in decision-making.\n    Familiarity with AI/ML concepts and the ability to translate technical possibilities into viable product features.\n  </SkillsAndTools>\n</Option>\n<Option title=\"Software Engineer\" number=\"8\">\n  <PrimaryFocus>\n  Design, develop, and maintain high-quality software applications and services that address user needs and align with overall business objectives.\n  </PrimaryFocus>\n  <KeyResponsibilities>\n    Collaborate with cross-functional teams (Product, Design, QA) to interpret requirements and deliver robust solutions.\n    Write clean, efficient, and testable code following best practices and coding standards.\n    Participate in system architecture and design discussions, contributing to the evolution of technical roadmaps.\n    Perform code reviews and provide constructive feedback to peers, maintaining a high bar for code quality.\n    Implement and maintain CI/CD pipelines to streamline deployment and reduce manual interventions.\n    Continuously improve system performance and scalability through profiling and optimization.\n  </KeyResponsibilities>\n  <SkillsAndTools>\n    Proficiency in one or more programming languages (e.g., Java, Python, JavaScript, C++).\n    Experience with modern frameworks/libraries (e.g., Spring Boot, Node.js, React, Django).\n    Solid understanding of software design principles (e.g., SOLID, DRY) and architectural patterns (e.g., microservices).\n    Familiarity with version control systems (Git), testing frameworks, and agile methodologies.\n    Working knowledge of containerization (Docker), orchestration (Kubernetes), and cloud platforms (AWS, Azure, GCP).\n  </SkillsAndTools>\n</Option>\n\"\"\"\n\nPROMPTS = {\n\"prompt\": \"\"\"\nYou will be given a list of Responsibilities and\nSkills listed for a job. Your task is to determine\nif the job is a good fit with any of the Options,\nand if so, which one.\n\n<Job>\n<Responsibilities>\n{responsibilities}\n</Responsibilities>\n<Skills>\n{skills}\n</Skills>\n</Job>\n\n<Options>\n{Options}\n</Options>\n\nStart by thinking step-by-step about the Job and its \nResponsibilities and Skills, in relation to each of the \nOptions.\n\nDecide if the Job is a good fit with ANY of the Options.\nIf NONE of the Options are relevant to the Job, say so and \nreturn a 0 as your FinalAnswer.\n\nOtherwise, decide which of the Options is the most similar\nto the Job and return its number as your FinalAnswer.\n\nReturn your response in the following format:\n<Analysis>\n[Your analysis of the job and its Responsibilities and Skills, in relation each of the Options]\n</Analysis>\n<FinalAnswer>\n0|1|2|3|4|5|6|7|8\n</FinalAnswer>\n\"\"\"\n}\n\n# Enable nested event loops\nnest_asyncio.apply()\n\n@dataclass\nclass JobData:\n    \"\"\"Represents a job posting with extracted information.\"\"\"\n    filename: str\n    responsibilities: List[str]\n    skills: List[str]\n\n@dataclass\nclass ClassificationResult:\n    \"\"\"Represents the result of a job classification.\"\"\"\n    filename: str\n    responsibilities: List[str]\n    skills: List[str]\n    analysis: str\n    role_classification: Optional[int]\n    role_title: Optional[str]\n\nclass JobClassifier:\n    \"\"\"Handles classification of jobs into predefined roles.\"\"\"\n    \n    def __init__(self, output_dir: str = 'role_classifications', batch_size: int = 3):\n        self.output_dir = output_dir\n        self.batch_size = batch_size\n        self.semaphore = asyncio.Semaphore(batch_size)\n        os.makedirs(output_dir, exist_ok=True)\n        \n    async def classify_job(self, job: JobData) -> ClassificationResult:\n        \"\"\"Classify a single job listing.\"\"\"\n        logging.info(f\"Classifying job '{job.filename}'\")\n        \n        prompt = PROMPTS[\"prompt\"].format(\n            responsibilities=job.responsibilities,  # Access as attribute instead of dict\n            skills=job.skills,  # Access as attribute instead of dict\n            Options=JOB_DESCRIPTIONS\n        )\n        \n        try:\n            response = await get_completion(prompt)\n            await asyncio.sleep(5)  # Rate limiting\n            return self._parse_response(job, response)\n        except Exception as e:\n            logging.error(f\"Failed to classify job '{job.filename}': {str(e)}\")\n            return ClassificationResult(\n                filename=job.filename,\n                responsibilities=job.responsibilities,\n                skills=job.skills,\n                analysis=f'Failed to process: {str(e)}',\n                role_classification=None,\n                role_title=None\n            )\n    \n    def _parse_response(self, job: JobData, response: str) -> ClassificationResult:\n        \"\"\"Parse LLM response into structured format.\"\"\"\n        soup = BeautifulSoup(f\"<root>{response}</root>\", 'lxml-xml')\n        \n        analysis = soup.find('Analysis')\n        role_choice = soup.find('FinalAnswer')\n        role_number = int(role_choice.text.strip()) if role_choice else None\n        \n        role_title = self._get_role_title(role_number)\n        \n        return ClassificationResult(\n            filename=job.filename,\n            responsibilities=job.responsibilities,\n            skills=job.skills,\n            analysis=analysis.text.strip() if analysis else '',\n            role_classification=role_number,\n            role_title=role_title\n        )\n    \n    def _get_role_title(self, role_number: Optional[int]) -> Optional[str]:\n        \"\"\"Get the title for a role number.\"\"\"\n        if role_number is None:\n            return None\n        if role_number == 0:\n            return \"Other\"\n            \n        wrapped_xml = f\"<root>{JOB_DESCRIPTIONS}</root>\"\n        job_descriptions_soup = BeautifulSoup(wrapped_xml, 'lxml-xml')\n        matching_job = job_descriptions_soup.find('Option', {'number': str(role_number)})\n        \n        if matching_job:\n            return matching_job['title']\n        logging.error(f\"No matching job found for role number {role_number}\")\n        return None\n\nclass JobProcessor:\n    \"\"\"Handles the processing of job data files.\"\"\"\n    \n    def __init__(self, input_dir: str = 'json_extracted_data'):\n        self.input_dir = input_dir\n        self.classifier = JobClassifier()\n        \n    def load_jobs(self) -> List[JobData]:\n        \"\"\"Load jobs from JSON files.\"\"\"\n        jobs = []\n        # Get list of all JSON files\n        total_files = len([f for f in os.listdir(self.input_dir) if f.endswith('.json')])\n        # Get files that were classified as GenAI-relevant\n        classified_files = self._get_classified_files()\n        # Get files that have already been processed\n        processed_files = {\n            f[:-5] for f in os.listdir(self.classifier.output_dir) \n            if f.endswith('.json')\n        }\n        # Get relevant files that haven't been processed yet\n        files_to_process = classified_files - processed_files\n        \n        logging.info(f\"Found {total_files} total files\")\n        logging.info(f\"Found {len(classified_files)} relevant GenAI files\")\n        logging.info(f\"Already processed: {len(processed_files)} files\")\n        logging.info(f\"Remaining to process: {len(files_to_process)} files\")\n        \n        # Only process files that are both relevant and unprocessed\n        for filename in os.listdir(self.input_dir):\n            if not filename.endswith('.json'):\n                continue\n                \n            name = filename[:-5]\n            if name not in files_to_process:\n                continue\n                \n            try:\n                job = self._load_job_file(filename)\n                if job:\n                    jobs.append(job)\n            except Exception as e:\n                logging.error(f\"Error processing {filename}: {str(e)}\")\n        \n        logging.info(f\"Processing {len(jobs)} remaining jobs\")\n        return jobs\n    \n    def _load_job_file(self, filename: str) -> Optional[JobData]:\n        \"\"\"Load and parse a single job file.\"\"\"\n        file_path = os.path.join(self.input_dir, filename)\n        try:\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n                if 'responsibilities' in data and 'skills' in data:\n                    return JobData(\n                        filename=filename[:-5],\n                        responsibilities=data['responsibilities'],\n                        skills=data['skills']\n                    )\n        except json.JSONDecodeError:\n            logging.error(f\"Invalid JSON in {filename}\")\n        return None\n    \n    def _get_classified_files(self) -> Set[str]:\n        \"\"\"Get set of files that have been previously classified as GenAI-related.\"\"\"\n        genai_dir = 'role_genai_classifications'\n        if not os.path.exists(genai_dir):\n            return set()\n        \n        genai_files = set()\n        for f in os.listdir(genai_dir):\n            if f.endswith('.json'):\n                try:\n                    with open(os.path.join(genai_dir, f), 'r') as file:\n                        data = json.load(file)\n                        if data.get('is_genai_role') is True:  # Explicitly check for True\n                            genai_files.add(f[:-5])\n                except Exception as e:\n                    logging.error(f\"Error reading {f}: {e}\")\n        return genai_files\n    \n    async def process_jobs(self) -> None:\n        \"\"\"Process all jobs.\"\"\"\n        jobs = self.load_jobs()\n        if not jobs:\n            return\n            \n        logging.info(f\"Processing {len(jobs)} relevant jobs\")\n        await self._process_jobs_batch(jobs)\n        logging.info(\"Job classification complete\")\n    \n    async def _process_jobs_batch(self, jobs: List[JobData]) -> None:\n        \"\"\"Process a batch of jobs concurrently.\"\"\"\n        async def process_with_semaphore(job: JobData) -> None:\n            async with self.classifier.semaphore:\n                result = await self.classifier.classify_job(job)\n                self._save_result(result)\n        \n        await asyncio.gather(*[process_with_semaphore(job) for job in jobs])\n    \n    def _save_result(self, result: ClassificationResult) -> None:\n        \"\"\"Save classification result to file.\"\"\"\n        filename = os.path.join(self.classifier.output_dir, f\"{result.filename}.json\")\n        with open(filename, 'w') as f:\n            json.dump(vars(result), f, indent=2)\n        logging.info(f\"Saved classification for job {result.filename}\")\n\ndef classify_job_roles(df: pd.DataFrame) -> None:\n    \"\"\"Main entry point for job classification.\"\"\"\n    if df.empty:\n        logging.warning(\"Empty DataFrame provided\")\n        return\n        \n    processor = JobProcessor()\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(processor.process_jobs())\n```\n:::\n\n\n::: {#9738bf89 .cell execution_count=9}\n``` {.python .cell-code}\nimport logging\nlogging.basicConfig(level=logging.INFO)\nloader = JobDataLoader()\njobs = loader.read_json_files()\ndf = pd.DataFrame(jobs)\nclassify_job_roles(df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-01-24 13:26:45,800 - INFO - Found 739 total files\n2025-01-24 13:26:45,800 - INFO - Found 247 relevant GenAI files\n2025-01-24 13:26:45,800 - INFO - Already processed: 247 files\n2025-01-24 13:26:45,801 - INFO - Remaining to process: 0 files\n2025-01-24 13:26:45,802 - INFO - Processing 0 remaining jobs\n```\n:::\n:::\n\n\n# Load final data\n\nNow I'll load and combine all of the different data outputs into a single dataframe.\n\nThe code block below uses a DataLoader class with helper classes DataPaths and SalaryProcessor to handle the data processing pipeline. The code merges job posting data with role classifications, processes salary information by converting CAD to USD and extracting salary ranges, and filters the results to only include the four specific AI-related roles of interest: AI Research Scientist, AI Solution Architect, AI/ML Engineer, and MLOps/AI Infrastructure Engineer. The code includes error handling for JSON file loading and salary processing. The final output is a pandas DataFrame containing the merged and processed data, which includes job details, role classifications, and standardized salary information.\n\n::: {#43dc2f69 .cell code-fold-show='false' execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code for loading and merging all data sources\"}\nfrom typing import List, Tuple, Optional, Dict\nimport pandas as pd\nimport json\nimport hashlib\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\n@dataclass\nclass DataPaths:\n    \"\"\"Configuration for data file paths\"\"\"\n    ROLE_CLASSIFICATIONS_DIR: Path = Path('role_classifications')\n    JSON_DATA_DIR: Path = Path('json_data')\n\nclass DataLoader:\n    def __init__(self, paths: DataPaths = DataPaths()):\n        self.paths = paths\n        self.salary_processor = SalaryProcessor()\n\n    def load_and_merge_data(self) -> pd.DataFrame:\n        \"\"\"Load and merge all data sources\"\"\"\n        # Load and merge base data\n        merged_df = self._load_base_data()\n        \n        # Process salaries\n        salary_data = self.salary_processor.extract_salary_ranges(merged_df['salary'])\n        merged_df = pd.concat([merged_df, salary_data], axis=1)\n\n        # Filter out any jobs not in this list\n        merged_df = merged_df[merged_df['role_title'].isin(\n            ['AI Research Scientist',\n            'AI Solution Architect',\n            'AI/ML Engineer',\n            'MLOps / AI Infrastructure Engineer']\n        )]\n        \n        print(f\"Shape of merged DataFrame: {merged_df.shape}\")\n        return merged_df\n\n    def _load_base_data(self) -> pd.DataFrame:\n        \"\"\"Load and merge base data sources\"\"\"\n        # Load role classifications\n        role_data = pd.DataFrame(self._load_json_files(self.paths.ROLE_CLASSIFICATIONS_DIR))\n        \n        # Load and process jobs data\n        jobs_data = pd.DataFrame(self._load_json_files(self.paths.JSON_DATA_DIR))\n        jobs_data['filename'] = jobs_data['url'].apply(\n            lambda x: f\"j{hashlib.md5(x.encode()).hexdigest()[:5]}\"\n        )\n        \n        # Merge dataframes\n        merged_df = pd.merge(jobs_data, role_data, on='filename', how='left')\n        return merged_df\n\n    def _load_json_files(self, directory: Path) -> List[Dict]:\n        \"\"\"Load JSON files from directory\"\"\"\n        json_files = []\n        for file_path in directory.glob('*.json'):\n            try:\n                with open(file_path, 'r') as f:\n                    json_files.append(json.load(f))\n            except Exception as e:\n                print(f\"Error reading {file_path}: {e}\")\n        return json_files\n\nclass SalaryProcessor:\n    \"\"\"Handles salary-related processing\"\"\"\n    CAD_TO_USD_RATE = 1.44  # 1 USD = 1.44 CAD\n\n    def extract_salary_ranges(self, salary_series: pd.Series) -> pd.DataFrame:\n        \"\"\"Extract salary ranges from a series of salary strings\"\"\"\n        salary_data = salary_series.apply(self._extract_salary_range)\n        return pd.DataFrame({\n            'min_salary': salary_data.apply(lambda x: x[0]),\n            'max_salary': salary_data.apply(lambda x: x[1]),\n            'mid_salary': salary_data.apply(lambda x: (x[0] + x[1])/2 if x[0] and x[1] else None)\n        })\n\n    def _extract_salary_range(self, salary_str: str) -> Tuple[Optional[float], Optional[float]]:\n        \"\"\"Extract minimum and maximum salary from salary string\"\"\"\n        try:\n            if not isinstance(salary_str, str) or '0+' in salary_str:\n                return None, None\n            \n            # Determine currency and convert if needed\n            is_cad = 'CAD' in salary_str\n            nums = (salary_str.replace('CAD ', '')\n                            .replace('USD ', '')\n                            .replace('K', '')\n                            .split(' - '))\n            \n            if len(nums) != 2:\n                return None, None\n            \n            # Convert to float and apply CAD conversion if needed\n            min_salary = float(nums[0])\n            max_salary = float(nums[1])\n            \n            if is_cad:\n                min_salary *= self.CAD_TO_USD_RATE\n                max_salary *= self.CAD_TO_USD_RATE\n                \n            return min_salary, max_salary\n        except Exception:\n            return None, None\n```\n:::\n\n\n::: {#8dda6748 .cell execution_count=11}\n``` {.python .cell-code}\n# Initialize the loader and load the data\n# Check if the csv file exists\nif not os.path.exists('genai_job_data.csv'):\n    loader = DataLoader()\n    df = loader.load_and_merge_data()\n    df.to_csv('genai_job_data.csv', index=False)\nelse:\n    df = pd.read_csv('genai_job_data.csv')\n\n# Now df contains all the merged data with processed salaries\nprint(f\"Total jobs loaded: {len(df)}\")\n\n# Example operations you can do with the DataFrame:\n\n# View available columns\nprint(\"\\nAvailable columns:\")\nprint(df.columns.tolist())\n\n# View distribution of roles\nprint(\"\\nRole distribution:\")\nprint(df['role_title'].value_counts())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal jobs loaded: 187\n\nAvailable columns:\n['title', 'company', 'location', 'level', 'salary', 'url', 'description', 'scraped_date', 'posted_date', 'raw_data', 'filename', 'responsibilities', 'skills', 'analysis', 'role_classification', 'role_title', 'min_salary', 'max_salary', 'mid_salary']\n\nRole distribution:\nrole_title\nAI/ML Engineer                        60\nAI Research Scientist                 52\nAI Solution Architect                 48\nMLOps / AI Infrastructure Engineer    27\nName: count, dtype: int64\n```\n:::\n:::\n\n\n# Salary analysis\n\nFirst I'll analyze the salary data for each role and experience level. I'll use a RandomForest model to adjust for location differences, and convert any CAD denominated salaries to USD. I'm interested to see what compensation is like for each role, and if there are any significant differences in compensation between the roles.\n\nThe code below implements a salary analysis system for the job posting data. It processes salary information across different roles and experience levels, handling currency conversions (CAD to USD) and adjusting for location differences using a RandomForest model. For each role and level combination, it calculates salary ranges, removes statistical outliers, and provides sample sizes. The system includes configuration settings for analysis parameters and comprehensive error handling. The code uses pandas for data manipulation and scikit-learn for the location adjustment model, with results formatted as salary ranges in USD.\n\n::: {#dbc6ca70 .cell code-fold-show='false' execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code for salary analysis\"}\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, TypedDict, Set\nimport logging\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\n@dataclass\nclass SalaryConfig:\n    \"\"\"Configuration settings for salary analysis\"\"\"\n    outlier_threshold: float = 1.5\n    n_estimators: int = 100\n    min_group_size: int = 3\n    currency_symbol: str = '$'\n    cv_folds: int = 5\n\nclass SalaryColumns(TypedDict):\n    \"\"\"Type definitions for salary DataFrame columns\"\"\"\n    min_salary_usd_k: float\n    max_salary_usd_k: float\n    median_salary_usd_k: float\n    sample_size: int\n    location_adjusted_zscore: float\n\nclass SalaryAnalyzer:\n    \"\"\"Analyzes salary distributions across role titles.\n    \n    Column Naming Conventions:\n    - *_usd_k: Values in thousands of USD\n    - *_zscore: Standardized scores (mean=0, std=1)\n    - sample_size: Number of data points in group\n    - *_median: Median values\n    - location_adjusted_*: Values adjusted for location differences\n    \n    Attributes:\n        config: Configuration settings for analysis\n        logger: Logger instance for the class\n    \"\"\"\n    \n    # Input column names\n    COL_MIN_SALARY = 'min_salary'\n    COL_MAX_SALARY = 'max_salary'\n    COL_ROLE_TITLE = 'role_title'\n    COL_LEVEL = 'level'\n    COL_LOCATION = 'location'\n    \n    # Processed column names\n    COL_MEDIAN_SALARY = 'median_salary_usd_k'\n    COL_MIN_SALARY_MEDIAN = 'min_salary_median_usd_k'\n    COL_MAX_SALARY_MEDIAN = 'max_salary_median_usd_k'\n    COL_SAMPLE_SIZE = 'sample_size'\n    \n    # Z-score columns\n    COL_MIN_ZSCORE = 'min_salary_location_adjusted_zscore'\n    COL_MID_ZSCORE = 'median_salary_location_adjusted_zscore'\n    COL_MAX_ZSCORE = 'max_salary_location_adjusted_zscore'\n    \n    def __init__(self, config: Optional[SalaryConfig] = None):\n        \"\"\"Initialize SalaryAnalyzer with optional configuration.\"\"\"\n        self.config = config or SalaryConfig()\n        self.logger = logging.getLogger(__name__)\n        self._model = None  # Cache for trained model\n        \n    def analyze_role_salaries(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Analyze salary distributions across role titles with location adjustment.\n        \n        Args:\n            df: DataFrame containing columns:\n                - min_salary: Minimum salary (float)\n                - max_salary: Maximum salary (float)\n                - role_title: Job role (str)\n                - level: Experience level (str)\n                - location: Job location (str)\n                \n        Returns:\n            DataFrame with aggregated salary statistics by role and level\n            \n        Raises:\n            ValueError: If required columns are missing or DataFrame is empty\n        \"\"\"\n        self._validate_input(df)\n        self.logger.info(f\"Starting salary analysis with {len(df)} records\")\n        \n        # Create working copy of DataFrame\n        analysis_df = self._prepare_data(df)\n        \n        # Process and clean data\n        analysis_df = self._process_data(analysis_df)\n        \n        # Generate final results\n        results = self._aggregate_results(analysis_df)\n        \n        self.logger.info(\"Salary analysis completed successfully\")\n        return results\n    \n    def _validate_input(self, df: pd.DataFrame) -> None:\n        \"\"\"Validate input DataFrame.\"\"\"\n        if df is None or df.empty:\n            raise ValueError(\"Input DataFrame cannot be None or empty\")\n            \n        required_cols = {\n            self.COL_MIN_SALARY, \n            self.COL_MAX_SALARY, \n            self.COL_ROLE_TITLE,\n            self.COL_LEVEL,\n            self.COL_LOCATION\n        }\n        \n        missing_cols = required_cols - set(df.columns)\n        if missing_cols:\n            raise ValueError(f\"DataFrame missing required columns: {missing_cols}\")\n    \n    def _prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Prepare data for analysis by filtering valid salaries.\"\"\"\n        valid_salary_mask = (\n            df[self.COL_MIN_SALARY].notna() & \n            df[self.COL_MAX_SALARY].notna()\n        )\n        \n        if not valid_salary_mask.any():\n            self.logger.warning(\"No valid salary data found\")\n            return pd.DataFrame()\n            \n        # Create working copy and calculate median salary\n        analysis_df = df[valid_salary_mask].copy()\n        analysis_df[self.COL_MEDIAN_SALARY] = (\n            analysis_df[[self.COL_MIN_SALARY, self.COL_MAX_SALARY]].mean(axis=1)\n        )\n        \n        return analysis_df\n\n    def _remove_outliers(self, group: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Remove statistical outliers from salary data.\"\"\"\n        if len(group) <= self.config.min_group_size:\n            return group\n            \n        # Only check min and max salary for outliers\n        salary_cols = [self.COL_MIN_SALARY, self.COL_MAX_SALARY]\n        \n        for col in salary_cols:\n            Q1 = group[col].quantile(0.25)\n            Q3 = group[col].quantile(0.75)\n            IQR = Q3 - Q1\n            \n            outlier_mask = ~(\n                (group[col] < (Q1 - self.config.outlier_threshold * IQR)) | \n                (group[col] > (Q3 + self.config.outlier_threshold * IQR))\n            )\n            group = group[outlier_mask]\n            \n        # Recalculate median after removing outliers\n        group[self.COL_MEDIAN_SALARY] = (\n            group[self.COL_MIN_SALARY] + group[self.COL_MAX_SALARY]\n        ) / 2\n            \n        return group\n\n    def _aggregate_results(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Aggregate and format the final results.\"\"\"\n        # First calculate group statistics\n        grouped = df.groupby([self.COL_ROLE_TITLE, self.COL_LEVEL]).agg({\n            self.COL_MIN_SALARY: ['count', 'median'],\n            self.COL_MAX_SALARY: 'median',\n        }).round(0)\n        \n        # Calculate median salary from min and max medians\n        grouped['median_salary'] = (\n            grouped[(self.COL_MIN_SALARY, 'median')] + \n            grouped[(self.COL_MAX_SALARY, 'median')]\n        ) / 2\n        \n        # Flatten and rename columns\n        grouped.columns = [\n            'sample_size' if col == (self.COL_MIN_SALARY, 'count')\n            else 'min_salary_usd_k' if col == (self.COL_MIN_SALARY, 'median')\n            else 'max_salary_usd_k' if col == (self.COL_MAX_SALARY, 'median')\n            else 'median_salary_usd_k' if col[0] == 'median_salary'\n            else col\n            for col in grouped.columns\n        ]\n        \n        # Order columns\n        ordered_cols = [\n            'sample_size',\n            'min_salary_usd_k',\n            'max_salary_usd_k',\n            'median_salary_usd_k'\n        ]\n        result = grouped.reindex(columns=ordered_cols)\n        \n        # Format salary values\n        salary_cols = [col for col in ordered_cols if col.endswith('_usd_k')]\n        for col in salary_cols:\n            result[col] = result[col].apply(\n                lambda x: f\"{self.config.currency_symbol}{x:,.0f}K\" if pd.notna(x) else \"N/A\"\n            )\n        \n        return result\n\n    \n    def _process_data(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process and clean the salary data.\"\"\"\n        # Remove outliers by group\n        df = (df.groupby([self.COL_ROLE_TITLE, self.COL_LEVEL])\n              .apply(self._remove_outliers)\n              .reset_index(drop=True))\n        \n        # Add location-adjusted scores\n        df = self._add_location_adjusted_scores(df)\n        \n        # Clean level names\n        df[self.COL_LEVEL] = (df[self.COL_LEVEL]\n                             .str.replace('-Level / Level', '', regex=False)\n                             .str.replace('-level', '', regex=False))\n        \n        return df\n    \n    def _train_model(self) -> RandomForestRegressor:\n        \"\"\"Train and cache RandomForest model.\"\"\"\n        if self._model is None:\n            self._model = RandomForestRegressor(\n                n_estimators=self.config.n_estimators,\n                random_state=42\n            )\n        return self._model\n    \n    def _add_location_adjusted_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add location-adjusted z-scores to the dataframe.\"\"\"\n        location_dummies = pd.get_dummies(df[self.COL_LOCATION], prefix='loc')\n        model = self._train_model()\n        \n        score_columns = {\n            self.COL_MIN_SALARY: self.COL_MIN_ZSCORE,\n            self.COL_MEDIAN_SALARY: self.COL_MID_ZSCORE,\n            self.COL_MAX_SALARY: self.COL_MAX_ZSCORE\n        }\n        \n        for source_col, target_col in score_columns.items():\n            df[target_col] = self._adjust_salaries(\n                df[source_col], \n                location_dummies,\n                model\n            )\n        \n        return df\n    \n    def _adjust_salaries(self, \n                        salary_series: pd.Series,\n                        X: pd.DataFrame,\n                        model: RandomForestRegressor) -> pd.Series:\n        \"\"\"Adjust salaries using RandomForest model.\"\"\"\n        # Evaluate model performance\n        scores = cross_val_score(\n            model, X, salary_series, \n            cv=self.config.cv_folds\n        )\n        self.logger.debug(f\"Cross-validation scores: {scores.mean():.3f} ± {scores.std():.3f}\")\n        \n        # Fit model and calculate residuals\n        model.fit(X, salary_series)\n        expected = model.predict(X)\n        residuals = salary_series - expected\n        \n        # Return standardized residuals\n        return (residuals - residuals.mean()) / residuals.std()\n```\n:::\n\n\n::: {#91ab689a .cell execution_count=13}\n``` {.python .cell-code}\nsalary_analyzer = SalaryAnalyzer()\nsalary_df = salary_analyzer.analyze_role_salaries(df)\nsalary_df\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-01-24 13:26:46,128 - INFO - Starting salary analysis with 187 records\n2025-01-24 13:26:48,622 - INFO - Salary analysis completed successfully\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>sample_size</th>\n      <th>min_salary_usd_k</th>\n      <th>max_salary_usd_k</th>\n      <th>median_salary_usd_k</th>\n    </tr>\n    <tr>\n      <th>role_title</th>\n      <th>level</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">AI Research Scientist</th>\n      <th>Entry</th>\n      <td>3</td>\n      <td>$140K</td>\n      <td>$170K</td>\n      <td>$155K</td>\n    </tr>\n    <tr>\n      <th>Mid</th>\n      <td>8</td>\n      <td>$122K</td>\n      <td>$209K</td>\n      <td>$166K</td>\n    </tr>\n    <tr>\n      <th>Senior</th>\n      <td>24</td>\n      <td>$159K</td>\n      <td>$251K</td>\n      <td>$205K</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">AI Solution Architect</th>\n      <th>Entry</th>\n      <td>2</td>\n      <td>$133K</td>\n      <td>$266K</td>\n      <td>$200K</td>\n    </tr>\n    <tr>\n      <th>Mid</th>\n      <td>7</td>\n      <td>$120K</td>\n      <td>$206K</td>\n      <td>$163K</td>\n    </tr>\n    <tr>\n      <th>Senior</th>\n      <td>33</td>\n      <td>$146K</td>\n      <td>$247K</td>\n      <td>$196K</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">AI/ML Engineer</th>\n      <th>Entry</th>\n      <td>1</td>\n      <td>$135K</td>\n      <td>$251K</td>\n      <td>$193K</td>\n    </tr>\n    <tr>\n      <th>Mid</th>\n      <td>11</td>\n      <td>$129K</td>\n      <td>$224K</td>\n      <td>$176K</td>\n    </tr>\n    <tr>\n      <th>Senior</th>\n      <td>28</td>\n      <td>$158K</td>\n      <td>$268K</td>\n      <td>$213K</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">MLOps / AI Infrastructure Engineer</th>\n      <th>Entry</th>\n      <td>2</td>\n      <td>$152K</td>\n      <td>$260K</td>\n      <td>$206K</td>\n    </tr>\n    <tr>\n      <th>Mid</th>\n      <td>5</td>\n      <td>$143K</td>\n      <td>$214K</td>\n      <td>$178K</td>\n    </tr>\n    <tr>\n      <th>Senior</th>\n      <td>16</td>\n      <td>$164K</td>\n      <td>$258K</td>\n      <td>$211K</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLooking across all the four GenAI roles (AI Research, Solutions Architecture, ML Engineering, and MLOps), compensation bands are similar. At senior levels (where I have the largest samples), median salaries cluster tightly between ~$195K-$210K USD. Mid-level positions show medians between ~$165K-$180K USD, and entry-level positions generally start between ~$155K-$205K USD, although this entry-level data is limited by small samples.\n\nThis consistency suggests these roles are valued similarly in the market, despite their different focuses.\n\n# Job responsibilities\n\nNext, I'm interested to see what the job responsibilities are for some of these roles, and how they differ. I'll use an LLM to identify the most common responsibilities and skills for a given role. This task will be quite complex, involving a lot of input data. So a model like `o1` will be more appropriate. I don't have access to `o1` via API, so I'll use the web interface to generate completions, and prepare XML files for the inputs to this task -- allowing me to copy-paste the XML into the chat.\n\nAs we'll see, Research Scientists lean heavily into discovery, theoretical advances, publication, and cutting-edge experimentation, whereas ML Engineers center on production-grade systems, robust architecture, MLOps, and aligning with business needs. Although in many cases, the lines between these roles will be blurred.\n\n::: {#446b831f .cell code-fold-show='false' execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code for XML generation\"}\nimport logging\nimport os\nfrom typing import Dict\nimport pandas as pd\n\nINSTRUCTION_PREFIX = \"\"\"\nIdentify the most common Responsibilities and Skills listed in the jobs below, returning a bulleted list in the format\n\n# Responsibilities\n- [responsibility]: [responsibility description]\n- [responsibility]: [responsibility description]\n...\n# Skills\n- [skill]: [skill description]\n- [skill]: [skill description]\n...\n\"\"\"\n\nclass XMLGenerator:\n    \"\"\"Handles generation of XML files from job data\"\"\"\n    \n    def __init__(self, output_dir: str = 'xml_roles'):\n        self.output_dir = output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        \n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s'\n        )\n    \n    def compile_xml_for_role(self, level: str, role_title: str, df: pd.DataFrame) -> Dict:\n        \"\"\"Generate XML-formatted string of job details for the given level and role title.\"\"\"\n        # Filter DataFrame for matching level and role\n        filtered_df = df[\n            (df['level'] == level) & \n            (df['role_title'] == role_title)\n        ]\n        \n        if len(filtered_df) == 0:\n            return {\n                'level': level,\n                'role_title': role_title,\n                'xml_output': '',\n                'error': f\"No jobs found for {level} {role_title}\"\n            }\n        \n        # Build XML string\n        xml_output = \"\"\n        for _, job in filtered_df.iterrows():\n            xml_output += \"<Job>\\n\"\n            xml_output += f\"<Responsibilities>{job['responsibilities']}</Responsibilities>\\n\"\n            xml_output += f\"<Skills>{job['skills']}</Skills>\\n\"\n            xml_output += \"</Job>\\n\"\n        \n        return {\n            'level': level,\n            'role_title': role_title,\n            'xml_output': INSTRUCTION_PREFIX + xml_output\n        }\n    \n    def save_xml(self, level: str, role_title: str, xml_data: Dict) -> None:\n        \"\"\"Save XML data to a file\"\"\"\n        if not xml_data['xml_output']:\n            #logging.warning(xml_data.get('error', 'Empty XML output'))\n            return\n            \n        # Clean and sanitize the filename components\n        cleaned_level = level.replace('-Level / Level', '').strip()\n        safe_level = cleaned_level.replace('/', '_').replace(' ', '_')\n        safe_role = role_title.replace('/', '_').replace(' ', '_')\n        filename = os.path.join(self.output_dir, f\"{safe_level}_{safe_role}.xml\")\n        \n        with open(filename, 'w') as f:\n            f.write(xml_data['xml_output'])\n        logging.debug(f\"Saved XML for {cleaned_level} {role_title} to {filename}\")\n    \n    def generate_all_xml(self, df: pd.DataFrame) -> None:\n        \"\"\"Generate XML files for all role/level combinations\"\"\"\n        if df.empty:\n            logging.warning(\"No jobs found in DataFrame - check if data was loaded correctly\")\n            return\n            \n        required_cols = ['level', 'role_title', 'responsibilities', 'skills']  # Changed here too\n        if not all(col in df.columns for col in required_cols):\n            logging.error(f\"DataFrame missing required columns: {required_cols}\")\n            return\n        \n        # Process each unique role/level combination\n        role_combinations = df[['level', 'role_title']].drop_duplicates()\n        for _, combo in role_combinations.iterrows():\n            xml_data = self.compile_xml_for_role(\n                combo['level'], \n                combo['role_title'], \n                df\n            )\n            self.save_xml(combo['level'], combo['role_title'], xml_data)\n```\n:::\n\n\n::: {#e86f399c .cell execution_count=15}\n``` {.python .cell-code}\nxml_generator = XMLGenerator()\nxml_generator.generate_all_xml(df)\n```\n:::\n\n\n## o1-generated Senior AI Research Scientist\n\nHere's what `o1` said about the Responsibilities and Skills observed in Senior AI Research Scientist roles:\n\n### Responsibilities\n- **Conduct advanced AI research**  \n  Many roles require pushing the state-of-the-art in Generative AI, LLMs, and related areas (e.g., video/multimodal models, diffusion models) through novel algorithms, architectures, and experimental studies.\n\n- **Train and fine-tune large-scale models**  \n  Commonly involves working with massive datasets and distributed training setups (thousands of GPUs, HPC environments) to develop foundation models and advanced AI systems.\n\n- **Develop and implement new algorithms or architectures**  \n  Spans designing novel model architectures (e.g., diffusion, transformer-based, multimodal fusion) and creating robust data processing or simulation pipelines to support AI solutions.\n\n- **Collaborate with cross-functional teams**  \n  Emphasizes close work with engineering, product management, research, and external stakeholders to integrate AI breakthroughs into real-world applications and products.\n\n- **Evaluate and measure AI performance**  \n  Entails building rigorous evaluation frameworks, designing new metrics, and systematically analyzing model behavior to ensure quality and reliability.\n\n- **Publish and communicate research findings**  \n  Many positions highlight writing influential papers, presenting at conferences, and sharing innovative results both internally and with the broader AI community.\n\n- **Build and maintain data pipelines**  \n  Involves constructing high-quality, scalable data pipelines or tooling to support training, fine-tuning, and inference of large models.\n\n- **Ensure production-grade implementation**  \n  Requires writing clean, efficient, and maintainable code as well as optimizing models and pipelines to meet performance, reliability, and quality standards.\n\n### Skills\n\n- **Proficiency in Python and deep learning frameworks**  \n  Strong coding skills in Python and hands-on experience with libraries such as PyTorch, TensorFlow, or JAX appear in nearly every role.\n\n- **Expertise with LLMs and Generative AI**  \n  Deep understanding of transformer architectures, diffusion models, multimodal systems, prompt engineering, and other advanced AI techniques is frequently mentioned.\n\n- **Experience with large-scale/distributed training**  \n  Many roles emphasize knowledge of HPC, GPU optimization, model parallelism (e.g., FSDP, DeepSpeed, Megatron-LM), and handling massive datasets.\n\n- **Strong software engineering practices**  \n  Testing, code review, debugging, version control, and producing clean, modular research or production code are consistently important.\n\n- **Collaboration and communication skills**  \n  Clear written and verbal communication, along with cross-functional teamwork, is vital for integrating AI solutions into products and relaying complex ideas.\n\n- **Research acumen and adaptability**  \n  Ability to read, interpret, and prototype cutting-edge AI literature, publish findings, and rapidly iterate on experiments.\n\n- **Machine Learning fundamentals**  \n  Solid grounding in ML theory (e.g., optimization, statistics, data structures) and experience with model evaluation, data manipulation, and pipeline design.\n\n- **Familiarity with prompt engineering and advanced NLP concepts**  \n  Many roles highlight crafting effective prompts, aligning model outputs with user needs, and leveraging text-generation or conversational AI techniques.\n\n## o1-generated Senior AI ML Engineer\n\nAnd here's what `o1` said about the Responsibilities and Skills observed in Senior AI ML Engineer roles:\n\n### Responsibilities\n\n- **Design, develop, and deploy AI/ML solutions**  \n  End-to-end creation of machine learning systems, from initial prototypes to production-ready deployments.\n\n- **Collaborate with cross-functional teams**  \n  Work closely with product managers, data scientists, engineers, and other stakeholders to align technical solutions with business goals.\n\n- **Monitor and optimize model performance**  \n  Track key metrics, fine-tune models, and iterate to ensure continuous improvement and reliability in production.\n\n- **Stay current with AI research and emerging technologies**  \n  Keep up-to-date with the latest breakthroughs in areas like LLMs, generative AI, and deep learning.\n\n- **Mentor and coach team members**  \n  Provide guidance on best practices, design patterns, code quality, and career development for junior or peer engineers.\n\n- **Develop scalable data/ML pipelines**  \n  Build robust infrastructure for data collection, preprocessing, model training, and deployment at scale.\n\n- **Implement and maintain CI/CD and coding best practices**  \n  Ensure code quality, streamline release processes, and enforce testing discipline for AI/ML components.\n\n- **Integrate and leverage LLMs/generative AI**  \n  Incorporate large language models or generative methods into products and workflows.\n\n- **Prototype and experiment**  \n  Conduct R&D, proof-of-concepts, and pilot programs to explore emerging AI techniques and validate new product ideas.\n\n- **Document and communicate findings**  \n  Produce clear technical documentation, share results with stakeholders, and provide actionable insights for decision-making.\n\n\n### Skills\n\n- **Proficiency in Python**  \n  Commonly required for AI/ML development, data manipulation, and scripting.\n\n- **Experience with ML/DL frameworks**  \n  Hands-on expertise in tools like PyTorch, TensorFlow, or JAX for building and training models.\n\n- **Familiarity with cloud platforms**  \n  Working knowledge of AWS, GCP, or Azure for deploying and scaling AI solutions.\n\n- **Expertise in LLMs/generative AI**  \n  Understanding of transformer architectures, prompt engineering, retrieval-augmented generation (RAG), and related libraries.\n\n- **Strong software engineering fundamentals**  \n  Solid grasp of algorithms, data structures, design patterns, and best practices for production code.\n\n- **Knowledge of MLOps and CI/CD**  \n  Experience with containerization (Docker, Kubernetes), version control (Git), and automated testing/monitoring.\n\n- **Data processing and SQL**  \n  Skills in handling large datasets, working with Spark or similar frameworks, and writing performant SQL queries.\n\n- **Effective communication and collaboration**  \n  Ability to translate complex technical concepts for non-technical stakeholders and work well in diverse teams.\n\n- **Problem-solving and debugging**  \n  Track record of diagnosing issues in production environments and implementing reliable fixes.\n\n- **Continuous learning mindset**  \n  Eagerness to stay on top of new AI research, frameworks, and technologies to innovate and improve solutions.\n\n# Job titles\n\nWhat are the most common job titles for these roles?\n\n::: {#f9cfdb47 .cell code-fold-show='false' execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Click to view the code for aggregating titles\"}\ndef aggregate_titles_by_role_level(df: pd.DataFrame, output_dir: str = 'role_titles') -> None:\n    \"\"\"\n    Aggregate job titles for each role_title and level combination and save to JSON.\n    \n    Args:\n        df: DataFrame containing 'role_title', 'level', and 'title' columns\n        output_dir: Directory to save the JSON output\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Group by role_title and level\n    grouped = df.groupby(['role_title', 'level'])\n    \n    for (role, level), group in grouped:\n        # Get unique titles for this combination\n        result = {\n            'titles': group['title'].unique().tolist()\n        }\n        \n        # Create safe filename from role and level\n        safe_role = role.replace('/', '_').replace(' ', '_')\n        safe_level = level.replace('/', '_').replace(' ', '_')\n        filename = f\"{safe_level}_{safe_role}.json\"\n        \n        # Save to JSON file\n        with open(os.path.join(output_dir, filename), 'w') as f:\n            json.dump(result, f, indent=2)\n            \n        print(f\"Saved titles for {level} {role}\")\n```\n:::\n\n\n::: {#607af472 .cell execution_count=17}\n``` {.python .cell-code}\naggregate_titles_by_role_level(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSaved titles for Entry-level AI Research Scientist\nSaved titles for Mid-Level / Level AI Research Scientist\nSaved titles for Senior-Level / Level AI Research Scientist\nSaved titles for Entry-level AI Solution Architect\nSaved titles for Mid-Level / Level AI Solution Architect\nSaved titles for Senior-Level / Level AI Solution Architect\nSaved titles for Entry-level AI/ML Engineer\nSaved titles for Mid-Level / Level AI/ML Engineer\nSaved titles for Senior-Level / Level AI/ML Engineer\nSaved titles for Entry-level MLOps / AI Infrastructure Engineer\nSaved titles for Mid-Level / Level MLOps / AI Infrastructure Engineer\nSaved titles for Senior-Level / Level MLOps / AI Infrastructure Engineer\n```\n:::\n:::\n\n\nI'll use `o1` again to generate a summary of the most common titles for each role/level combination. In the prompt, I will omit my own role classification so as not to bias the results. I'll ask it to \"identify the most common titles listed below (ignoring slight variations), and then for those titles identify how often they occurred.\"\n\nHere's a sample of what it said.\n\nFor Senior AI Research Scientist roles, `o1` observed the following titles to be common to this role:\n\n- Research Scientist/Researcher (14 occurrences)\n- Research Engineer (7 occurrences)\n- Machine Learning Engineer (3 occurrences)\n- Generative AI Engineer (2 occurrences)\n- Software Engineer (Generative AI) (2 occurrences)\n\nFor Senior AI ML Engineer roles, `o1` observed the following titles to be common to this role:\n\n- Senior Software Engineer (6 occurrences)\n- Senior Machine Learning Engineer (5 occurrences)\n- Staff Software Engineer (5 occurrences)\n- Senior AI Engineer (4 occurrences)\n- Machine Learning Researcher (3 occurrences)\n- Staff Machine Learning Engineer (2 occurrences)\n- Principal Software Engineer (2 occurrences)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}