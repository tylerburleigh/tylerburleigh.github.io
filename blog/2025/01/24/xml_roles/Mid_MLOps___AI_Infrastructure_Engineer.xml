
Identify the most common Responsibilities and Skills listed in the jobs below, returning a bulleted list in the format

# Responsibilities
- [responsibility]: [responsibility description]
- [responsibility]: [responsibility description]
...
# Skills
- [skill]: [skill description]
- [skill]: [skill description]
...
<Job>
<Responsibilities>['Collaborate closely with Data Scientists and Data Engineers to design and implement scalable, efficient machine learning pipelines that meet business needs.', 'Evaluate machine learning models to ensure optimal performance and scalability, making necessary adjustments to improve outcomes.', 'Deploy machine learning models into production environments, ensuring seamless integration, and continuously monitor their performance to maintain reliability and accuracy.', 'Manage and oversee data science infrastructure to streamline the model development and deployment process, ensuring efficient workflows.', 'Propose and implement appropriate tools, including languages, libraries, and frameworks, to enhance project execution and outcomes.', 'Work in collaboration with infrastructure architects to develop scalable and efficient solutions that align with organizational goals and technological capabilities.', 'Collaborate with multi-functional teams to integrate machine learning models into existing systems and processes, ensuring a smooth transition and effective implementation.', 'Stay ahead of the latest advancements in Machine Learning (ML), Artificial Intelligence (AI), and Generative AI (GenAI) to incorporate innovative techniques and technologies into projects.', 'Mentor peers and provide guidance on ML/LLMOps standard practices, fostering a culture of continuous learning and improvement within the team.']</Responsibilities>
<Skills>['Strong programming skills in Python, with experience in machine learning libraries like TensorFlow, PyTorch, or scikit-learn.', 'Strong familiarity with MLOps practices, including model monitoring, versioning, and lifecycle management.', 'Advanced knowledge of SQL and experience working with relational databases, alongside proficiency in RDBMS and NoSQL databases.', 'Experience in infrastructure management, including cloud computing, Linux OS, networks, Docker, and Kubernetes.', 'Expertise in cloud-native architecture, preferably using the Azure stack, with skills in Azure ML, Databricks (Spark), and Azure Data Factory.', 'Proficient in building ETL pipelines for feature engineering on large-scale datasets using Spark.', 'Experience with Large Language Models (LLMs) including proprietary or open source models.', 'Ability to balance urgency with the delivery of high-quality, pragmatic solutions.', 'Expertise in delivering analytics and machine learning products, with a deep understanding of agile product delivery in enterprise environments.']</Skills>
</Job>
<Job>
<Responsibilities>['Enhance LLM training efficiency by optimizing scripts and architectures, leveraging CUDA and advanced GPU acceleration techniques to improve performance and reduce training time.', 'Optimize preprocessing pipelines by resolving timeout issues and implementing caching strategies.', 'Ensure reliable distributed networking by addressing connection failures and implementing monitoring systems.', 'Minimize downtime on rental machines through efficient recovery workflows and off-hour error mitigation.', 'Streamline debugging of distributed systems, addressing node failures and network partitions.', 'Develop scalable logging frameworks for efficient error detection and resolution.', 'Optimize system scalability by balancing workloads and reducing communication overhead.', 'Design fault-tolerant systems to handle node crashes and ensure seamless job restarts.', 'Monitor system health and implement recovery strategies for unresponsive nodes.', 'Manage massive data sets for large-scale model training workflows.', 'Develop real-time monitoring frameworks for training processes to detect and alert issues promptly.']</Responsibilities>
<Skills>['Strong proficiency in Python and experience with distributed ML frameworks (e.g., PyTorch, TensorFlow, Horovod, or Ray).', 'Understanding of networking protocols and distributed communication libraries (e.g., NCCL, gRPC).', 'Hands-on experience with cloud platforms (AWS, GCP, Azure) and cluster orchestration tools (Kubernetes, Slurm).', 'Proven ability to debug and resolve issues in large-scale distributed systems.', 'Familiarity with fault tolerance, caching strategies, and scalable logging systems.', 'Excellent problem-solving and communication skills.']</Skills>
</Job>
<Job>
<Responsibilities>['• Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.', '• Design complex data solutions.', '• Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.', '• Incorporate core data management competencies including data governance, data security, and data quality.', '• Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.', '• Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.', '• Test data movement, transformation code, and data components.', '• Design and implement secure data pipelines to support LLM and GenAI applications across international markets.', '• Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.', '• Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.', '• Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.', '• Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.', '• Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.', '• Perform other duties as assigned.']</Responsibilities>
<Skills>['• Technical expertise in Large Language Models (LLMs) and Generative AI platforms (Anthropic, OpenAI).', '• Prompt engineering and LLM optimization techniques.', '• Retrieval-Augmented Generation (RAG) architectures.', '• Vector database implementations (Postgres, OpenSearch, Pinecone, or Weaviate).', '• AI Agent development and orchestration.', '• Enterprise API development and integration.', '• Responsible AI and AI safety practices.', '• Data security and privacy frameworks.', '• Cloud and AI infrastructure experience, including AWS services (especially Bedrock, Lambda, S3).', '• LLM deployment strategies and integration.', '• Model serving and scaling techniques.', '• Container orchestration (Kubernetes/Docker).', '• Infrastructure as Code (Terraform).', '• MLOps and monitoring practices.', '• Data processing and storage, including Snowflake and data warehouse technologies.', '• Real-time and batch processing systems.', '• ETL/ELT pipeline development.', '• Data quality and validation frameworks.', '• Vector embedding techniques.', '• Knowledge base creation and management.', '• Development practices, including Python, SQL, and shell scripting.', '• API design and development.', '• Git and CI/CD pipelines.', '• Testing and monitoring frameworks.', '• Documentation and technical writing.', '• Agile/Scrum methodologies.']</Skills>
</Job>
<Job>
<Responsibilities>['Design, implement, and optimize distributed training frameworks tailored for large language models.', 'Develop custom modules, plugins, and features to enhance framework scalability and performance.', 'Optimize communication patterns (e.g., gradient synchronization, all-reduce) in distributed training.', 'Implement techniques like mixed precision, tensor parallelism, pipeline parallelism, and sharded training.', 'Conduct in-depth profiling and debugging of training jobs to identify and resolve bottlenecks.', 'Collaborate with hardware teams to optimize performance for GPUs, TPUs, and other accelerators.', 'Ensure training systems scale efficiently to thousands of nodes and petabytes of data.', 'Develop resilience mechanisms for fault-tolerant and checkpointed training pipelines.', 'Work closely with researchers, data engineers, and platform teams to ensure training frameworks meet model and workload requirements.', 'Provide guidance and tools to improve the overall efficiency of the LLM development lifecycle.']</Responsibilities>
<Skills>['Expertise in distributed training frameworks (e.g., PyTorch DDP, DeepSpeed, Megatron-LM, TensorFlow XLA).', 'Strong understanding of parallelism techniques (e.g., data, tensor, pipeline, and ZeRO-based parallelism).', 'Familiarity with GPU/TPU hardware and deep learning performance optimizations.', 'Proficient in Python and C++ or CUDA for high-performance computing.', 'Experience with memory optimization techniques (e.g., activation checkpointing, gradient sharding).', 'Knowledge of training dynamics for large-scale LLMs, including hyperparameter tuning and optimization.', 'Analytical problem-solving skills and a focus on performance improvement.', 'Strong collaboration and communication skills across teams.']</Skills>
</Job>
<Job>
<Responsibilities>['Design, develop, and maintain robust and scalable GenAI Solutions, including inference services, automated workflows, and data ingestion systems.', 'Optimize infrastructure resources to maximize efficiency and minimize costs.', 'Build and maintain tools for model experimentation, visualization, and monitoring.', 'Collaborate with cross-functional teams to understand their needs and translate them into technical solutions.', 'Stay up-to-date with the latest advancements in AI, machine learning, and cloud technologies.']</Responsibilities>
<Skills>['Strong proficiency in programming languages like Java or Python.', 'Familiarity with GenAI frameworks (LangChain, LlamaIndex, etc.).', 'Experience with cloud platforms (AWS, GCP) and containerization technologies (Docker, Kubernetes).', 'Strong problem-solving and analytical skills.', 'Excellent communication and collaboration skills.']</Skills>
</Job>
<Job>
<Responsibilities>['Design, build, and maintain systems for training and evaluating models.', 'Work closely with Applied Scientists to build systems for training large multimodal models.', 'Evaluate performance of the training infrastructure, diagnose problems, and address gaps.', 'Develop reliable infrastructure to schedule training and model evaluation jobs across clusters.', 'Collaborate with researchers to create new techniques, infrastructure, and tooling for emerging research capabilities.', 'Manage project prioritization, deliverables, timelines, and stakeholder communication.', 'Educate the team on best practices and influence technical strategy.', 'Operate in a dynamic environment to deliver high-quality software.']</Responsibilities>
<Skills>['Ability to evaluate and diagnose performance issues in training infrastructure.', 'Experience in developing reliable infrastructure for scheduling jobs across clusters.', 'Strong collaboration skills with researchers and scientists.', 'Project management skills, including prioritization and communication.', 'Ability to educate and influence team on best practices and technical strategy.', 'Adaptability to operate in a dynamic and fast-paced environment.']</Skills>
</Job>
