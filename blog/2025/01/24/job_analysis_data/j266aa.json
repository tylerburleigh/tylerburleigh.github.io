[
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Prompt Engineering and Large Language Models (LLMs)",
    "activity": "Fine-tuning and customization of language models",
    "analysis": "The Job Responsibilities mention designing and implementing secure data pipelines to support LLM and GenAI applications, which suggests involvement with large language models. However, there is no specific mention of fine-tuning or customizing language models, which is a specialized task. The responsibilities focus more on data solutions, governance, and infrastructure rather than model training or fine-tuning.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Prompt Engineering and Large Language Models (LLMs)",
    "activity": "Prompt engineering and iterative refinement",
    "analysis": "The Job Responsibilities do not explicitly mention prompt engineering or iterative refinement of language models. The focus is on data solutions, governance, and infrastructure for AI applications, rather than crafting and refining prompts for LLMs. Therefore, this activity does not seem directly relevant to the responsibilities listed.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Prompt Engineering and Large Language Models (LLMs)",
    "activity": "Model evaluation and performance assessment",
    "analysis": "The Job Responsibilities do not specifically mention evaluating or assessing the performance of language models. The responsibilities are more aligned with data management, governance, and infrastructure for AI applications. There is no indication of tasks related to model evaluation or performance assessment.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Prompt Engineering and Large Language Models (LLMs)",
    "activity": "Integration with downstream applications",
    "analysis": "The Job Responsibilities include designing and implementing secure data pipelines to support LLM and GenAI applications, which implies integration with downstream applications. This aligns with the activity of incorporating LLM outputs into end-user experiences or pipelines. Therefore, this activity is relevant to the responsibilities.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Prompt Engineering and Large Language Models (LLMs)",
    "activity": "Model interpretability and debugging",
    "analysis": "The Job Responsibilities do not mention tasks related to model interpretability or debugging. The focus is on data solutions, governance, and infrastructure rather than analyzing model outputs or employing interpretability techniques. Thus, this activity does not appear to be relevant to the responsibilities listed.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Architecture blueprint for AI-driven pipelines",
    "analysis": "The job responsibilities include designing and implementing secure data pipelines to support LLM and GenAI applications, as well as creating and optimizing data streaming architectures for real-time AI applications. These tasks align closely with the activity of designing end-to-end workflows for AI-driven pipelines, which involves data ingestion, preprocessing, and integration of components like data storage and API interfaces. Therefore, this activity is relevant to the job responsibilities.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Compute requirements and resource planning",
    "analysis": "The job responsibilities do not explicitly mention assessing computational demands or planning for scalability in terms of infrastructure like GPUs or TPUs. While there is a focus on building and maintaining infrastructure, the specific aspect of compute requirements and resource planning is not directly addressed. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Performance optimization and latency control",
    "analysis": "The job responsibilities include implementing monitoring and observability solutions for AI data pipelines to ensure performance and reliability. However, there is no specific mention of employing caching mechanisms, model distillation, or latency control strategies. The focus is more on data pipelines rather than performance optimization of AI models. Therefore, this activity is not directly relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Model lifecycle management and governance",
    "analysis": "The job responsibilities mention incorporating core data management competencies, including data governance, and collaborating with international teams to establish data governance practices for AI applications. However, there is no specific mention of model lifecycle management, such as continuous retraining or versioning of models. Therefore, this activity is partially relevant but not directly addressed in the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "System interoperability and integration",
    "analysis": "The job responsibilities include developing APIs and integration layers for enterprise AI services, which aligns with the activity of aligning generative AI services with existing enterprise systems and establishing APIs. This indicates a focus on system interoperability and integration, making this activity relevant to the job responsibilities.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Conducting research to evaluate AI quality",
    "analysis": "The job responsibilities do not explicitly mention conducting research to evaluate AI quality, such as investigating system accuracy, robustness, or designing experiments. The focus is more on data solutions, governance, and infrastructure. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Developing AI systems and algorithms for automated insights",
    "analysis": "The job responsibilities do not specifically mention developing AI systems or algorithms for automated insights. The focus is on data solutions, pipelines, and infrastructure rather than building AI methods for analyzing user behavior or system performance. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Synthetic data generation",
    "analysis": "The job responsibilities do not mention synthetic data generation or methods for producing synthetic datasets. The focus is on data solutions, governance, and infrastructure rather than generating synthetic data for AI models. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Identifying performance metrics and success criteria",
    "analysis": "The job responsibilities do not explicitly mention identifying performance metrics or success criteria. The focus is on data solutions, governance, and infrastructure rather than defining measurable indicators for AI models. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Creating labeled datasets for AI evaluation and optimization",
    "analysis": "The job responsibilities do not mention creating labeled datasets for AI evaluation and optimization. The focus is on data solutions, governance, and infrastructure rather than preparing data annotations for model training. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Designing, running, and analyzing experiments (A/B tests)",
    "analysis": "The job responsibilities do not mention designing, running, or analyzing experiments such as A/B tests. The focus is on data solutions, governance, and infrastructure rather than conducting controlled experiments for AI models. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Solution Architecture for Generative AI",
    "activity": "Sampling data for evaluation",
    "analysis": "The job responsibilities do not mention sampling data for evaluation. The focus is on data solutions, governance, and infrastructure rather than using statistical methods to sample data for AI model evaluation. Therefore, this activity is not relevant to the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Annotation and Labeling",
    "activity": "Designing labeling tasks for data annotation",
    "analysis": "The Job Responsibilities focus on building and operationalizing data solutions, designing complex data solutions, and incorporating data management competencies. While these tasks involve data handling and management, they do not specifically mention designing labeling tasks for data annotation. The responsibilities are more aligned with data engineering and management rather than data annotation and labeling.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Annotation and Labeling",
    "activity": "Collecting human annotation data",
    "analysis": "The Job Responsibilities do not mention managing logistics or communication for annotation projects, nor do they involve overseeing performance of annotation teams. The focus is on data solutions, governance, and AI applications, which are distinct from the tasks involved in collecting human annotation data.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Annotation and Labeling",
    "activity": "Analyzing human annotation data",
    "analysis": "The Job Responsibilities include performing analysis of complex data sources and recommending data for analytical processes. However, the analysis mentioned is more related to data solutions and AI applications rather than analyzing human annotation data specifically. The responsibilities do not explicitly cover statistical analysis of annotation data or evaluating dataset quality in the context of data annotation.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Science, Data Engineering, and Analytics",
    "activity": "Statistical methods",
    "analysis": "The Job Responsibilities focus on building and operationalizing data solutions, designing data solutions, performing data analysis, and developing secure data pipelines. While these tasks involve data management and engineering, there is no explicit mention of applying statistical techniques for hypothesis testing, inference, or building predictive models. The responsibilities are more aligned with data engineering and infrastructure rather than statistical analysis.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Science, Data Engineering, and Analytics",
    "activity": "Data visualization and exploratory analysis",
    "analysis": "The Job Responsibilities include tasks such as performing analysis of complex sources and collaborating to support delivery and educate end users on complex data products. However, there is no specific mention of employing visualization tools or conducting exploratory data analysis to reveal patterns or communicate insights. The focus is more on data engineering and pipeline development rather than data visualization.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Science, Data Engineering, and Analytics",
    "activity": "Data mining and analysis of large conversational datasets",
    "analysis": "The Job Responsibilities involve performing data and system analysis, testing data movement, and designing secure data pipelines. While these tasks relate to data processing and analysis, there is no specific mention of exploring user-generated interactions or analyzing conversational datasets. The focus is more on data infrastructure and pipeline development rather than data mining of conversational datasets.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Science, Data Engineering, and Analytics",
    "activity": "Developing data models and pipelines for analytics",
    "analysis": "The Job Responsibilities include building and operationalizing complex data solutions, designing data solutions, and developing secure data pipelines. These tasks align closely with implementing scalable data processing and ETL workflows, which are essential for developing data models and pipelines for analytics. The responsibilities emphasize data engineering and pipeline development, making this activity relevant.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Data Science, Data Engineering, and Analytics",
    "activity": "Define metrics and success criteria",
    "analysis": "The Job Responsibilities do not explicitly mention defining metrics or success criteria. The focus is on data engineering tasks such as building data solutions, designing pipelines, and ensuring data security and governance. While metrics and success criteria are important for assessing model health, they are not directly addressed in the listed responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Machine Learning Operations (MLOps)",
    "activity": "Deployment and MLOps",
    "analysis": "The Job Responsibilities mention designing and implementing secure data pipelines, developing APIs and integration layers, and creating data streaming architectures. These tasks align with the deployment and MLOps activity, which involves designing and maintaining CI/CD pipelines and ensuring the performance, scalability, and reliability of AI systems in production. The focus on operationalizing complex data solutions and supporting delivery also suggests relevance to MLOps practices.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Machine Learning Operations (MLOps)",
    "activity": "Monitoring and observability",
    "analysis": "The Job Responsibilities include implementing monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations. This directly aligns with the activity of implementing real-time monitoring and logging solutions to track model performance, latency, and errors in production. The emphasis on ensuring performance and reliability indicates a strong relevance to monitoring and observability.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Machine Learning Operations (MLOps)",
    "activity": "Data and model versioning",
    "analysis": "The Job Responsibilities do not explicitly mention version control systems for datasets or model artifacts. While there is a focus on data management competencies, such as data governance and data quality, there is no direct reference to versioning or ensuring reproducibility and traceability throughout the model lifecycle. Therefore, this activity may not be directly relevant to the stated responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Machine Learning Operations (MLOps)",
    "activity": "Continuous testing and validation",
    "analysis": "The Job Responsibilities include testing data movement, transformation code, and data components, which suggests a focus on validation and testing. However, there is no explicit mention of automating validation steps for models or introducing guardrails like canary testing or shadow deployments. While there is some overlap, the specific focus on continuous testing and validation of models is not clearly addressed in the responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Machine Learning Operations (MLOps)",
    "activity": "Infrastructure and scalability",
    "analysis": "The Job Responsibilities mention designing and implementing secure data pipelines and creating data streaming architectures, which could imply a need for scalable infrastructure. However, there is no explicit mention of optimizing and scaling model-serving infrastructure or using container orchestration tools. The focus is more on data solutions rather than model-serving infrastructure, suggesting limited relevance to this activity.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Machine Learning Operations (MLOps)",
    "activity": "Governance, compliance, and security",
    "analysis": "The Job Responsibilities include incorporating core data management competencies such as data governance, data security, and data quality. Additionally, there is a focus on ensuring compliance with regional data protection requirements and collaborating to establish data governance practices. These responsibilities align closely with the activity of establishing governance frameworks for data privacy, ethics, and regulatory compliance, indicating strong relevance.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Machine Learning Operations (MLOps)",
    "activity": "Model lifecycle management and continuous improvement",
    "analysis": "The Job Responsibilities do not explicitly mention model lifecycle management or continuous improvement activities such as retraining, hyperparameter tuning, or performance evaluations. While there is a focus on supporting delivery and educating end users, the specific tasks related to model lifecycle management are not addressed. Therefore, this activity may not be directly relevant to the stated responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "User Experience (UX) Research",
    "activity": "User interviews and surveys",
    "analysis": "The Job Responsibilities focus on building and operationalizing data solutions, data management, and AI applications. There is no mention of gathering qualitative or quantitative insights on user needs through interviews or surveys. The responsibilities are more technical and data-centric, rather than user experience research-oriented.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "User Experience (UX) Research",
    "activity": "Usability testing and prototyping",
    "analysis": "The Job Responsibilities do not include tasks related to designing or conducting usability tests, working with prototypes, or enhancing user experience. The focus is on data solutions, AI applications, and data governance, which are not directly related to usability testing or prototyping.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "User Experience (UX) Research",
    "activity": "Persona development and user journey mapping",
    "analysis": "The Job Responsibilities do not involve creating user personas or mapping user journeys. The tasks are centered around data solutions, AI applications, and data management, which do not align with the activities of persona development or user journey mapping.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "User Experience (UX) Research",
    "activity": "Qualitative data analysis",
    "analysis": "While the Job Responsibilities involve data analysis, they are focused on complex data solutions, data governance, and AI applications rather than synthesizing interview notes or open-ended survey responses. The qualitative data analysis described in the activity is more aligned with user experience research, which is not the focus of the job responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Software Engineering",
    "activity": "Designing and implementing software architecture",
    "analysis": "The job responsibilities include designing complex data solutions and implementing secure data pipelines, which align with defining system components, data flows, and integration points as described in this activity. The focus on ensuring alignment with business needs and performance requirements is also relevant to the responsibilities of building and operationalizing complex data solutions.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Software Engineering",
    "activity": "Developing production-grade applications",
    "analysis": "The job responsibilities emphasize building and maintaining data solutions, developing APIs, and creating data streaming architectures. While these tasks involve writing code and ensuring quality, the specific focus on production-grade applications and coding standards is not explicitly mentioned in the responsibilities. The responsibilities are more data-centric rather than application-centric.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Software Engineering",
    "activity": "Continuous integration and delivery (CI/CD)",
    "analysis": "The job responsibilities do not explicitly mention CI/CD processes such as automating build, test, and deployment processes. The focus is more on data solutions, pipelines, and governance rather than software delivery processes.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Software Engineering",
    "activity": "Monitoring and incident management",
    "analysis": "The job responsibilities include implementing monitoring and observability solutions for AI data pipelines, which directly relates to implementing logging, metrics, and alerting solutions for system health. Additionally, performing data and system analysis, assessment, and resolution for complex defects and incidents aligns with incident management.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Software Engineering",
    "activity": "API development and system integration",
    "analysis": "The job responsibilities include developing APIs and integration layers for enterprise AI services, which directly aligns with designing and implementing APIs for internal and external integrations. The emphasis on compliance and secure services also matches the best practices for authentication and authorization mentioned in this activity.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Software Engineering",
    "activity": "Software testing and quality assurance",
    "analysis": "The job responsibilities mention testing data movement, transformation code, and data components, which relates to software testing and quality assurance. However, the focus is more on data-specific testing rather than comprehensive software testing and quality assurance practices.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "Software Engineering",
    "activity": "DevOps and infrastructure as code",
    "analysis": "The job responsibilities do not explicitly mention DevOps practices or infrastructure as code. The focus is more on data solutions, pipelines, and governance rather than managing and provisioning cloud resources or promoting a DevOps culture.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "AI Evaluation, Experimentation and Research",
    "activity": "Conducting research to evaluate AI quality",
    "analysis": "The job responsibilities focus on building and operationalizing data solutions, designing data solutions, and ensuring data governance, security, and quality. While these tasks are related to data management and infrastructure, they do not explicitly mention conducting research to evaluate AI quality, such as investigating system accuracy, robustness, or fairness. The responsibilities are more aligned with data engineering and pipeline development rather than AI evaluation research.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "AI Evaluation, Experimentation and Research",
    "activity": "Developing AI systems and algorithms for automated insights",
    "analysis": "The job responsibilities include designing and implementing secure data pipelines to support LLM and GenAI applications, as well as developing APIs and integration layers for enterprise AI services. These tasks suggest involvement in developing AI systems and infrastructure, which aligns with the activity of building and refining AI methods for automated insights. The focus on AI applications and services indicates relevance to this activity.",
    "is_relevant": true
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "AI Evaluation, Experimentation and Research",
    "activity": "Synthetic data generation",
    "analysis": "The job responsibilities do not explicitly mention synthetic data generation. The focus is on data solutions, governance, security, and pipeline development. While synthetic data generation could be a part of data solutions, there is no direct mention or implication of generating synthetic datasets for AI models in the responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "AI Evaluation, Experimentation and Research",
    "activity": "Identifying performance metrics and success criteria",
    "analysis": "The job responsibilities do not explicitly mention identifying performance metrics or success criteria. The focus is on data management, pipeline development, and ensuring data quality and security. While performance metrics could be relevant to data quality and system performance, there is no direct mention of defining or tracking these metrics in the responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "AI Evaluation, Experimentation and Research",
    "activity": "Creating labeled datasets for AI evaluation and optimization",
    "analysis": "The job responsibilities do not explicitly mention creating labeled datasets. The focus is on data solutions, governance, security, and pipeline development. While labeled datasets could be part of data solutions, there is no direct mention or implication of preparing data annotations for AI evaluation in the responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "AI Evaluation, Experimentation and Research",
    "activity": "Designing, running, and analyzing experiments (A/B tests)",
    "analysis": "The job responsibilities do not explicitly mention designing, running, or analyzing experiments such as A/B tests. The focus is on data solutions, governance, security, and pipeline development. While experiments could be part of data analysis, there is no direct mention or implication of conducting controlled experiments in the responsibilities.",
    "is_relevant": false
  },
  {
    "filename": "j266aa",
    "responsibilities": [
      "\u2022 Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
      "\u2022 Design complex data solutions.",
      "\u2022 Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
      "\u2022 Incorporate core data management competencies including data governance, data security, and data quality.",
      "\u2022 Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
      "\u2022 Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
      "\u2022 Test data movement, transformation code, and data components.",
      "\u2022 Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
      "\u2022 Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
      "\u2022 Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
      "\u2022 Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
      "\u2022 Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
      "\u2022 Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
      "\u2022 Perform other duties as assigned."
    ],
    "category": "AI Evaluation, Experimentation and Research",
    "activity": "Sampling data for evaluation",
    "analysis": "The job responsibilities do not explicitly mention sampling data for evaluation. The focus is on data solutions, governance, security, and pipeline development. While data sampling could be part of data analysis, there is no direct mention or implication of using statistical methods to sample data for evaluation in the responsibilities.",
    "is_relevant": false
  }
]