{
  "responsibilities": [
    "Design, develop, and maintain data pipelines to integrate, transform, and load data from diverse sources.",
    "Optimize data workflows for performance, reliability, and scalability.",
    "Collaborate with analysts and data scientists to maintain and enhance the data warehouse architecture.",
    "Develop ETL/ELT processes to ensure timely and accurate data delivery.",
    "Automate and monitor ETL workflows for continuous reliability.",
    "Implement tools and frameworks to ensure data quality, accuracy, and consistency.",
    "Proactively identify and resolve data issues.",
    "Partner with software engineers, product managers, and business stakeholders to understand data requirements.",
    "Share knowledge and collaborate to ensure alignment with organizational objectives.",
    "Monitor and optimize the performance of data systems.",
    "Troubleshoot and resolve data-related challenges in real-time.",
    "Create and maintain documentation for data pipelines, workflows, and system configurations.",
    "Promote best practices for data governance, coding standards, and security."
  ],
  "skills": [
    "Proficiency with cloud platforms (AWS, Azure, or Google Cloud).",
    "Hands-on experience with tools like Databricks, ELT/ETL (preferably Fivetran), SQL, Python, and Spark.",
    "Familiarity with event-tracking tools like Segment (nice to have).",
    "Strong analytical skills and the ability to solve complex data engineering challenges.",
    "Ability to work effectively in cross-functional teams.",
    "Ability to communicate technical concepts to diverse audiences."
  ],
  "qualifications": [
    "5+ years of experience in data engineering with hands-on expertise in building scalable and reliable data platforms.",
    "Proven experience with data pipelines, data integration, and warehouse optimization.",
    "Bachelorâ€™s degree in computer science, Engineering, or a related field."
  ]
}