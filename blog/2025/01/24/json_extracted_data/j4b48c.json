{
  "responsibilities": [
    "Design, implement, and maintain scalable data lakehouse solutions in Microsoft Azure, ensuring data reliability, integrity, and security.",
    "Develop, optimize, and manage ETL/ELT pipelines for data integration, transformation, and storage using Azure Data Factory, DataBricks, and Delta Lake.",
    "Collaborate with cross-functional teams to define data architecture strategies that support business goals and analytics requirements.",
    "Monitor and improve data pipeline performance, addressing bottlenecks and ensuring efficient data flow and processing.",
    "Implement best practices for data governance, data quality, and metadata management in the data lakehouse environment.",
    "Manage and optimize large-scale data processing jobs using Apache Spark and SparkSQL within the DataBricks environment.",
    "Develop and maintain automated data workflows and processes, leveraging Infrastructure as Code (IaC) and Policy as Code (PaC) where applicable.",
    "Ensure seamless integration of data sources, enabling real-time analytics and business intelligence through streaming and batch processing.",
    "Conduct regular audits of data infrastructure to ensure compliance with security policies and regulatory requirements.",
    "Provide technical leadership and mentorship to data engineers and data scientists, fostering a culture of continuous learning and improvement."
  ],
  "skills": [
    "Familiarity with Infrastructure as Code (IaC) and Policy as Code (PaC).",
    "Hands-on exposure to logging and monitoring solutions.",
    "Familiarity with Microsoft Azure IaaS & PaaS solutions.",
    "Expertise to continuously deliver while valuing and maintaining a strong attention to detail.",
    "Experience with Agile software development organizations.",
    "Ability to quickly identify and drive to the optimal solution when presented with a series of constraints."
  ],
  "qualifications": [
    "Bachelorâ€™s level degree in Computer Science, Engineering, or appropriate work experience required.",
    "Minimum 5+ years of experience designing, building, and enhancing ETL processes for data warehousing, data marts, data integrations, data analysis across the enterprise.",
    "Minimum 3+ years of technical experience with data warehousing in a Microsoft/Azure environment with tools such as SQL Server, Azure SQL, Azure Synapse Analytics, Azure Data Lake Storage - ADLS Gen2, Azure Streaming Analytics, DataBricks, DeltaLake, Apache Spark/SparkSQL.",
    "Experience with software and infrastructure change management, release management, and source code control required."
  ]
}