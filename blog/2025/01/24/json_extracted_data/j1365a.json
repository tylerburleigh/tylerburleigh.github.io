{
  "responsibilities": [
    "Design, build, and maintain data pipelines that efficiently move and transform data between production systems and data lakes, ensuring data integrity and availability.",
    "Optimize data storage and processing using formats like Parquet and other columnar storage solutions, ensuring that our data architecture is both performant and cost-effective.",
    "Leverage expertise in Scala Spark, PySpark, or similar technologies to write scalable data processing jobs that handle large volumes of data with high efficiency.",
    "Work with AWS cloud services to deploy, manage, and monitor data pipelines, ensuring they are resilient, secure, and scalable to meet the growing demands of the business.",
    "Collaborate with data scientists and engineers to understand data requirements, build validations from use cases, troubleshoot issues, and continuously improve data infrastructure.",
    "Help to create and cost-optimize ETLs to support custom features for ingestion into machine learning models.",
    "Provide technical leadership and mentorship to junior engineers, promoting best practices in data engineering and helping to build a culture of continuous learning and improvement.",
    "Create comprehensive documentation accessible to both technical and non-technical audiences.",
    "Automate data pipeline monitoring and alerts.",
    "Advise on the best data processing strategies to align with business objectives."
  ],
  "skills": [
    "Proficiency in Scala Spark, PySpark, or similar data processing frameworks, with a proven ability to write efficient, scalable data processing jobs.",
    "Familiarity with data storage formats like Parquet, and experience with columnar storage solutions and optimization techniques.",
    "Hands-on experience with AWS services, such as S3, Glue, EMR, Redshift, or similar tools, to build and manage large-scale data processing workflows.",
    "Strong understanding of data warehousing concepts, ETL/ELT processes, and data modeling techniques.",
    "Excellent problem-solving skills, with the ability to troubleshoot complex data pipeline issues and ensure data quality across the system.",
    "Strong communication and collaboration skills, with experience working in cross-functional teams to deliver data solutions that meet business needs.",
    "A commitment to staying up-to-date with the latest trends and best practices in data engineering, with a passion for continuous improvement and innovation."
  ],
  "qualifications": [
    "Bachelor’s or Master’s Degree in Computer Science, Engineering, or a related field.",
    "5+ years of experience in data engineering, with a strong focus on building and optimizing data pipelines in a cloud-based environment, preferably AWS."
  ]
}