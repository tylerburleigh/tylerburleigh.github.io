{
  "responsibilities": [
    "Develop scalable data pipelines and architectures, incorporating MLOps best practices for large language models (LLMs).",
    "Support data collection, analysis, content understanding, storage, and processing tasks.",
    "Write code for training, testing, and deploying machine learning models.",
    "Monitor and troubleshoot machine learning models to maintain accuracy and performance.",
    "Perform requirements analysis, collaborate with team members, and document solutions.",
    "Work with large-scale data sets and manage data flow between systems.",
    "Organize and process large batches of text and geometric data.",
    "Communicate findings through quantitative analysis, visuals, and actionable insights."
  ],
  "skills": [
    "Hands-on experience with training deep neural networks (e.g., CNNs, transformers).",
    "Proficiency with at least one deep learning framework such as PyTorch or TensorFlow.",
    "Experience with large language models (LLMs), embedding models, vector databases, and Retrieval-Augmented Generation (RAG) systems.",
    "Experience with data modeling, architecture, and processing, including handling 2D/3D geometric data.",
    "Proficiency in AWS cloud services, including SageMaker Studio, for scalable data processing and model development.",
    "Strong understanding of fundamental computer science algorithms and their scalability.",
    "Proficient in coding, covering both procedural and data-analytics-oriented languages (like Python).",
    "Ability to convert theoretical concepts into practical, prototype-ready solutions."
  ],
  "qualifications": [
    "MS in Machine Learning, Artificial Intelligence, Mathematics, Statistics, Computer Science, or a related field.",
    "5 years of experience in machine learning engineering or a related field."
  ]
}