{
  "responsibilities": [
    "Develop solutions to scale message queue and streaming infrastructure.",
    "Support data infrastructure including Kafka, Airflow, RabbitMQ, Trino, and GCP alternatives.",
    "Work with users to determine capacity and functionality requirements.",
    "Develop monitoring tools and maintain reliable operations.",
    "Partner with database teams for optimal configuration of applications.",
    "Collaborate on data migration tooling, change data capture analytics pipelines, and streaming solutions.",
    "Migrate infrastructure to the cloud using GCP."
  ],
  "skills": [
    "Expertise in developing backend services and ETL streaming.",
    "Proficiency in message processing using compiled languages.",
    "Experience with CI/CD tools such as Jenkins and GitLab.",
    "Ability to write automation scripts and libraries in the data domain."
  ],
  "qualifications": [
    "Desire to assume responsibility for infrastructure pieces and define strategies.",
    "Experience relevant to Kafka, Airflow, and GCP.",
    "5+ years of experience in the data world."
  ]
}