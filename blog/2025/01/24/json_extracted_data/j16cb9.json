{
  "responsibilities": [
    "Enhance LLM training efficiency by optimizing scripts and architectures, leveraging CUDA and advanced GPU acceleration techniques to improve performance and reduce training time.",
    "Optimize preprocessing pipelines by resolving timeout issues and implementing caching strategies.",
    "Ensure reliable distributed networking by addressing connection failures and implementing monitoring systems.",
    "Minimize downtime on rental machines through efficient recovery workflows and off-hour error mitigation.",
    "Streamline debugging of distributed systems, addressing node failures and network partitions.",
    "Develop scalable logging frameworks for efficient error detection and resolution.",
    "Optimize system scalability by balancing workloads and reducing communication overhead.",
    "Design fault-tolerant systems to handle node crashes and ensure seamless job restarts.",
    "Monitor system health and implement recovery strategies for unresponsive nodes.",
    "Manage massive data sets for large-scale model training workflows.",
    "Develop real-time monitoring frameworks for training processes to detect and alert issues promptly."
  ],
  "skills": [
    "Strong proficiency in Python and experience with distributed ML frameworks (e.g., PyTorch, TensorFlow, Horovod, or Ray).",
    "Understanding of networking protocols and distributed communication libraries (e.g., NCCL, gRPC).",
    "Hands-on experience with cloud platforms (AWS, GCP, Azure) and cluster orchestration tools (Kubernetes, Slurm).",
    "Proven ability to debug and resolve issues in large-scale distributed systems.",
    "Familiarity with fault tolerance, caching strategies, and scalable logging systems.",
    "Excellent problem-solving and communication skills."
  ],
  "qualifications": [
    "Masterâ€™s or PhD degree in Computer Science, Machine Learning, or a related field.",
    "3+ years of experience in distributed systems or machine learning infrastructure."
  ]
}