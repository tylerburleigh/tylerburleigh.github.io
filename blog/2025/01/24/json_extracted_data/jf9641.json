{
  "responsibilities": [
    "Partner with data engineers and architects to design and implement scalable database solutions and data models, organizing both structured and unstructured data.",
    "Develop processes for data cleansing, enrichment, and validation to improve quality and reliability.",
    "Collaborate with data scientists to develop machine learning models to enhance the data transformation pipelines, such as transformer-based methods to create probabilistic record matching.",
    "Monitor, solve, and optimize data pipelines for performance and efficiency.",
    "Document data architecture designs, integration procedures, and data management workflows in a clear, comprehensive manner.",
    "Continuously evaluate new cloud technologies and data processing frameworks to enhance pipeline performance.",
    "Develop automation scripts and tools to ease the deployment, scaling, and management of data systems within the cloud environment."
  ],
  "skills": [
    "Strong background in machine learning, NLP, and foundation models.",
    "Proficiency with big data processing frameworks such as Apache Spark.",
    "Experience with programming languages like Python, Scala, or SQL.",
    "Ability to write efficient, well-documented code and maintain version control in collaboration with a team.",
    "Excellent problem-solving skills and the capacity to work under tight deadlines in a fast-paced environment.",
    "Previous work in agile development environments and knowledge of project management tools such as JIRA."
  ],
  "qualifications": [
    "Bachelor's or Masterâ€™s degree in Computer Science, Engineering, or a related technical field.",
    "A minimum of 3 years of relevant experience in a data engineering role with a strong understanding of data modeling, ETL/ELT development, and data warehousing principles.",
    "Proven experience with Databricks and Azure cloud services and data solutions including but not limited to Azure SQL Data Warehouse, Azure Cosmos DB, Azure Data Lake Storage, and Azure Data Factory."
  ]
}