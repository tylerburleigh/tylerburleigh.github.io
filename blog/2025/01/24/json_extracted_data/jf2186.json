{
  "responsibilities": [
    "Apply a product-focused mindset to understand business needs and design scalable, adaptable systems.",
    "Deconstruct complex challenges, document technical solutions, and plan iterative improvements.",
    "Build and scale robust data infrastructure for batch and real-time processing of billions of records.",
    "Automate cloud infrastructure, services, and observability to enhance system efficiency and reliability.",
    "Develop CI/CD pipelines and integrate automated testing for smooth, reliable deployments.",
    "Work closely with data engineers, data scientists, product managers, and other stakeholders.",
    "Identify business challenges and opportunities using data analysis and mining.",
    "Support analytics initiatives by delivering insights into product usage, campaign performance, and revenue growth.",
    "Conduct ad-hoc analyses, manage long-term projects, and create reports and dashboards.",
    "Partner with business stakeholders to understand analytical needs and define key metrics.",
    "Collaborate with cross-functional teams to gather business requirements and deliver tailored data solutions.",
    "Deliver impactful presentations that translate complex data into clear, actionable insights."
  ],
  "skills": [
    "Product-driven development mindset",
    "Problem-solving and technical design",
    "Data infrastructure and processing",
    "Automation and cloud infrastructure",
    "CI/CD pipeline development and automated testing",
    "Cross-functional collaboration",
    "Data analysis and mining",
    "Analytics and reporting",
    "Ad-hoc analysis and dashboarding",
    "Stakeholder engagement",
    "Data storytelling and presentation"
  ],
  "qualifications": [
    "Bachelors degree in Computer Science, Engineering, or a related field, or equivalent training, fellowship, or work experience.",
    "5-7 years of industry experience in big data systems, data processing, and SQL databases.",
    "3 years of experience with Spark data frames, Spark SQL, and PySpark.",
    "3 years of hands-on experience in writing modular, maintainable code, preferably in Python and SQL.",
    "Strong proficiency in SQL, dimensional modeling, and working with analytical big data warehouses like Hive and Snowflake.",
    "Experience with ETL workflow management tools such as Airflow.",
    "2+ years of experience in building reports and dashboards using BI tools like Looker.",
    "Proficiency with version control and CI/CD tools like Git and Jenkins CI.",
    "Experience working with and analyzing data using notebook solutions such as Jupyter, EMR Notebooks, and Apache Zeppelin."
  ]
}