{
  "responsibilities": [
    "Develop and maintain scalable, secure, and high-performance data architectures to support business needs.",
    "Design, implement, and optimize complex data pipelines for real-time and batch processing using technologies such as Spark, Kafka, and cloud-based ETL tools.",
    "Implement a robust data quality framework to ensure the highest quality of data on the platform.",
    "Automate manual processes, monitor data systems, and resolve data quality issues.",
    "Collaborate with data analysts and scientists to support data initiatives and ensure consistent, optimal data delivery architecture for ongoing projects."
  ],
  "skills": [
    "Proven expertise in Spark and Databricks technologies.",
    "Experience with machine learning workflows and LLMs.",
    "Strong stakeholder management and communication skills.",
    "Strong problem-solving skills and ability to work independently in a fast-paced environment.",
    "Knowledge of data governance, security, and compliance best practices."
  ],
  "qualifications": [
    "Bachelor’s or Master’s degree in Computer Science, Information Technology, or a related field.",
    "5+ years of experience in engineering and maintaining large-scale data pipelines and distributed systems.",
    "Advanced knowledge and 3+ years of programming experience with big data processing frameworks such as Apache Spark and Kafka.",
    "5+ years of programming experience using languages such as Python, Java, C#, or Scala.",
    "5+ years of experience developing on cloud platforms and tools (e.g., AWS Glue, GCP Dataflow, Azure Data Factory).",
    "Strong SQL skills and experience with both relational and non-relational databases.",
    "Experience with version control systems and CI/CD pipelines (e.g., Git)."
  ]
}