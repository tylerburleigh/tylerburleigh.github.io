{
  "responsibilities": [
    "Own the core company data pipeline and scale up data processing flow to meet data growth at Outreach.",
    "Implement systems for tracking and monitoring data integrity, quality, and consistency.",
    "Develop frameworks and tools to support self-service data pipeline management (ETL) using big data technologies.",
    "Collaborate with Data Science to leverage models for optimization and develop new models.",
    "Work with the Data Platform team to efficiently shape data.",
    "Collaborate with Product Management and User Interface Designers to effectively surface data."
  ],
  "skills": [
    "Proficiency in Hadoop Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet).",
    "Proficiency in at least one SQL language (MySQL, PostgreSQL, SqlServer, Oracle).",
    "Strong understanding of SQL Engine and advanced performance tuning.",
    "Strong skills in scripting languages (Python, Ruby, Perl, Bash).",
    "Experience with workflow management tools (Airflow preferred).",
    "Ability to work directly with data analytics to bridge business requirements with data engineering."
  ],
  "qualifications": [
    "Extensive experience with Hadoop Ecosystem.",
    "Proficiency in SQL languages.",
    "Good understanding of SQL Engine and performance tuning.",
    "Strong scripting language skills.",
    "Experience with workflow management tools."
  ]
}