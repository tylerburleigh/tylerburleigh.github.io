{
  "responsibilities": [
    "Build and maintain data lake and big data pipelines/services.",
    "Facilitate the movement of billions of messages each day.",
    "Work with business stakeholders and platform/engineering teams to enable growth and retention strategies.",
    "Help stakeholder teams ingest data faster into the data lake.",
    "Improve data pipeline efficiency and instigate self-serve data engineering.",
    "Build micro-services and architect/design self-serve capabilities at scale.",
    "Work on AWS-based data lake using open source projects like Spark and Airflow.",
    "Identify ways to improve the platform and user experience."
  ],
  "skills": [
    "Strong programming skills in Python, Java, or Scala.",
    "Experience writing SQL and structuring data.",
    "Knowledge of data modeling and data warehousing concepts.",
    "Experience building data pipelines, platforms, micro-services, and REST APIs.",
    "Experience with Spark, Hive, Airflow, and other streaming technologies.",
    "Familiarity with modern software development practices (Agile, TDD, CICD).",
    "Strong focus on data quality and experience with tools/frameworks for detecting data issues.",
    "Experience working on Amazon Web Services (EMR, Kinesis, RDS, S3, SQS).",
    "Open-mindedness and willingness to try unconventional solutions."
  ],
  "qualifications": [
    "BS in Computer Science or equivalent experience.",
    "At least 5+ years of professional experience as a Sr. Software Engineer or Sr. Data Engineer.",
    "Experience building highly reliable services and managing a multi-petabyte scale data lake.",
    "Ability to transform vague requirements into solid solutions.",
    "Motivation to solve challenging problems with creativity and coding skills.",
    "Experience building self-service tooling and platforms (preferred).",
    "Experience with Kappa architecture platforms and Databricks (preferred).",
    "Contribution to open source projects (preferred)."
  ]
}