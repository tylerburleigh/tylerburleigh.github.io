{
  "responsibilities": [
    "Design, develop, and implement data pipelines and ETL processes using Microsoft Azure technologies.",
    "Automate manual processes and optimize data delivery.",
    "Re-design infrastructure for greater scalability.",
    "Ensure data quality through data governance and code development best practices.",
    "Assist in the design and development of data models and schemas.",
    "Work cross-functionally with stakeholders to support their data infrastructure requirements."
  ],
  "skills": [
    "Proficiency in a scripting language such as Python.",
    "Proficiency in SQL for writing complex, optimized queries.",
    "Experience with APIs, data modeling, ETL/ELT, and data warehousing.",
    "Strong problem-solving and troubleshooting skills.",
    "Strong communication skills.",
    "Ability to collaborate with and learn from multi-disciplinary teams.",
    "Interest in adapting to and learning emerging technologies."
  ],
  "qualifications": [
    "Bachelor's degree or higher in a quantitative/technical field (e.g., Computer Science, Statistics, Engineering) or equivalent experience.",
    "Experience in Apache Spark or other distributed processing systems (preferred).",
    "Experience in AWS or Azure cloud services (preferred).",
    "Microsoft Azure/Fabric Certifications (preferred)."
  ]
}