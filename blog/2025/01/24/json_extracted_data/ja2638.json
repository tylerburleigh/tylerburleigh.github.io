{
  "responsibilities": [
    "Develop event-driven distributed systems that process large amounts of data and integrate with downstream back end services",
    "Build internal tools and libraries to help accelerate other backend teams",
    "Build streaming data pipelines",
    "Work with data science and data engineering teams to build best-in-class SDLC processes",
    "Oversee the design and maintenance of data systems and contribute to the continual enhancement of the data platform",
    "Collaborate with the team to define, track, and meet SLOs",
    "Maintain and expand existing systems, tooling, and infrastructure",
    "Other duties as required"
  ],
  "skills": [
    "Strong competencies in data structures, distributed systems, algorithms, and software design",
    "Strong knowledge of Java, Java frameworks (Springboot or Quarkus), Design Patterns, and Domain Driven Design",
    "Experience with Kafka, Pub/Sub, or some other streaming platform",
    "Strong knowledge of tools like Airflow to orchestrate data pipelines",
    "Familiarity with Docker and Kubernetes",
    "Experience with at least one major cloud platform (AWS, GCP, Azure)",
    "Strong organization and collaboration skills",
    "Excellent written and oral communications skills",
    "Nice to have: Knowledge of Python, GO, Reactive programming"
  ],
  "qualifications": [
    "A solid foundation in computer science",
    "6+ years of experience in data engineering or software engineering",
    "Passionate about clean code architecture and software craftsmanship",
    "Have built distributed systems to solve complex problems over very large datasets"
  ]
}