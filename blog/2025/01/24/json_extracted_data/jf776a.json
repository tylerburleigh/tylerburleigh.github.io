{
  "responsibilities": [
    "Work with business stakeholders to understand their goals, challenges, and decisions",
    "Assist with building solutions that standardize our data approach to common problems across the company",
    "Incorporate observability and testing best practices into projects",
    "Assist in the development of processes to ensure our data is trusted and well-documented",
    "Effectively work with data analysts on refining the data model used for reporting and analytical purposes",
    "Improve availability and consistency of data points crucial for analysis"
  ],
  "skills": [
    "Demonstrated experience with Python, Docker, Kubernetes, GCP and/or AWS",
    "Experience with Relational DBs / SQL",
    "Experience with Data Warehouses and Lakes, such as Bigquery, Databricks, or Snowflake",
    "Experience in designing and building data pipelines that scale",
    "Strong communication skills, with the ability to convey technical solutions to both technical and non-technical stakeholders",
    "Experience working effectively in a fast-paced, agile environment as part of a collaborative team",
    "Ability to work independently and as part of a team",
    "Willingness and enthusiasm to learn new technologies and tackle challenging problems",
    "Pragmatism: balancing scrappiness and rigor"
  ],
  "qualifications": [
    "This role typically requires 5+ years of related experience",
    "Experience building across clouds (Nice to Have)",
    "Experience with streaming systems like Segment (Nice to Have)",
    "Knowledge of different data modeling paradigms, e.g. relational, data vault, and medallion (Nice to Have)",
    "Some experience in Infrastructure as Code tools like Terraform (Nice to Have)"
  ]
}