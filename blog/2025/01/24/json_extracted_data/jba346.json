{
  "responsibilities": [
    "Lead data engineering strategy for complex projects from beginning-to-end",
    "Design and deploy highly reliable, scalable, and timely data products",
    "Collaborate to ensure best practices in data architecture are leveraged across multiple environments (e.g., data warehouses, data lake, etc.)",
    "Effectively communicate data engineering concepts to a non-technical audience",
    "Assemble large, complex data sets that meet business requirements",
    "Identify, design, and implement internal process improvements",
    "Create data tools for analytics and Data Scientists that facilitate the building & optimizing of products",
    "Work with Data & Analytics experts to strive for greater functionality in our data systems"
  ],
  "skills": [
    "Ability to build and optimize big-data pipelines, architectures, and data sets in cloud platforms such as Azure, AWS, GCP",
    "Experience in building Data Warehouse and Data Lake environments",
    "Proficiency in utilizing Python",
    "Experience with containerization and microservices (e.g., Docker, Kubernetes)",
    "Experience with both relational SQL and NoSQL databases",
    "Experience with ETL tools such as Azure Data Factory"
  ],
  "qualifications": [
    "Bachelor's Degree or equivalent work experience",
    "3+ years of experience building and optimizing big-data pipelines, architectures, data sets in cloud platforms such as Azure, AWS, GCP",
    "3+ years of experience building Data Warehouse and Data Lake environments",
    "3+ years of experience utilizing Python"
  ]
}