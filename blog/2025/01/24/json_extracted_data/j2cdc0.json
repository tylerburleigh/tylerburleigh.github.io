{
  "responsibilities": [
    "Implement end to end data pipelines to extract and transform data from a variety of sources for multiple business initiatives.",
    "Design and implement reusable base data models in Snowflake to support multiple data applications such as analytics and marketing tools.",
    "Optimize data pipelines with a focus on data quality testing to improve data observability.",
    "Work cross functionally with other data and business functions to champion data engineering initiatives and ensure that projects are delivered on time aligned to stakeholder expectations.",
    "Contribute to data infrastructure decisions and implementations."
  ],
  "skills": [
    "Proficiency in object-oriented and functional scripting languages (Python).",
    "Ability to author queries (SQL) and familiarity with relational databases.",
    "Knowledge of data modeling techniques for data warehousing.",
    "Experience with data platforms such as Snowflake, Redshift, or BigQuery.",
    "Familiarity with data transformation tools (DBT) and containerized applications (Docker).",
    "Understanding of cloud platforms (Azure, AWS, GCP).",
    "Desire to continuously learn and implement the latest technologies and analytical tools.",
    "Open and transparent communication skills, with the ability to adjust communication for both technical and non-technical audiences."
  ],
  "qualifications": [
    "1+ years of experience building end to end ETL/ELT data pipelines.",
    "Experience working with data platforms like Snowflake, Redshift, or BigQuery.",
    "Willingness to embrace and act upon feedback.",
    "Growth mindset and care about helping people with their finances.",
    "Ability to clear a credit and criminal record check as a condition of employment."
  ]
}