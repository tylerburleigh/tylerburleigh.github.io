{
  "responsibilities": [
    "Work closely with stakeholders in engineering, product, data science, and governance to make high quality datasets available to consumers in a timely manner",
    "Develop scalable data ETL pipelines that automate manual data processes, optimize data delivery, and adhere with privacy and governance principles",
    "Implement and manage data warehousing solutions and ensure data integrity and quality through rigorous testing and validation",
    "Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate",
    "Implement and maintain data security practices to ensure data privacy and protection, and compliance with data governance policies and regulations"
  ],
  "skills": [
    "Experience in building data pipelines to serve reporting needs",
    "Experience owning all or part of a team roadmap",
    "Ability to prioritize requests from multiple stakeholders in disparate domains",
    "Ability to effectively communicate complex projects to non-technical stakeholders",
    "Strong analytical and problem-solving skills",
    "Excellent communication and teamwork abilities",
    "Attention to detail and commitment to data quality"
  ],
  "qualifications": [
    "BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field",
    "5+ years experience in SQL or similar languages",
    "5+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc)",
    "Hands-on experience with Google BigQuery, Spark, and Hadoop",
    "Experience in version control systems such as Git, and workflow management tools such as Airflow",
    "Experience in ETL tools and data architecture and warehousing experience",
    "Experience leading a small team of data or software engineers"
  ]
}