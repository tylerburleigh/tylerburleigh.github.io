{
  "responsibilities": [
    "Design and implement robust evaluation infrastructure to measure model capabilities and risks across multiple domains",
    "Lead technical projects to build and scale evaluation systems that could become industry standards",
    "Collaborate with domain experts to translate their insights into concrete evaluation frameworks",
    "Build sandboxed testing environments and automated pipelines for continuous model assessment",
    "Work closely with researchers to rapidly prototype and iterate on new evaluation approaches",
    "Partner with cross-functional teams to advance Anthropic's safety mission",
    "Contribute to Capability Reports that inform critical deployment decisions"
  ],
  "skills": [
    "Strong software engineering skills with extensive Python experience",
    "Ability to write clean, well-structured code that others can build upon",
    "Experience working with distributed systems",
    "Ability to define technical specifications and execute towards them",
    "Self-starter who thrives in fast-paced, collaborative environments",
    "Ability to balance urgency with careful, methodical implementation"
  ],
  "qualifications": [
    "Experience leading and conducting fast, iterative experiments with frontier AI models",
    "Experience designing or implementing evaluations involving sampling + prompting LLMs",
    "Understanding of AI safety concepts and concerns",
    "Background in one or more relevant domains (biosecurity, cybersecurity, and others)",
    "Experience working on sensitive or security-critical projects"
  ]
}