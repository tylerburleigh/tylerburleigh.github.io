{
  "responsibilities": [
    "Architect, deploy, and scale data storage and processing infrastructure to support analytics and data science workloads.",
    "Manage and maintain data lake and clustered computing services, ensuring reliability, security, and scalability.",
    "Build and optimize frameworks and tools to simplify big data technology usage.",
    "Collaborate with cross-functional teams to align data infrastructure with business goals and requirements.",
    "Ensure data governance and security best practices across all platforms.",
    "Monitor, troubleshoot, and optimize system performance and resource utilization."
  ],
  "skills": [
    "Proficiency in Kubernetes and Helm for container orchestration.",
    "Hands-on experience with clustered computing technologies such as Spark, Trino, Flink, Ray, Kafka, StarRocks, or similar.",
    "Programming skills in C++, C#, Java, or Python.",
    "Scripting skills in Python or Bash for automation and tooling.",
    "Strong understanding of data storage technologies, distributed computing, and big data processing pipelines.",
    "Knowledge of data security best practices and managing access in complex systems."
  ],
  "qualifications": [
    "5+ years of experience with Kubernetes and Helm, with a deep understanding of container orchestration.",
    "5+ years of programming experience in C++, C#, Java, or Python.",
    "3+ years of experience scripting in Python or Bash for automation and tooling.",
    "Experience in administering and optimizing clustered computing technologies on Kubernetes.",
    "Ability to thrive in a fast-paced, complex work environment and tackle hard problems."
  ]
}