{
  "responsibilities": [
    "Contribute to the architecture, design, and growth of Data Products and Data pipelines in Scala and Python/Spark while maintaining uptime SLAs.",
    "Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data cloud environments.",
    "Implement the Lakehouse architecture, working with key partners to shift towards a Lakehouse-centric data platform.",
    "Lead a module and be well-versed with the Business functionality to support Data questions and analysis for Business stakeholders.",
    "Understand Business Use Cases/problems and provide relevant Business & technical solutions.",
    "Collaborate with Data Product Managers, Data Architects, and Data Engineers to design, implement, and deliver successful data solutions.",
    "Maintain detailed documentation of work and changes to support data quality and data governance.",
    "Ensure high operational efficiency and quality of solutions to meet SLAs and support commitment to customers (Data Science, Data Analytics teams).",
    "Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for the team.",
    "Be a problem solver; research and network to find solutions when presented with new challenges.",
    "Seek out answers to business problems and look for opportunities to automate processes & optimize cost.",
    "Engage with and understand customers, forming relationships to understand and prioritize both innovative new offerings and incremental platform improvements."
  ],
  "skills": [
    "Strong hands-on experience with Cloud technologies like AWS (S3, EMR, EC2) and building data pipelines.",
    "Strong algorithmic problem-solving expertise.",
    "Strong SQL skills and ability to create queries to extract data, build performant datasets, and perform Data Analysis.",
    "Strong hands-on experience with distributed systems such as Spark, Pyspark to query and process data.",
    "Strong fundamental Python programming skills.",
    "Solid experience with data integration toolsets (i.e., Airflow) and writing and maintaining Data Pipelines with Databricks.",
    "Demonstrated competency in data modeling and analysis techniques.",
    "Strong attention to detail and excellent analytical skills.",
    "Verbal and written communication skills and ability to collaborate with others effectively."
  ],
  "qualifications": [
    "At least 7 years of data engineering experience developing large data pipelines.",
    "Experience with at least one major Massively Parallel Processing (MPP) or cloud database technology (Snowflake, Redshift, Big Query).",
    "Experience in Data Modeling techniques and Data Warehousing standard methodologies and practices.",
    "Experience with Software Application development.",
    "Good data acumen to understand domain/functional knowledge and articulate analysis of data points in business context for stakeholders.",
    "Demonstrated excellent interpersonal skills, communication skills including ability to partner with others and build consensus in a cross-functional team toward a desired outcome.",
    "Experience working with a test-first mentality.",
    "Deep understanding of AWS or other cloud providers as well as infrastructure as code.",
    "Familiar with Scrum and Agile methodologies.",
    "Bachelorâ€™s degree in computer science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study, and/or equivalent work experience."
  ]
}