{
  "responsibilities": [
    "Understand enterprise data and interpret it to derive business value",
    "Design and engineer data models representing key business entities",
    "Ensure data quality and integrity across data sources",
    "Convert complex business and technical rules into logic for data flows and pipelines",
    "Work with Data Science teams to implement data products for various use cases",
    "Collaborate with stakeholders and incorporate their requirements into solution design",
    "Standardize metadata into a common glossary with necessary documentation for data consumers"
  ],
  "skills": [
    "Hands-on experience with AWS technologies for data processing and analytics",
    "Experience with AWS Glue for ETL, Glue Catalog, Glue Data Quality, AWS Step Functions, AWS Lambda, AWS Athena",
    "Proficiency in AWS CLI, Identity & Access Management (IAM), and BI Tools like Tableau and Quicksight",
    "Experience in Python, Pyspark, and SQL",
    "Performance tuning of Spark jobs",
    "Experience with relational database systems, relational models, dimensional models, and analytical models",
    "Engineering data products for analytical models",
    "Building feedback loops between model deployment and its data",
    "Understanding of Data Management and Data Governance principles",
    "Data modeling and a passion for analytics",
    "Knowledge of popular data formats like Parquet, Iceberg",
    "Excellent verbal and written communication skills"
  ],
  "qualifications": [
    "Undergraduate degree in Computer Science, Mathematics, Engineering, or equivalent",
    "Graduate degree in business or quantitative science strongly preferred",
    "Canadian Reliability Security Clearance required",
    "Experience working as Product Owner in Scrum or Service Request Manager in Kanban projects",
    "Experience in a large financial services organization",
    "Knowledge of fundamental statistical models and techniques"
  ]
}