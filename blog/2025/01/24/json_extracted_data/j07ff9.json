{
  "responsibilities": [
    "Design, implement, and maintain scalable and reliable data marts and transformation processes.",
    "Collaborate with data analysts, data scientists, and business stakeholders to understand their data needs and requirements.",
    "Develop and optimize data models and schemas to ensure data is stored efficiently and can be retrieved and analyzed quickly.",
    "Implement ETL processes and frameworks to transform raw data from multiple sources into structured formats suitable for analysis.",
    "Work closely with internal Data, Architecture, and DevOps teams to ensure the reliability and scalability of data systems.",
    "Monitor and troubleshoot data pipelines, and look for opportunities for continuous improvement.",
    "Document data systems, data flow, and data processes."
  ],
  "skills": [
    "Strong proficiency in SQL and experience with database and data warehouse systems.",
    "Experience building and deploying data processes in a cloud environment (Azure, AWS, GCP).",
    "Experience in at least one programming language (e.g., Python, Java, Scala).",
    "Understanding of best practices in designing ETL/ELT processes, data modeling, and data warehousing.",
    "Good written and oral English (B2 or higher)."
  ],
  "qualifications": [
    "Bachelor's or Masterâ€™s degree in Computer Science, Information Systems, or a related field, or equivalent work experience.",
    "Experience with Databricks and Spark (preferred).",
    "Experience with Azure cloud (preferred).",
    "Experience with Postgres or Vertica (preferred).",
    "Experience with Python or Kotlin (preferred).",
    "Experience with Salesforce data (preferred).",
    "Experience building datasets for analytics and reporting tools (preferred)."
  ]
}