{
  "responsibilities": [
    "Design, build and maintain core data infrastructure pieces that support various data use cases.",
    "Enhance the data stack, lineage monitoring, and alerting to prevent incidents and improve data quality.",
    "Implement best practices for data management, storage, and security to ensure data integrity and compliance with regulations.",
    "Own the core company data pipeline, converting business needs into efficient and reliable data pipelines.",
    "Participate in code reviews to ensure code quality and share knowledge.",
    "Lead efforts to evaluate and integrate new technologies and tools to enhance data infrastructure.",
    "Define and manage evolving data models and data schemas.",
    "Manage SLA for data sets that power company metrics.",
    "Mentor junior team members, providing guidance and support in their professional development.",
    "Collaborate with data scientists, analysts, and other stakeholders to drive efficiencies in their work, supporting complex data processing, storage, and orchestration."
  ],
  "skills": [
    "Proficiency in SQL and Python, with the ability to translate complexity into efficient code.",
    "Experience with data workflow development and management tools (e.g., dbt, Airflow).",
    "Solid understanding of distributed computing principles.",
    "Experience with cloud-based data platforms such as AWS, GCP, or Azure.",
    "Strong analytical and problem-solving skills.",
    "Excellent communication and collaboration skills.",
    "Experience with data tooling, data governance, business intelligence, and data privacy is a plus."
  ],
  "qualifications": [
    "Bachelor's degree or higher in Computer Science, Engineering, or a related field.",
    "5+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure."
  ]
}