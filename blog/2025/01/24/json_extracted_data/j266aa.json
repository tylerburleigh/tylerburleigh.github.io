{
  "responsibilities": [
    "• Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.",
    "• Design complex data solutions.",
    "• Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.",
    "• Incorporate core data management competencies including data governance, data security, and data quality.",
    "• Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.",
    "• Perform data and system analysis, assessment, and resolution for complex defects and incidents and correct as appropriate.",
    "• Test data movement, transformation code, and data components.",
    "• Design and implement secure data pipelines to support LLM and GenAI applications across international markets.",
    "• Build and maintain vector database infrastructure for efficient storage and retrieval of embeddings used in RAG applications.",
    "• Develop APIs and integration layers for enterprise AI services, ensuring compliance with regional data protection requirements.",
    "• Create and optimize data streaming architectures for real-time AI applications across multiple geographic regions.",
    "• Implement monitoring and observability solutions for AI data pipelines to ensure performance and reliability across international operations.",
    "• Collaborate with international teams to establish data governance practices for AI applications across different regulatory environments.",
    "• Perform other duties as assigned."
  ],
  "skills": [
    "• Technical expertise in Large Language Models (LLMs) and Generative AI platforms (Anthropic, OpenAI).",
    "• Prompt engineering and LLM optimization techniques.",
    "• Retrieval-Augmented Generation (RAG) architectures.",
    "• Vector database implementations (Postgres, OpenSearch, Pinecone, or Weaviate).",
    "• AI Agent development and orchestration.",
    "• Enterprise API development and integration.",
    "• Responsible AI and AI safety practices.",
    "• Data security and privacy frameworks.",
    "• Cloud and AI infrastructure experience, including AWS services (especially Bedrock, Lambda, S3).",
    "• LLM deployment strategies and integration.",
    "• Model serving and scaling techniques.",
    "• Container orchestration (Kubernetes/Docker).",
    "• Infrastructure as Code (Terraform).",
    "• MLOps and monitoring practices.",
    "• Data processing and storage, including Snowflake and data warehouse technologies.",
    "• Real-time and batch processing systems.",
    "• ETL/ELT pipeline development.",
    "• Data quality and validation frameworks.",
    "• Vector embedding techniques.",
    "• Knowledge base creation and management.",
    "• Development practices, including Python, SQL, and shell scripting.",
    "• API design and development.",
    "• Git and CI/CD pipelines.",
    "• Testing and monitoring frameworks.",
    "• Documentation and technical writing.",
    "• Agile/Scrum methodologies."
  ],
  "qualifications": [
    "• Bachelor's degree in Computer Science or related field.",
    "• 3+ years of data engineering experience.",
    "• Strong cross-cultural communication skills.",
    "• Track record of supporting AI/ML initiatives.",
    "• Experience with AI governance and compliance.",
    "• Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.",
    "• Four years of data engineering or equivalent experience."
  ]
}