{
  "responsibilities": [
    "Assemble large, complex datasets and create data pipelines that fulfill both functional and non-functional business requirements.",
    "Identify and design enhancements to streamline data workflows and automate quality monitoring.",
    "Implement machine learning algorithms for AI-driven validation to detect discrepancies, anomalies, and errors.",
    "Implement CI/CD practices to enhance data ingestion and delivery processes.",
    "Build and maintain infrastructure for efficient extraction, transformation, and loading (ETL) of data from diverse sources.",
    "Collaborate closely with data, design, product, and executive teams to address technical data-related challenges.",
    "Provide technical assistance to various teams within the organization, ensuring their data infrastructure needs are met."
  ],
  "skills": [
    "Proficiency in programming languages such as Python and JavaScript.",
    "Proficiency in SQL, including PostgreSQL, MySQL, and SQL Server.",
    "Experience working with ETL tools, with a preference for FlatFile.",
    "Proficiency with Snowflake and AWS technologies.",
    "Proficiency with dbt, Apache Airflow, and Airbyte.",
    "Working experience in AI and machine learning.",
    "Familiarity with designing schema for data warehouses, including star and snowflake schema.",
    "Familiarity with big data technologies such as Hadoop, Spark, and Kafka.",
    "Strong problem-solving skills.",
    "Ability to work in an innovative, dynamic environment."
  ],
  "qualifications": [
    "Masterâ€™s degree in Computer Science, Engineering, or equivalent.",
    "6+ years of experience in data engineering roles, including building pipelines, ETL jobs, and designing schema.",
    "Experience in tech start-ups is a bonus.",
    "Experience in and around the construction/service industry is a bonus."
  ]
}