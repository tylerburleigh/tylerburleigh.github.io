{
  "filename": "j6d7fc",
  "responsibilities": [
    "Contribute to the architecture, design, and growth of Data Products and Data pipelines in Scala and Python/Spark while maintaining uptime SLAs.",
    "Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data cloud environments.",
    "Implement the Lakehouse architecture, working with key partners to shift towards a Lakehouse-centric data platform.",
    "Lead a module and be well-versed with the Business functionality to support Data questions and analysis for Business stakeholders.",
    "Understand Business Use Cases/problems and provide relevant Business & technical solutions.",
    "Collaborate with Data Product Managers, Data Architects, and Data Engineers to design, implement, and deliver successful data solutions.",
    "Maintain detailed documentation of work and changes to support data quality and data governance.",
    "Ensure high operational efficiency and quality of solutions to meet SLAs and support commitment to customers (Data Science, Data Analytics teams).",
    "Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for the team.",
    "Be a problem solver; research and network to find solutions when presented with new challenges.",
    "Seek out answers to business problems and look for opportunities to automate processes & optimize cost.",
    "Engage with and understand customers, forming relationships to understand and prioritize both innovative new offerings and incremental platform improvements."
  ],
  "qualifications": [
    "Strong hands-on experience with Cloud technologies like AWS (S3, EMR, EC2) and building data pipelines.",
    "Strong algorithmic problem-solving expertise.",
    "Strong SQL skills and ability to create queries to extract data, build performant datasets, and perform Data Analysis.",
    "Strong hands-on experience with distributed systems such as Spark, Pyspark to query and process data.",
    "Strong fundamental Python programming skills.",
    "Solid experience with data integration toolsets (i.e., Airflow) and writing and maintaining Data Pipelines with Databricks.",
    "Demonstrated competency in data modeling and analysis techniques.",
    "Strong attention to detail and excellent analytical skills.",
    "Verbal and written communication skills and ability to collaborate with others effectively."
  ],
  "analysis": "The job responsibilities and skills listed focus primarily on data engineering, data warehousing, and data pipeline development. The responsibilities include designing and developing scalable data solutions, implementing Lakehouse architecture, and collaborating with data product managers and engineers. The skills required include strong experience with cloud technologies, distributed systems like Spark, SQL, Python programming, and data integration tools.\n\nThere is no mention of working with Generative AI (GenAI) or language models (LLMs) in the responsibilities or skills. The focus is on data processing, data architecture, and data analysis rather than on natural language processing or AI model development. The job seems to be centered around data infrastructure and analytics rather than AI or language model applications.\n\nTherefore, based on the provided information, this job does not involve working with Generative AI or language models.",
  "is_genai_role": false
}