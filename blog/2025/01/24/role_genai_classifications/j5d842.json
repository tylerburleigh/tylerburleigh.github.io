{
  "filename": "j5d842",
  "responsibilities": [
    "Partnering with other solution architects, engineering, product, and business teams to understand their strategies and technical needs and help define high-value solutions.",
    "Dynamically engaging with developers, scientific researchers, and data scientists across a range of technical areas.",
    "Strategically partnering with lighthouse customers and industry-specific solution partners targeting NVIDIA's computing platform.",
    "Working closely with customers to help them adopt and build solutions using NVIDIA technology.",
    "Analyzing performance and power efficiency of deep learning inference workloads.",
    "Traveling to conferences and customers as required."
  ],
  "qualifications": [
    "Strong fundamentals in programming, optimizations, and software design, especially in Python.",
    "Strong problem-solving and debugging skills.",
    "Excellent knowledge of theory and practice of Large Language Models and Deep Learning inference.",
    "Excellent presentation, communication, and collaboration skills.",
    "Experience with NVIDIA GPUs and software libraries, such as NVIDIA NeMo Framework, NVIDIA Triton Inference Server, TensorRT, TensorRT-LLM.",
    "Excellent C/C++ programming skills, including debugging, profiling, code optimization, performance analysis, and test design.",
    "Familiarity with parallel programming and distributed computing platforms."
  ],
  "analysis": "The job involves working with NVIDIA technology and requires collaboration with developers, researchers, and data scientists. The responsibilities include analyzing performance and power efficiency of deep learning inference workloads, which suggests a focus on machine learning and AI technologies. The skills section explicitly mentions \"Excellent knowledge of theory and practice of Large Language Models and Deep Learning inference,\" indicating a direct involvement with language models. Additionally, experience with NVIDIA software libraries such as NVIDIA NeMo Framework and TensorRT-LLM further supports the involvement with language models and potentially Generative AI, as these tools are often used in developing and deploying AI models, including LLMs.",
  "is_genai_role": true
}