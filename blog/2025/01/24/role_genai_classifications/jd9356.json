{
  "filename": "jd9356",
  "responsibilities": [
    "Develop and optimize scalable, efficient data pipelines for both batch and streaming data using AWS technologies.",
    "Leverage advanced SQL skills to ensure high performance and scalability of data workflows.",
    "Collaborate with stakeholders to design and deliver data products that align with business requirements.",
    "Ensure data products are reliable, maintainable, and meet performance standards.",
    "Implement robust data governance frameworks to ensure compliance with organizational policies and standards.",
    "Establish and enforce data quality standards to maintain the accuracy, consistency, and integrity of data assets.",
    "Apply data management fundamentals to optimize data organization, storage, and retrieval processes.",
    "Write clean, reusable code in Python or similar programming languages to automate data workflows.",
    "Identify opportunities for automation to improve the efficiency and reliability of data processes.",
    "Leverage a deep understanding of data storage principles to design efficient, cost-effective storage solutions using AWS services such as S3, Redshift, and DynamoDB.",
    "Work closely with data engineers, analysts, and data scientists to support data-driven initiatives.",
    "Provide mentorship to junior engineers, promoting technical growth and knowledge sharing.",
    "Assist in creating and managing dashboards or visualizations using tools like AWS QuickSight, Tableau, or similar BI tools."
  ],
  "qualifications": [
    "Advanced SQL skills with demonstrated ability in query optimization and complex data transformations.",
    "Strong programming skills in Python or similar programming languages, with experience in ETL/ELT pipeline development.",
    "Proven experience implementing data governance frameworks and enforcing data quality standards.",
    "Solid understanding of data management fundamentals, including governance, quality frameworks, and storage optimization.",
    "Knowledge of data storage principles, such as partitioning, indexing, and data lifecycle management.",
    "Hands-on experience with Kafka for streaming data pipelines and event-driven architectures is a plus.",
    "Knowledge of machine learning workflows or data science concepts is desirable.",
    "Experience working with genomic or bioinformatics data is a bonus.",
    "Strong problem-solving and analytical skills.",
    "Excellent communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",
    "A proactive, collaborative mindset with a focus on delivering results."
  ],
  "analysis": "The job responsibilities and skills listed focus primarily on data engineering, data management, and data governance. The responsibilities include developing data pipelines, optimizing data workflows, ensuring data quality, and designing storage solutions using AWS technologies. The skills required include advanced SQL, programming in Python, data governance, and data management fundamentals. There is a mention of knowledge of machine learning workflows or data science concepts as desirable, but this is not a core requirement. There is no explicit mention of working with Generative AI (GenAI) or language models (LLMs) in the responsibilities or skills. The job seems to be centered around data engineering and management rather than AI model development or deployment.",
  "is_genai_role": false
}