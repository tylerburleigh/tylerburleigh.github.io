[
  {
    "objectID": "blog/2019/09/14/index.html",
    "href": "blog/2019/09/14/index.html",
    "title": "Data science job market analysis",
    "section": "",
    "text": "Motivating questions\n\nWhat does the data science market in Toronto look like, and how does it compare to market in some major US cities where data science is currently booming (New York City, San Francisco, Seattle, and Boston)? I haven‚Äôt come across an analysis of the Canadian data science job market, and personally I‚Äôd like to move to Ontario again in the future, so understanding this market would be useful.\nWhat skills are in highest demand at different levels of seniority? I haven‚Äôt seen an analysis that takes seniority into account, which could be masking some interesting patterns. Most of the analyses I‚Äôve seen are aimed at people trying to ‚Äúbreak into‚Äù the data science profession. What about people who are growing into their careers?\nHow many remote data science jobs are there and what do these jobs look like?\n\n\nMethods\nThe data was from a scrape of LinkedIn job ads in September 2019. I scraped job ads for ‚Äúdata scientist‚Äù and ‚Äúdata analyst‚Äù positions at the levels of Entry, Associate, and Mid-senior in New York City, San Francisco, Boston, Seattle, and Toronto. For each city I targeted the ‚Äúgreater metro area‚Äù ‚Äì in NYC, for example, this included some cities in NJ just on the other side of the Hudson river. I analyzed the ‚Äúdata scientist‚Äù and ‚Äúdata analyst‚Äù positions separately.\n\n\n\nGetting the raw data\nFirst I‚Äôll scrape the data from LinkedIn. I wrote a package to do this called LinkedInJobsScrapeR.\n\n‚û°\n\n\nScraping parameters\nTo scrape the data I‚Äôll need to define the parameters for scraping.\nI‚Äôll limit myself to Data Scientist and Data Analyst positions in major cities that I think have a lot of tech jobs. These are also cities that I would consider moving to if given the right opportunity. I‚Äôll also restrict the search to ‚Äúseniority levels‚Äù 2, 3, and 4 ‚Äì these correspond to Entry level, Associate, and Mid-senior levels on LinkedIn.\njob_titles &lt;- c('data scientist', 'data analyst')\nlocations &lt;- list(\n    c('New York City Metropolitan Area', 'NYC'),\n    c('San Francisco Bay Area', 'SF'),\n    c('Greater Boston', 'BOS'),\n    c('Greater Seattle Area', 'SEA'),\n    c('Greater Toronto Area Metropolitan Area', 'TO')\n)\nexperience_levels &lt;- c(2, 3, 4)\n\n\nScraping\nI‚Äôll write a loop to iterate through all of the parameters defined above and scrape the data using my package. This loop will iterate over the list of locations, the list of experience levels, and the list of job titles.\n# For the jobs to scrape, loop through all of the...\n#   i = locations\n#   k = experience levels\n#   j = job titles\nfor(i in 1:length(locations)){\n  for(k in 1:length(experience_levels)){\n    for (j in 1:length(job_titles)){\n      \n      # Print the job so I can monitor\n      # the task as it does its thing\n      print(paste0(\"CURRENT JOB: \", \n                    job_titles[j], \": \", \n                    experience_levels[k], \": \", \n                    locations[[i]][1]))\n      \n      # I don't want a space in the directory name\n      job_title_no_space &lt;- gsub(\"\\\\s\", \"\", job_titles[j])\n      \n      # Check if files exist in the directory\n      # and skip if they do. This is helpful\n      # in case I need to restart the scrape job\n      # (which I did)\n      files &lt;- list.files(paste0('data/',\n                                job_title_no_space, '/',\n                                experience_levels[k], '/',\n                                locations[[i]][2]))\n      if(length(files) &gt; 0) next\n      \n      LinkedInJobsScrapeR::scrape_job(locations_index = i,\n                                       experience_level_index = k,\n                                       job_titles_index = j)  \n    }\n  }\n}\n\n\n\nData wrangling\nOK I now have about 11500 files scraped. Let‚Äôs extract and clean the data! First I‚Äôll extract what I‚Äôm calling the job metadata. Things like location, job title, company. It‚Äôs not the contents of the job ads, but it helps to describe or contextualize the job ads. Then I‚Äôll extract the contents of the job ads themselves ‚Äì the job description, type of position (FT / PT), and industry.\n\n‚û°\n\n\nJob ad metadata\nThere‚Äôs a lot of redundancy in the scraped files, because each file contains HTML about all of the other ads in the search results. A consequence of this is that I only need to look at a single file in each directory to extract the basic metadata.\n# Generate a list of files for metadata extraction\n# we only need 1 file per job search results page\n# so we will take the first one for each location folder\nfiles_for_metadata &lt;- c()\nfor(i in 1:length(locations)){\n  for(k in 1:length(experience_levels)){\n    for (j in 1:length(job_titles)){\n      \n      job_title_no_space &lt;- gsub(\"\\\\s\", \"\", job_titles[j])\n      \n      file &lt;- list.files(paste0('data/',\n                                job_title_no_space, '/',\n                                experience_levels[k], '/',\n                                locations[[i]][2]),\n                         full.names = T)[[1]]\n      \n      files_for_metadata &lt;- c(files_for_metadata, file)\n    }\n  }\n}\n\nmetadata &lt;- data.frame()\nfor(i in 1:length(files_for_metadata)){\n  m &lt;- LinkedInJobsScrapeR::get_job_ad_metadata(files_for_metadata[i])\n  m %&lt;&gt;% mutate(location_abbr = str_split(files_for_metadata[i], \"/\")[[1]][4],\n                position = str_split(files_for_metadata[i], \"/\")[[1]][2])\n  metadata &lt;- rbind(metadata, m)\n}\n\n\nContents of job ads\nNow I‚Äôll extract the contents of the job ads: the job description and other criteria listed with the job ad, such as employment type and industry. This time I‚Äôll need to look at every individual file. There‚Äôs a lot to read/write, so this operation takes a while.\njob_ads &lt;- list.files(\"data\", recursive = T, full.names = T)\ndescriptions &lt;- data.frame()\ncriteria &lt;- data.frame()\nfor(i in 1:length(job_ads)){\n  details &lt;- get_job_description(job_ads[i])\n  descriptions &lt;- rbind(descriptions, details$description)\n  criteria &lt;- rbind(criteria, details$criteria)\n}\nI‚Äôve already gone ahead and cached the results, so I‚Äôll save some time and load them from the CSVs. :)\nzip_file &lt;- paste0(here::here(), \"/linkedin_jobs_data.zip\")\n\nmetadata &lt;- data.table::fread(unzip(zip_file, \"metadata.csv\"))\ndescriptions &lt;- data.table::fread(unzip(zip_file, \"descriptions.csv\"))\ncriteria &lt;- data.table::fread(unzip(zip_file, \"criteria.csv\"))\n\nfile.remove(\"metadata.csv\", \"descriptions.csv\", \"criteria.csv\")\n## [1] TRUE TRUE TRUE\n\n\nJoin, rename, reorganize\nNext it looks like some jobs were cross-posted from one geo-location to another, so I‚Äôll deduplicate the dataframes. I can dedupe using the job_id variable (LinkedIn‚Äôs own job identification tokens). I‚Äôll also do some other data wrangling here.\nOne of the choices that I‚Äôm making here is to exclude Data Engineer positions. I do this by excluding jobs where ‚ÄúEngineer‚Äù is in the job title, unless ‚ÄúScientist‚Äù is also in the job title.\nmetadata %&gt;%\n  filter(job_id %in% descriptions$job_id,\n         !grepl(\"Engineer\", title, ignore.case = T) | \n          grepl(\"Engineer\", title, ignore.case = T) & grepl(\"Scientist\", title, ignore.case = T)) %&gt;%\n  distinct(job_id, location, .keep_all = T) -&gt; metadata\n\ncriteria %&gt;%\n  filter(job_id %in% metadata$job_id) %&gt;%\n  distinct(job_id, name, content, .keep_all = T) -&gt; criteria\n\n# We'll create a new dataframe representing the job seniority levels\n#   and then join it to the descriptions dataframe\ncriteria %&gt;%\n  mutate(job_id = as.character(job_id)) %&gt;%\n  filter(name == \"Seniority level\") %&gt;%\n  select(level = content, job_id) -&gt; levels\n\n# We'll create a list of the job positions and locations for filtering later\nmetadata %&gt;%\n  mutate(job_id = as.character(job_id)) %&gt;%\n  select(job_id, location_abbr, position) -&gt; select_metadata\n\ndescriptions %&gt;%\n  filter(job_id %in% metadata$job_id) %&gt;%\n  distinct(job_id, .keep_all = T) %&gt;%\n  left_join(levels) %&gt;%\n  left_join(select_metadata) -&gt; descriptions\n## Joining, by = \"job_id\"\n## Joining, by = \"job_id\"\n\n\n\nFeatures\nWhat are the skills that employers are looking for in Data Scientist and Data Analyst roles?\nTo answer this question, I‚Äôll need to do some feature engineering. I‚Äôll use regular expressions (RegEx) and then search within the job description texts for those strings. I‚Äôll try to determine what programming languages, degrees, disciplines, and other skills employers are looking for. (Presumably if these appear in the ad it‚Äôs because the employer is looking for these things in a candidate).\nI‚Äôll also put here any ‚Äúhelper functions‚Äù that I write to use later.\n\n‚û°\n\n\nDefining the features\nThis was fun, but also tricky. I want to search for ‚ÄúR‚Äù to see how many companies are looking for people with knowledge of R programming, but ‚ÄúR‚Äù is just a single capital letter! What about ‚ÄúRedmond, WA‚Äù or ‚ÄúR&D‚Äù? When writing RegEx, it‚Äôs important to consider edge cases and false positives and build the regular expression with those in mind.\nR_regex &lt;- \"(?&lt;![:alnum:])R(?![:alnum:]|&)\"\nSAS_regex &lt;- \"(?&lt;![:alnum:])SAS(?![:alnum:])\"\nExcel_regex &lt;- \"(?&lt;![:alnum:])Excel(?![:alnum:])\"\nSPSS_regex &lt;- \"(?&lt;![:alnum:])SPSS(?![:alnum:])\"\nml_regex &lt;- \"(?&lt;![:alnum:])Machine Learning|ML|machine learning|AI(?![:alnum:])\"\ngit_regex &lt;- \"(?&lt;![:alnum:])git|Git(?![:alnum:])\"\nma_regex &lt;- \"(?&lt;![:alnum:])Master's|Masters|MA|M.A.|MPH|M.P.H.(?![:alnum:])\"\nphd_regex &lt;- \"(?&lt;![:alnum:])PHD|PhD|Ph.D.|Doctorate|Doctor(?![:alnum:])\"\nba_regex &lt;- \"(?&lt;![:alnum:])BS|B.S.|B.A.|Bachelors|Bachelor's|BA(?![:alnum:])\"\nss_regex &lt;- \"Psychology|Social Science|Political Science|Behavioral Science|Behavior Science|Behavioural Science\"\nts_regex &lt;- 'temporal|time series|timeseries|longitudinal'\nspatial_regex &lt;- 'Spatial|spatial|GIS|GRASS|IDRISI|FME'\n\ndescriptions %&gt;%\n  mutate(\n         # Languages\n         python = str_detect(description, regex(\"python\", ignore_case = T)),\n         R = str_detect(description, regex(R_regex, ignore_case = F)),\n         SPSS = str_detect(description, regex(SPSS_regex, ignore_case = F)),\n         SAS = str_detect(description, regex(SAS_regex, ignore_case = F)),\n         Tableau = str_detect(description, regex('Tableau', ignore_case = T)),\n         SQL = str_detect(description, regex('SQL', ignore_case = F)),\n         Matlab = str_detect(description, regex('Matlab', ignore_case = T)),\n         Spark = str_detect(description, regex('Spark', ignore_case = F)),\n         Hive = str_detect(description, regex('Hive|Hadoop|HQL', ignore_case = F)),\n         JS = str_detect(description, regex('javascript', ignore_case = T)),\n         Excel = str_detect(description, regex(Excel_regex, ignore_case = F)),\n         Stata = str_detect(description, regex('stata', ignore_case = T)),\n         \n         # Skills\n         stats = str_detect(description, regex('statistics|statistical', ignore_case = T)),\n         regression = str_detect(description, regex('regression', ignore_case = F)),\n         experiments = str_detect(description, regex('experiments', ignore_case = F)),\n         ml = str_detect(description, regex(ml_regex, ignore_case = F)),\n         nlp = str_detect(description, regex('NLP|natural language', ignore_case = T)),\n         ts = str_detect(description, regex(ts_regex, ignore_case = T)),\n         git = str_detect(description, regex(git_regex, ignore_case = T)),\n         viz = str_detect(description, regex('visuali|viz', ignore_case = T)),\n         unstruct_data = str_detect(description, regex(' unstructured data', ignore_case = T)),\n         big_data = str_detect(description, regex('large data|big data', ignore_case = T)),\n         spatial = str_detect(description, regex(spatial_regex, ignore_case = F)),\n         \n         # Degrees\n         ba = str_detect(description, regex(ba_regex, ignore_case = T)),\n         ma = str_detect(description, regex(ma_regex, ignore_case = T)),\n         phd = str_detect(description, regex(phd_regex, ignore_case = T)),\n         cs = str_detect(description, regex(\"CompSci|Comp. Sci|Comp Sci|Computer Science\", ignore_case = T)),\n         ss = str_detect(description, regex(ss_regex, ignore_case = T)),\n         math = str_detect(description, regex(\"Mathematics|Math \", ignore_case = T)),\n         neuro = str_detect(description, regex(\"Neuroscience\", ignore_case = T)),\n         physics = str_detect(description, regex(\"Physics\", ignore_case = T)),\n         econ = str_detect(description, regex(\"Economics\", ignore_case = T)),\n         pubhealth = str_detect(description, regex(\"Public Health\", ignore_case = T)),\n         bioinfo = str_detect(description, regex(\"Bioinformatics\", ignore_case = T)),\n         \n         # Other\n         remote = str_detect(description, regex('remote', ignore_case = T)),\n         \n  ) -&gt; descriptions\n\n\nHelper functions\nI‚Äôll write some functions for operations that get repeated later.\nOne of these is generating barplots from tabulated data.\n\ngenerate_barplot()\n# Function for plotting the results of tabulated data\ngenerate_barplot &lt;- function(tbl, position, content, n, location_set) {\n  # Long to wide\n  tbl %&gt;%\n    select(-total) %&gt;%\n    melt(., id.vars = c(\"level\")) -&gt; tbl_l\n  \n  # Relevel factor so that the ordering makes sense\n  tbl_l$level &lt;- factor(tbl_l$level, levels = c(\"Entry level\", \"Associate\", \"Mid-Senior level\"))\n  \n  # Use this when identifying location set in analysis\n  location_sets &lt;- c(\"NYC, SF, Seattle, Boston, and Toronto\", \n                     \"Toronto\",\n                     \"NYC, SF, Seattle, Boston, and Toronto that had a remote option\")\n  \n  plot_caption &lt;- paste0(\"Jobs were scraped from LinkedIn in Sept 2019\\nLocations included \", location_sets[location_set], \"\\nPositions were Entry, Associate, and Senior levels\\nsource: https://tylerburleigh.com/YADSJMA\")\n  \n  plot_title &lt;- paste0(content, ' in \"', position, '\" job ads (N = ', n, ')')\n  \n  # Graph\n  ggplot(data = tbl_l, aes(x = reorder(variable, value), y = value, fill = level)) + \n    geom_bar(position=\"dodge\", stat=\"identity\") +\n    scale_y_continuous(breaks = seq(0, 100, 5), limits = c(0, 100), expand = c(0, 0, 0, 0)) + \n    ylab(\"percent of jobs\") + \n    xlab(\"\") + \n    coord_flip() + \n    theme_minimal() +\n    scale_fill_manual(\"legend\", values = c(\"Entry level\" = \"#ffeda0\", \n                                           \"Associate\" = \"#feb24c\", \n                                           \"Mid-Senior level\" = \"#f03b20\")) +\n    labs(title = plot_title, caption = plot_caption) -&gt; plot\n  \n  print(plot)\n}\n\n\nsummarize_tools()\nSummarize tools and programming languages.\nsummarize_tools &lt;- function(data) {\n  data %&gt;%\n    group_by(level) %&gt;%\n    summarize(total = n(),\n              SQL = round(sum(SQL)/total*100),\n              Python = round(sum(python)/total*100),\n              R = round(sum(R)/total*100),\n              Spark = round(sum(Spark)/total*100),\n              Hive = round(sum(Hive)/total*100),\n              Tableau = round(sum(Tableau)/total*100),\n              SAS = round(sum(SAS)/total*100),\n              JavaScript = round(sum(JS)/total*100),\n              SPSS = round(sum(SPSS)/total*100),\n              Matlab = round(sum(Matlab)/total*100),\n              Excel = round(sum(Excel)/total*100),\n              Stata = round(sum(Stata)/total*100),\n              )\n}\n\n\nsummarize_other_skills()\nSummarize other skills like ‚Äústatistics‚Äù or ‚Äúmachine learning‚Äù.\nsummarize_other_skills &lt;- function(data) {\n  data %&gt;%\n    group_by(level) %&gt;%\n    summarize(total = n(),\n              Statistics = round(sum(stats)/total*100),\n              `Machine Learning` = round(sum(ml)/total*100),\n              NLP = round(sum(nlp)/total*100),\n              Git = round(sum(git)/total*100),\n              Regression = round(sum(regression)/total*100),\n              `Time Series` = round(sum(ts)/total*100),\n              Visualization = round(sum(viz)/total*100),\n              `Big Datasets` = round(sum(big_data)/total*100),\n              `Unstructured Data` = round(sum(unstruct_data)/total*100),\n              `GIS/Spatial` = round(sum(spatial)/total*100)\n              )\n}\n\n\nsummarize_degrees()\nSummarize degrees.\nsummarize_degrees &lt;- function(data) {\n  data %&gt;%\n    group_by(level) %&gt;%\n    summarize(total = n(),\n              Bachelor = round(sum(ba)/total*100),\n              Master = round(sum(ma)/total*100),\n              PhD = round(sum(phd)/total*100)\n              )\n}\n\n\nsummarize_disciplines()\nSummarize disciplines.\nsummarize_disciplines &lt;- function(data) {\n  data %&gt;%\n    group_by(level) %&gt;%\n    summarize(total = n(),\n              `Computer Science` = round(sum(cs)/total*100),\n              `Social Science` = round(sum(ss)/total*100),\n              Neuroscience = round(sum(phd)/total*100),\n              `Public Health` = round(sum(pubhealth)/total*100),\n              Physics = round(sum(physics)/total*100),\n              Economics = round(sum(econ)/total*100),\n              Bioinformatics = round(sum(bioinfo)/total*100)\n              )\n}\n\n\n\n\nData Scientist Jobs\n\nAll locations\n\nAll locations\n4893 data scientist job ads were included in this analysis.\n\nTools / languages\n\nCode for the above graph:\ndescriptions %&gt;%\n  filter(position == ds_filter) %&gt;%\n  summarize_tools() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Tools and languages', n_ads, 1)\n\n\nOther skills\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter) %&gt;%\n  summarize_other_skills() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Other skills', n_ads, 1)\n\n\nDegrees\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter) %&gt;%\n  summarize_degrees() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Degrees', n_ads, 1)\n\n\nDisciplines\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter) %&gt;%\n  summarize_disciplines() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Disciplines', n_ads, 1)\n\n\n\n\nToronto\n\nToronto\n292 data scientist job ads were included in this analysis.\n\nTools / languages\n\nCode for the above graph:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_tools() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Tools and languages', n_ads, 2)\n\n\nOther skills\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_other_skills() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Other skills', n_ads, 2)\n\n\nDegrees\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_degrees() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Degrees', n_ads, 2)\n\n\nDisciplines\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_disciplines() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Disciplines', n_ads, 2)\n\n\n\n\nRemote option\n\nRemote option\n158 data scientist job ads were included in this analysis.\n\nTools / languages\n\nCode for the above graph:\ndescriptions %&gt;%\n  filter(position == ds_filter, remote) %&gt;%\n  summarize_tools() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Tools and languages', n_ads, 3)\n\n\nOther skills\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, remote) %&gt;%\n  summarize_other_skills() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Other skills', n_ads, 3)\n\n\nDegrees\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, remote) %&gt;%\n  summarize_degrees() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Degrees', n_ads, 3)\n\n\nDisciplines\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, remote) %&gt;%\n  summarize_disciplines() %&gt;%\n  generate_barplot(., \"Data Scientist\", 'Disciplines', n_ads, 3)\n\n\n\n\n\nData Analyst jobs\n\nAll locations\n\nAll locations\n3340 data analyst job ads were included in this analysis.\n\nTools / languages\n\nCode for the above graph:\ndescriptions %&gt;%\n  filter(position == da_filter) %&gt;%\n  summarize_tools() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Tools and languages', n_ads, 1)\n\n\nOther skills\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == da_filter) %&gt;%\n  summarize_other_skills() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Other skills', n_ads, 1)\n\n\nDegrees\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == da_filter) %&gt;%\n  summarize_degrees() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Degrees', n_ads, 1)\n\n\nDisciplines\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == da_filter) %&gt;%\n  summarize_disciplines() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Disciplines', n_ads, 1)\n\n\n\n\nToronto\n\nToronto\n309 data analyst job ads were included in this analysis.\n\nTools / languages\n\nCode for the above graph:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_tools() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Tools and languages', n_ads, 2)\n\n\nOther skills\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_other_skills() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Other skills', n_ads, 2)\n\n\nDegrees\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_degrees() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Degrees', n_ads, 2)\n\n\nDisciplines\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == ds_filter, location_abbr == \"TO\") %&gt;%\n  summarize_disciplines() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Disciplines', n_ads, 2)\n\n\n\n\nRemote option\n\nAll locations\n113 data analyst job ads were included in this analysis.\n\nTools / languages\n\nCode for the above graph:\ndescriptions %&gt;%\n  filter(position == da_filter, remote) %&gt;%\n  summarize_tools() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Tools and languages', n_ads, 1)\n\n\nOther skills\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == da_filter, remote) %&gt;%\n  summarize_other_skills() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Other skills', n_ads, 1)\n\n\nDegrees\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == da_filter, remote) %&gt;%\n  summarize_degrees() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Degrees', n_ads, 1)\n\n\nDisciplines\n\nCode for the graph above:\ndescriptions %&gt;%\n  filter(position == da_filter, remote) %&gt;%\n  summarize_disciplines() %&gt;%\n  generate_barplot(., \"Data Analyst\", 'Disciplines', n_ads, 1)\n\n\n\n\n\nKey results\nThis is what I see when I look at the data, approaching it from a career development / job opportunity frame.\n\nData scientist jobs\nA data scientist looks like someone who:\n\nKnows SQL and Python or R; preferably one of the Big Data frameworks like Hive or Spark\nHas a BA or MA ‚Äì but preferably a PhD ‚Äì in Computer Science or Neuroscience\nKnows statistics / machine learning, how to work with large datasets, and data visualization; NLP wouldn‚Äôt hurt\n\nOther observations:\n\nStatistics and machine learning are more important at more senior levels\nPython and R are more important at more senior levels\nBigger Python-R gap in Toronto than in US cities\n\n\n\nData analyst jobs\nA data analyst looks like someone who:\n\nKnows Excel, SQL; preferably knows Tableau and R or Python\nPreferably knows statistics / machine learning, visualization, and how to work with large datasets\nHas a BA ‚Äì but preferably an MA ‚Äì in Computer Science, Economics, or Social Science\n\nOther observations:\n\nLess technical than data scientist jobs across the board\nRequire fewer programming language tools and more of the ‚Äúeasier‚Äù tools like Excel\nLess focused on statistics and more focused on data visualization than data scientist jobs\nEntry level looks great for people coming from the Social Sciences\nPhD isn‚Äôt needed"
  },
  {
    "objectID": "blog/2019/09/27/index.html",
    "href": "blog/2019/09/27/index.html",
    "title": "Predicting t-shirt size from height and weight",
    "section": "",
    "text": "Today I was given a task that sounded pretty straight-forward: What t-shirt size would you send to someone if you don‚Äôt know their shirt size, but instead you know their height, weight, and gender?\nIn fact, it seemed so straight-forward that I was sure there must be prior art out there that I could re-use. A StackOverflow, a mathematical formula, a GitHub repo, a blog post ‚Äì there had to be something! To my surprise, there wasn‚Äôt any, not that I could find anyway.\nI guess this is a problem that hasn‚Äôt received a lot of attention. Or at least, it‚Äôs not the sort of problem that someone in the open source / open science community has tackled.\nSo I set out to build my own predictive algorithm."
  },
  {
    "objectID": "blog/2019/09/27/index.html#read-in-the-data",
    "href": "blog/2019/09/27/index.html#read-in-the-data",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Read in the data",
    "text": "Read in the data\nOf course the data had to be in a SAS format. üôÑ\nLuckily there‚Äôs an R package for reading SAS data files.\n\nlibrary(foreign)\nlibrary(tidyverse)\nlibrary(MASS)\n\na &lt;- read.xport(\"ARX_F.XPT\") # Arthritis data\nb &lt;- read.xport(\"BMX_F.XPT\") # Body measurements\nd &lt;- read.xport(\"DEMO_F.XPT\") # Demographics\n\nThese are the variables I‚Äôll pull out from the different dataframes.\n\nSEQN - Participant ID\nARXCCIN - Inhale chest circumference in CM\nBMXWT - Weight in KG\nBMXHT - Height in CM\nBMXBMI - Body Mass Index (BMI)\nDMDHRGND - Gender of participant (1 = Male, 2 = Female)\n\nI‚Äôm using the inhale chest circumference because I found in some exploratory analyses (not reported here) that it has a stronger correlation to the other measurements. I guess the exhale circumference was more noisy for some reason?\n\na %&gt;% dplyr::select(id = SEQN, chest_in_cm = ARXCCIN) -&gt; chest_measures\nb %&gt;% dplyr::select(id = SEQN, weight_kg = BMXWT, height_cm = BMXHT, bmi = BMXBMI) -&gt; height_weight\nd %&gt;% dplyr::select(id = SEQN, gender = DMDHRGND) %&gt;%\n  mutate(gender = case_when(gender == 1 ~ \"M\", TRUE ~ \"F\")) -&gt; gender\n\n# Join datasets and select only the rows that have all measurements\nchest_measures %&gt;%\n  left_join(., height_weight, by = c('id')) %&gt;%\n  left_join(., gender, by = c('id')) %&gt;%\n  filter(!is.na(chest_in_cm),\n         !is.na(height_cm), \n         !is.na(weight_kg),\n         !is.na(gender), \n         !is.na(bmi)) -&gt; df"
  },
  {
    "objectID": "blog/2019/09/27/index.html#height-and-weight-model",
    "href": "blog/2019/09/27/index.html#height-and-weight-model",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Height and weight model",
    "text": "Height and weight model\nTo build a model, I‚Äôll run a stepwise linear regression to determine the model of best fit. I‚Äôll enter height, weight, and gender as predictors and allow interaction terms. I‚Äôll use AIC (Akaike‚Äôs Information Criterion) for model selection, since it penalizes models with added complexity and I want a parsimonious model that doesn‚Äôt overfit the data.\nI see that the best model includes all terms, including weight*height and weight*gender interaction terms.\n\nlm(data = subset(df, select= c(chest_in_cm, height_cm, weight_kg, gender)), \n   chest_in_cm ~ .) -&gt; mod\nstep.model &lt;- stepAIC(mod, direction = \"both\", trace = TRUE, scope = . ~ .^2)\n\nStart:  AIC=14857.43\nchest_in_cm ~ height_cm + weight_kg + gender\n\n                      Df Sum of Sq    RSS   AIC\n+ height_cm:weight_kg  1       220 115291 14851\n- height_cm            1         2 115513 14856\n+ weight_kg:gender     1        89 115421 14856\n&lt;none&gt;                             115510 14857\n+ height_cm:gender     1         7 115503 14859\n- gender               1      2052 117563 14937\n- weight_kg            1    397116 512627 21725\n\nStep:  AIC=14850.64\nchest_in_cm ~ height_cm + weight_kg + gender + height_cm:weight_kg\n\n                      Df Sum of Sq    RSS   AIC\n+ weight_kg:gender     1    144.02 115147 14847\n&lt;none&gt;                             115291 14851\n+ height_cm:gender     1     13.37 115277 14852\n- height_cm:weight_kg  1    219.87 115510 14857\n- gender               1   2064.63 117355 14930\n\nStep:  AIC=14846.88\nchest_in_cm ~ height_cm + weight_kg + gender + height_cm:weight_kg + \n    weight_kg:gender\n\n                      Df Sum of Sq    RSS   AIC\n&lt;none&gt;                             115147 14847\n+ height_cm:gender     1     3.499 115143 14849\n- weight_kg:gender     1   144.020 115291 14851\n- height_cm:weight_kg  1   274.743 115421 14856\n\nsummary(step.model)\n\n\nCall:\nlm(formula = chest_in_cm ~ height_cm + weight_kg + gender + height_cm:weight_kg + \n    weight_kg:gender, data = subset(df, select = c(chest_in_cm, \n    height_cm, weight_kg, gender)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.3179  -3.2347   0.2087   3.4462  15.4911 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         41.491415   4.708485   8.812  &lt; 2e-16 ***\nheight_cm            0.086786   0.028263   3.071 0.002148 ** \nweight_kg            0.666925   0.055507  12.015  &lt; 2e-16 ***\ngenderM             -0.030639   0.599462  -0.051 0.959239    \nheight_cm:weight_kg -0.001087   0.000328  -3.314 0.000925 ***\nweight_kg:genderM    0.016955   0.007065   2.400 0.016449 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.001 on 4604 degrees of freedom\nMultiple R-squared:  0.8152,    Adjusted R-squared:  0.815 \nF-statistic:  4061 on 5 and 4604 DF,  p-value: &lt; 2.2e-16\n\n# Save the model for later\nheight_weight_model &lt;- step.model"
  },
  {
    "objectID": "blog/2019/09/27/index.html#body-mass-index-bmi-model",
    "href": "blog/2019/09/27/index.html#body-mass-index-bmi-model",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Body Mass Index (BMI) model",
    "text": "Body Mass Index (BMI) model\nNow I‚Äôll do the same with BMI and gender.\n\nlm(data = subset(df, select= c(chest_in_cm, bmi, gender)), chest_in_cm ~ .) -&gt; mod\nstep.model &lt;- stepAIC(mod, direction = \"both\", trace = TRUE, scope = . ~ .^2)\n\nStart:  AIC=17986.13\nchest_in_cm ~ bmi + gender\n\n             Df Sum of Sq    RSS   AIC\n+ bmi:gender  1       725 227076 17973\n&lt;none&gt;                    227801 17986\n- gender      1     11229 239030 18206\n- bmi         1    385144 612946 22547\n\nStep:  AIC=17973.44\nchest_in_cm ~ bmi + gender + bmi:gender\n\n             Df Sum of Sq    RSS   AIC\n&lt;none&gt;                    227076 17973\n- bmi:gender  1    724.73 227801 17986\n\nsummary(step.model)\n\n\nCall:\nlm(formula = chest_in_cm ~ bmi + gender + bmi:gender, data = subset(df, \n    select = c(chest_in_cm, bmi, gender)))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.7682  -5.0630   0.2613   5.2463  23.2969 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 56.77733    0.66042  85.971  &lt; 2e-16 ***\nbmi          1.31235    0.02203  59.573  &lt; 2e-16 ***\ngenderM     -0.34122    0.92789  -0.368 0.713084    \nbmi:genderM  0.11906    0.03105   3.834 0.000128 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.021 on 4606 degrees of freedom\nMultiple R-squared:  0.6355,    Adjusted R-squared:  0.6353 \nF-statistic:  2677 on 3 and 4606 DF,  p-value: &lt; 2.2e-16\n\n# Save the model for later\nbmi_model &lt;- step.model"
  },
  {
    "objectID": "blog/2019/09/27/index.html#predict-chest-size-given-height-weight-and-gender",
    "href": "blog/2019/09/27/index.html#predict-chest-size-given-height-weight-and-gender",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Predict chest size given height, weight, and gender",
    "text": "Predict chest size given height, weight, and gender\nNext we‚Äôll take the final model selected from the above procedure and use it to predict chest circumference, in inches, given height, weight, and gender.\nTo test the model, I‚Äôll use average values.\n\nmean(df$weight_kg)\n\n[1] 82.48026\n\nmean(df$height_cm)\n\n[1] 168.0299\n\nmean(df$bmi)\n\n[1] 29.12334\n\n\nHeight, weight, and gender\n\ninput &lt;- data.frame(height_cm = 168, weight_kg = 83, gender = \"F\")\n\n# 1 cm = 0.393701 inches\npredict(height_weight_model, input) * 0.393701\n\n       1 \n37.90109 \n\n\nBMI and gender\n\ninput &lt;- data.frame(bmi = 29, gender = \"F\")\npredict(bmi_model, input) * 0.393701\n\n       1 \n37.33684"
  },
  {
    "objectID": "blog/2019/09/27/index.html#chest-size-to-shirt-size",
    "href": "blog/2019/09/27/index.html#chest-size-to-shirt-size",
    "title": "Predicting t-shirt size from height and weight",
    "section": "Chest size to shirt size",
    "text": "Chest size to shirt size\nFinally, I‚Äôll need to take the chest size prediction and convert it to a shirt size. Remember the size chart?\n\nFor some reason the chart leaves out ‚ÄúXS‚Äù and the ranges lso don‚Äôt provide full coverage of the possible chest size values (34-36‚Äù and then 38-40‚Äù???). So I‚Äôll extend the upper bound of each range to meet the lower bound of each size above it. This will cause the predictions to err on the size of larger sizes rather than smaller sizes, which I think is better because if a shirt is too big, at least you can still wear it!\n\ninput &lt;- data.frame(height_cm = 168, weight_kg = 83, gender = \"F\")\n\ndata.frame(chest = predict(height_weight_model,input)[[1]] * 0.393701) %&gt;%\n  mutate(shirt_size = case_when(\n          chest &lt; 32 ~ \"XS\",\n          between(chest, 32, 36) ~ \"S\",\n          between(chest, 36, 40) ~ \"M\",\n          between(chest, 40, 44) ~ \"L\",\n          between(chest, 44, 48) ~ \"XL\",\n          between(chest, 48, 52) ~ \"2XL\",\n          between(chest, 52, 56) ~ \"3XL\",\n          between(chest, 56, 64) ~ \"4XL\",\n          chest &gt; 64 ~ \"5XL\"\n    )\n)\n\n     chest shirt_size\n1 37.90109          M"
  },
  {
    "objectID": "blog/2023/08/19/index.html",
    "href": "blog/2023/08/19/index.html",
    "title": "Building a prediction model to detect spam email",
    "section": "",
    "text": "Getting back into the swing of things. This is my first blog post in more than 3 years!\nFor this post, I‚Äôll be using the Week 33 Tidy Tuesday dataset. This one is all about spam email.\nFrom the dataset description:\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(tidymodels)\nlibrary(usemodels)\nlibrary(future)\nlibrary(rpart)\nlibrary(rpart.plot)\nknitr::opts_chunk$set(echo = TRUE, fig.width = 4.5, fig.height = 2.5)"
  },
  {
    "objectID": "blog/2023/08/19/index.html#average-values-by-spam",
    "href": "blog/2023/08/19/index.html#average-values-by-spam",
    "title": "Building a prediction model to detect spam email",
    "section": "Average values, by spam",
    "text": "Average values, by spam\nLet‚Äôs start by looking at some averages (mean and median), split by the outcome variable.\n\ndf %&gt;%\n  group_by(yesno) %&gt;%\n  summarise_all(mean)\n\n# A tibble: 2 √ó 7\n  yesno crl.tot dollar  bang  money    n000   make\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 TRUE     471. 0.174  0.514 0.213  0.247   0.152 \n2 FALSE    161. 0.0116 0.110 0.0171 0.00709 0.0735\n\n\nWe can see that on average, spam emails have higher mean values for each of the predictors. No surprise there.\nHowever, the medians of some variables are zero, which suggests those variables have heavily positively skewed distributions with many zero values (sometimes called ‚Äúzero-inflation‚Äù).\n\ndf %&gt;%\n  group_by(yesno) %&gt;%\n  summarise_all(median)\n\n# A tibble: 2 √ó 7\n  yesno crl.tot dollar  bang money  n000  make\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 TRUE      194   0.08 0.331     0     0     0\n2 FALSE      54   0    0         0     0     0\n\n\nWe can confirm this by looking at the counts of zero values, in relation to the total counts.\nAs we see, the vast majority of the spam emails had non-zero values on these variables, and non-spam emails had significantly fewer non-zero values, with the exception of crl.tot. In particular, spam emails were MUCH more likely to contain ‚Äú!‚Äù, ‚Äú$‚Äù, ‚Äú000‚Äù, and ‚Äúmoney‚Äù.\n\nno = df %&gt;% filter(yesno == FALSE) %&gt;% select(-yesno)\nyes = df %&gt;% filter(yesno == TRUE) %&gt;% select(-yesno)\n\nround(colSums(no&gt;0)/nrow(no)*100)\n\ncrl.tot  dollar    bang   money    n000    make \n    100      10      27       2       3      15 \n\nround(colSums(yes&gt;0)/nrow(yes)*100)\n\ncrl.tot  dollar    bang   money    n000    make \n    100      61      83      38      33      35"
  },
  {
    "objectID": "blog/2023/08/19/index.html#distributions-by-spam",
    "href": "blog/2023/08/19/index.html#distributions-by-spam",
    "title": "Building a prediction model to detect spam email",
    "section": "Distributions, by spam",
    "text": "Distributions, by spam\nNext, let‚Äôs look at the distributions.\nFor these plots, since they all have extreme skew, I‚Äôm going to truncate them at the 90th percentile and look at the left side where most of the mass is.\n\nfor (c in c('crl.tot', 'dollar', 'bang', 'money', 'n000', 'make')){\n  \n  qtile_90 &lt;- quantile(df[[c]], .90)\n  \n  df %&gt;%\n    filter(!!sym(c) &lt; qtile_90) %&gt;%\n    ggplot(aes(x = !!sym(c), fill=yesno)) +\n      geom_density(alpha=.7) +\n      ggtitle(c) -&gt; plot\n  print(plot)\n  \n}"
  },
  {
    "objectID": "blog/2023/08/19/index.html#feature-correlations",
    "href": "blog/2023/08/19/index.html#feature-correlations",
    "title": "Building a prediction model to detect spam email",
    "section": "Feature correlations",
    "text": "Feature correlations\nIt doesn‚Äôt look like the features are very strongly correlated. The strongest correlation is between n000 and dollar, which is not particularly surprising since I would expect that ‚Äú000‚Äù would tend to appear in the context of a dollar value like ‚Äú$1000‚Äù.\n\ndf %&gt;% \n  select(-yesno) %&gt;%\n  cor(use = \"complete.obs\")\n\n           crl.tot    dollar       bang      money       n000       make\ncrl.tot 1.00000000 0.2019477 0.03632120 0.08099318 0.16597657 0.08916478\ndollar  0.20194768 1.0000000 0.14291296 0.10469131 0.31097072 0.11741853\nbang    0.03632120 0.1429130 1.00000000 0.05107591 0.07010334 0.05829200\nmoney   0.08099318 0.1046913 0.05107591 1.00000000 0.05258693 0.18815518\nn000    0.16597657 0.3109707 0.07010334 0.05258693 1.00000000 0.13407211\nmake    0.08916478 0.1174185 0.05829200 0.18815518 0.13407211 1.00000000\n\n\nIf we convert the features to boolean, we can see that the presence of features have stronger correlations. The strongest correlation is again between dollar and n000, but money and dollar also occur together more often than not.\n\ndf %&gt;%\n  select(-yesno, -crl.tot) %&gt;%\n  mutate(dollar = dollar &gt; 0,\n         bang = bang &gt; 0,\n         money = money &gt; 0,\n         n000 = n000 &gt; 0,\n         make = make &gt; 0\n  ) %&gt;%\n  cor(use = \"complete.obs\")\n\n          dollar      bang     money      n000      make\ndollar 1.0000000 0.3779057 0.5058803 0.5372603 0.4133374\nbang   0.3779057 1.0000000 0.3290505 0.3404896 0.2641339\nmoney  0.5058803 0.3290505 1.0000000 0.4140177 0.4092119\nn000   0.5372603 0.3404896 0.4140177 1.0000000 0.3757565\nmake   0.4133374 0.2641339 0.4092119 0.3757565 1.0000000"
  },
  {
    "objectID": "blog/2023/08/19/index.html#simple-classification-algorithm",
    "href": "blog/2023/08/19/index.html#simple-classification-algorithm",
    "title": "Building a prediction model to detect spam email",
    "section": "Simple classification algorithm",
    "text": "Simple classification algorithm\nJust for fun, let‚Äôs see how well we can distinguish spam vs.¬†not spam using a simple heuristic.\nI‚Äôll label anything as spam if it contained at least 1 ‚Äúmoney‚Äù, ‚Äú$‚Äù, ‚Äú000‚Äù, and ‚Äú!‚Äù OR if it contained more than 100 uninterrupted sequences of capital letters and at least 1 ‚Äú!‚Äù. This is just what comes to mind after looking at the frequency plots above.\n\ndf %&gt;%\n  mutate(simple_spam_flag = factor((money &gt; 0 & dollar &gt; 0 & bang &gt; 0 & n000 &gt; 0) | \n                                     (crl.tot &gt; 100 & bang &gt; 0), \n                                   levels=c(TRUE, FALSE))\n         ) -&gt; df_flag\n\nThis simple classification algorithm achieved an accuracy of 79%, with 64% sensitivity and 89% specificity. This doesn‚Äôt seem too bad. But what is a good baseline of performance?\n\nconfusionMatrix(df_flag$simple_spam_flag, df_flag$yesno, mode='everything')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE  1172   309\n     FALSE  641  2479\n                                          \n               Accuracy : 0.7935          \n                 95% CI : (0.7815, 0.8051)\n    No Information Rate : 0.606           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5533          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.6464          \n            Specificity : 0.8892          \n         Pos Pred Value : 0.7914          \n         Neg Pred Value : 0.7946          \n              Precision : 0.7914          \n                 Recall : 0.6464          \n                     F1 : 0.7116          \n             Prevalence : 0.3940          \n         Detection Rate : 0.2547          \n   Detection Prevalence : 0.3219          \n      Balanced Accuracy : 0.7678          \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\nWe can see that the base rate of spam is 39%.\n\nmean(df$yesno == TRUE)\n\n[1] 0.3940448\n\n\nA good baseline model might be to predict the majority class, which in this case is not-spam.\nThis ‚Äúalways predict FALSE‚Äù baseline model can be expected to achieve 1 minus the base rate of spam (i.e., 61%), and we can see that this is the case if we construct just such a model. This tells us that the heuristic model above is quite a bit better than a completely naive model.\n\nconfusionMatrix(factor(rep('FALSE',nrow(df_flag))), df_flag$yesno, mode='everything')\n\nWarning in confusionMatrix.default(factor(rep(\"FALSE\", nrow(df_flag))), :\nLevels are not in the same order for reference and data. Refactoring data to\nmatch.\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE     0     0\n     FALSE 1813  2788\n                                          \n               Accuracy : 0.606           \n                 95% CI : (0.5917, 0.6201)\n    No Information Rate : 0.606           \n    P-Value [Acc &gt; NIR] : 0.5064          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.000           \n            Specificity : 1.000           \n         Pos Pred Value :   NaN           \n         Neg Pred Value : 0.606           \n              Precision :    NA           \n                 Recall : 0.000           \n                     F1 :    NA           \n             Prevalence : 0.394           \n         Detection Rate : 0.000           \n   Detection Prevalence : 0.000           \n      Balanced Accuracy : 0.500           \n                                          \n       'Positive' Class : TRUE"
  },
  {
    "objectID": "blog/2023/08/19/index.html#decision-tree",
    "href": "blog/2023/08/19/index.html#decision-tree",
    "title": "Building a prediction model to detect spam email",
    "section": "Decision Tree",
    "text": "Decision Tree\nProceeding from simpler to more complex, we can go a step further and try fitting a decision tree model. The decision tree will help us to identify a more sophisticated rule set for classifying spam mail. Decision trees also have the advantage of being highly interpretable.\nWe‚Äôll start by splitting our data into training and test ‚Äì that way, we won‚Äôt be testing performance on the same data that our model was trained on, and we can minimize the risk of overfitting.\n\nset.seed(200)\nsplit &lt;- initial_split(df)\ntrain &lt;- training(split)\ntest &lt;- testing(split)\n\nNext, we‚Äôll fit the model and then visualize its logic.\nWe can read this chart by starting at the root node and following the branches until we reach a terminal node. The predicted value at this terminal node will give us the prediction that the model has made, and the path that we followed to get there provides its reasoning for the prediction.\nSo for example, if we follow the tree to the left-most terminal node, we can see that it would predict that an email was spam if it contained dollar &gt;= 0.056. If we follow the tree to the right-most terminal, we can see that it would predict that an email was not spam if it contained dollar &lt; 0.056 and bang &gt;= 0.12. The percentage value in the node tells us what percentage of emails met these criteria. So 24% of emails met the former criteria, and 54% the latter.\n\ndecision_tree &lt;- rpart(yesno ~ ., data=train, method='class')\ndecision_tree\n\nn= 3450 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 3450 1358 FALSE (0.3936232 0.6063768)  \n   2) dollar&gt;=0.0555 839   94 TRUE (0.8879619 0.1120381) *\n   3) dollar&lt; 0.0555 2611  613 FALSE (0.2347759 0.7652241)  \n     6) bang&gt;=0.1205 739  330 TRUE (0.5534506 0.4465494)  \n      12) crl.tot&gt;=80.5 361   75 TRUE (0.7922438 0.2077562) *\n      13) crl.tot&lt; 80.5 378  123 FALSE (0.3253968 0.6746032)  \n        26) bang&gt;=0.802 85   34 TRUE (0.6000000 0.4000000) *\n        27) bang&lt; 0.802 293   72 FALSE (0.2457338 0.7542662) *\n     7) bang&lt; 0.1205 1872  204 FALSE (0.1089744 0.8910256) *\n\nrpart.plot(decision_tree)\n\n\n\n\nFinally, we can test the model on the out-of-sample test dataset and see how it performs.\nOverall it achieved an accuracy of 85%, with 78% sensitivity and 89% specificity. This model has similar specificity as the heuristic model, but better sensitivity, and therefore better overall accuracy.\n\ntest_pred &lt;- predict(decision_tree, test, type='class')\nconfusionMatrix(test_pred, test$yesno, mode='everything')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE   355    77\n     FALSE  100   619\n                                          \n               Accuracy : 0.8462          \n                 95% CI : (0.8241, 0.8666)\n    No Information Rate : 0.6047          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6755          \n                                          \n Mcnemar's Test P-Value : 0.0982          \n                                          \n            Sensitivity : 0.7802          \n            Specificity : 0.8894          \n         Pos Pred Value : 0.8218          \n         Neg Pred Value : 0.8609          \n              Precision : 0.8218          \n                 Recall : 0.7802          \n                     F1 : 0.8005          \n             Prevalence : 0.3953          \n         Detection Rate : 0.3084          \n   Detection Prevalence : 0.3753          \n      Balanced Accuracy : 0.8348          \n                                          \n       'Positive' Class : TRUE"
  },
  {
    "objectID": "blog/2023/08/19/index.html#random-forest",
    "href": "blog/2023/08/19/index.html#random-forest",
    "title": "Building a prediction model to detect spam email",
    "section": "Random forest",
    "text": "Random forest\nNext, we‚Äôll try a random forest model. Random forest models tend to perform better than decision trees, due to the fact that they are ensemble decision trees, meaning they group together the decisions of lots of decision trees. But as a result, they tend to be less interpretable. So if our goal was only to create the most accurate prediction model possible, then a random forest would be better suited to the task.\n\ncv &lt;- vfold_cv(train)\n\nI‚Äôll use usemodels::use_ranger to give me a starting template.\n\nuse_ranger(yesno ~ ., train)\n\nranger_recipe &lt;- \n  recipe(formula = yesno ~ ., data = train) \n\nranger_spec &lt;- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") \n\nranger_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(ranger_recipe) %&gt;% \n  add_model(ranger_spec) \n\nset.seed(17214)\nranger_tune &lt;-\n  tune_grid(ranger_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n\n\nI‚Äôll remove the parameter tuning to keep things simple.\n\nranger_recipe &lt;- \n  recipe(formula = yesno ~ ., data = df)\n\nranger_spec &lt;- \n  rand_forest(trees = 1000) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") \n\nranger_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(ranger_recipe) %&gt;% \n  add_model(ranger_spec) \n\nNext, I‚Äôll fit the model using a resampling approach.\n\nset.seed(100)\nplan(multisession)\n\nfit_rf &lt;- fit_resamples(\n  ranger_workflow,\n  cv,\n  metrics = metric_set(accuracy, sens, spec),\n  control = control_resamples(verbose = TRUE,\n                              save_pred = TRUE,\n                              extract = function(x) x)\n)\n\nOverall, accuracy is pretty good: 89% accuracy, 79% sensitivity, and 95% specificity.\n\nfit_rf %&gt;%\n  collect_metrics()\n\n# A tibble: 3 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.886    10 0.00497 Preprocessor1_Model1\n2 sens     binary     0.793    10 0.00797 Preprocessor1_Model1\n3 spec     binary     0.946    10 0.00523 Preprocessor1_Model1\n\n\nNext, we can check the performance on the test set.\nWe can use collect_metrics() function on the last fit.\n\nranger_workflow %&gt;%\n  last_fit(split) %&gt;% \n  collect_metrics()\n\n# A tibble: 2 √ó 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.888 Preprocessor1_Model1\n2 roc_auc  binary         0.930 Preprocessor1_Model1\n\n\nOr we can use confusionMatrix() to get a bit more information.\nPerformance on the test set is similar to the training performance. Overall, accuracy is pretty good ‚Äì and better than the decision tree. For a spam detection filter, we‚Äôd want to bias towards minimizing false positives (it would arguably be worse for people to lose legitimate mail to the filter, than to have spam mail slip through), and here we see that the specificity was quite good at ~95%.\n\nranger_workflow %&gt;%\n  last_fit(split) %&gt;% \n  extract_workflow() -&gt; final_model\n\nconfusionMatrix(predict(final_model, test)$.pred_class, test$yesno, mode='everything', positive='TRUE')\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction TRUE FALSE\n     TRUE   362    34\n     FALSE   93   662\n                                          \n               Accuracy : 0.8897          \n                 95% CI : (0.8701, 0.9072)\n    No Information Rate : 0.6047          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7639          \n                                          \n Mcnemar's Test P-Value : 2.652e-07       \n                                          \n            Sensitivity : 0.7956          \n            Specificity : 0.9511          \n         Pos Pred Value : 0.9141          \n         Neg Pred Value : 0.8768          \n              Precision : 0.9141          \n                 Recall : 0.7956          \n                     F1 : 0.8508          \n             Prevalence : 0.3953          \n         Detection Rate : 0.3145          \n   Detection Prevalence : 0.3440          \n      Balanced Accuracy : 0.8734          \n                                          \n       'Positive' Class : TRUE"
  },
  {
    "objectID": "blog/2023/08/25/index.html",
    "href": "blog/2023/08/25/index.html",
    "title": "Using data normalization to better compare change over time in regions with different population sizes",
    "section": "",
    "text": "For this post, I‚Äôll be using the Week 34 Tidy Tuesday dataset, which contains data on refugee movement around the world. I want to look at the change in refugee outflows over time in different nations, and see if I can identify countries with meaningfully large increases in refugee outflows.\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(gghighlight)\ndf &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-22/population.csv')\n\nData cleaning\nFirst, some data cleaning.\nTo keep things simple, I‚Äôm only going to keep nations that had refugee data for all of the 13 years spanning 2010-2022.\n\n(df %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(n_years = n_distinct(year)) %&gt;%\n  filter(n_years == 13))$coo_name -&gt; coo_to_keep\n\ndf %&gt;%\n  filter(coo_name %in% coo_to_keep) %&gt;%\n  select(coo_name,\n         coo_iso,\n         year,\n         refugees) -&gt; df_clean\n\nNormalization\nNext, to make comparisons between nations more apples-apples, I‚Äôm going to do some normalization.\nI want to normalize in terms of population size and change over baseline.\nFirst, I‚Äôll fetch population data from World Bank using wbstats.\n\nwb_search(\"SP.POP.TOTL\", fields='indicator_id') %&gt;%\n  head(1)\n\n# A tibble: 1 √ó 3\n  indicator_id indicator         indicator_desc                                 \n  &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;                                          \n1 SP.POP.TOTL  Population, total Total population is based on the de facto defi‚Ä¶\n\n\n\npops &lt;- wb_data(\"SP.POP.TOTL\", start_date = 2010, end_date = 2022) %&gt;%\n  select(iso3c, date, \"SP.POP.TOTL\") %&gt;%\n  rename(pop = \"SP.POP.TOTL\",\n         iso = iso3c)\n\ndf_clean %&gt;%\n  left_join(pops, by=c('coo_iso'='iso', 'year'='date')) -&gt; df_enriched\n\ndf_enriched %&gt;%\n  head()\n\n# A tibble: 6 √ó 5\n  coo_name               coo_iso  year refugees        pop\n  &lt;chr&gt;                  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan            AFG      2010        0   28189672\n2 Iran (Islamic Rep. of) IRN      2010       30   75373855\n3 Iraq                   IRQ      2010        6   31264875\n4 Pakistan               PAK      2010     6398  194454498\n5 Egypt                  EGY      2010        5   87252413\n6 China                  CHN      2010        6 1337705000\n\n\nNext, I‚Äôll compute a new variable: refugees_per_1k_pop that represents refugees leaving per 1000 persons in the original population. This is a good way to normalize, because we‚Äôd expect a larger count of refugees leaving from countries that had more people to begin with.\n\ndf_enriched %&gt;%\n  group_by(year, coo_name, coo_iso) %&gt;%\n  summarize(refugees = sum(refugees),\n            pop = first(pop)) %&gt;%\n  mutate(refugees_per_1k_pop = refugees/(pop/1000)) -&gt; df_enriched\n\ndf_enriched %&gt;%\n  head()\n\n# A tibble: 6 √ó 6\n# Groups:   year, coo_name [6]\n   year coo_name            coo_iso refugees      pop refugees_per_1k_pop\n  &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;               &lt;dbl&gt;\n1  2010 Afghanistan         AFG      3054699 28189672            108.    \n2  2010 Albania             ALB        14771  2913021              5.07  \n3  2010 Algeria             DZA         6665 35856344              0.186 \n4  2010 Angola              AGO       134851 23364185              5.77  \n5  2010 Antigua and Barbuda ATG           28    85695              0.327 \n6  2010 Argentina           ARG          553 40788453              0.0136\n\n\nI‚Äôll do a bit of cleaning again, to remove those nations for whom I didn‚Äôt have a complete record of population data, and so couldn‚Äôt calculate refugees_per_1k_pop for every year.\n\n(df_enriched %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(n_years = sum(refugees_per_1k_pop &gt; 0, na.rm=T)) %&gt;%\n  filter(n_years == 13))$coo_name -&gt; coo_to_keep_2\n\ndf_enriched %&gt;%\n  filter(coo_name %in% coo_to_keep_2)-&gt; df_enriched_clean\n\nNext, I‚Äôll use 2010 as a baseline year, and subtract each year‚Äôs value from that. This will allow me to measure change over time from this common baseline, and compare nations in terms of a normalized change.\n\ndf_enriched_clean %&gt;%\n  filter(year == 2010) %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(baseline_refugees_per_1k_pop = sum(refugees)/(first(pop)/1000)) -&gt; baseline_year\n\ndf_enriched_clean %&gt;%\n  left_join(baseline_year, by='coo_name') %&gt;%\n  mutate(change_from_baseline = refugees_per_1k_pop - baseline_refugees_per_1k_pop) -&gt; df_enriched_clean\n\ndf_enriched_clean %&gt;%\n  head()\n\n# A tibble: 6 √ó 8\n# Groups:   year, coo_name [6]\n   year coo_name            coo_iso refugees      pop refugees_per_1k_pop\n  &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;               &lt;dbl&gt;\n1  2010 Afghanistan         AFG      3054699 28189672            108.    \n2  2010 Albania             ALB        14771  2913021              5.07  \n3  2010 Algeria             DZA         6665 35856344              0.186 \n4  2010 Angola              AGO       134851 23364185              5.77  \n5  2010 Antigua and Barbuda ATG           28    85695              0.327 \n6  2010 Argentina           ARG          553 40788453              0.0136\n# ‚Ñπ 2 more variables: baseline_refugees_per_1k_pop &lt;dbl&gt;,\n#   change_from_baseline &lt;dbl&gt;\n\n\nIdentifying regions of interest\nNext, I want to identify a smaller set of ‚Äúinteresting‚Äù COOs that have experienced large increases over the baseline. I‚Äôll identify an upper bound percentile of max change over baseline, and then I‚Äôll use a value that approximates that as a filter. This gives me 4 ‚Äúinteresting‚Äù nations.\n\ndf_enriched_clean %&gt;%\n  group_by(coo_name) %&gt;%\n  summarize(max_change_from_baseline = max(change_from_baseline)) %&gt;%\n  summarize(p90_change = quantile(max_change_from_baseline, .975, na.rm=T))\n\n# A tibble: 1 √ó 1\n  p90_change\n       &lt;dbl&gt;\n1       30.0\n\ndf_enriched_clean %&gt;%\n  group_by(coo_name, coo_iso) %&gt;%\n  summarize(max_change_from_baseline = max(change_from_baseline),\n            last_value = last(change_from_baseline, order_by=year)) %&gt;%\n  filter(max_change_from_baseline &gt; 32) -&gt; coos_with_large_changes_over_baseline\n\n`summarise()` has grouped output by 'coo_name'. You can override using the\n`.groups` argument.\n\ncoos_with_large_changes_over_baseline\n\n# A tibble: 4 √ó 4\n# Groups:   coo_name [4]\n  coo_name             coo_iso max_change_from_baseline last_value\n  &lt;chr&gt;                &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt;\n1 Central African Rep. CAF                         99.8       98.7\n2 Eritrea              ERI                         76.9       67.3\n3 Syrian Arab Rep.     SYR                        343.       295. \n4 Ukraine              UKR                        149.       149. \n\n\nData visualization\nFinally, I‚Äôll plot change over the 2010 baseline (in refugees per 1k population), and highlight the 4 interesting nations identified above.\nI‚Äôll use this to help me pick colors for the ggtitle text.\n\nscales::show_col(scales::hue_pal()(4))\n\n\n\n\n\ndf_enriched_clean %&gt;%\n  mutate(class = coo_name %in% coos_with_large_changes_over_baseline$coo_name,\n         year = as.Date(paste0(as.character(year), '-01-01'))) %&gt;%\n  arrange(year, desc(class)) %&gt;%\n  mutate(coo_iso = fct_inorder(coo_iso)) %&gt;%\n  ggplot(aes(x=year, y=change_from_baseline, color=coo_iso)) +\n    geom_line() +\n    scale_x_date(date_labels=\"%Y\", date_breaks=\"1 year\") +\n    ggthemes::theme_solarized() +\n    gghighlight::gghighlight(class == TRUE) +\n    ggtitle(\"&lt;strong&gt;&lt;span style='color:#00BFC4'&gt;SYR&lt;/span&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;span style='color:#C77CFF'&gt;UKR&lt;/span&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;span style='color:#F8766D'&gt;CAF&lt;/span&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;span style='color:#7CAE00'&gt;ERI&lt;/span&gt;&lt;/strong&gt; experienced large increases in&lt;br&gt;normalized refugee outflow (i.e., refugees per 1k population),&lt;br&gt; compared to their 2010 baseline.\") +\n    xlab('Year') +\n    ylab('Change in Normalized Refugee Outflow*') +\n    labs(caption = \"&lt;span style='font-size:7pt'&gt;*Change in refugees per 1k population from the baseline value observed in 2010.&lt;/span&gt;\") +\n    theme(plot.title = ggtext::element_markdown(),\n          plot.caption = ggtext::element_markdown()) -&gt; plot\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\nplot"
  },
  {
    "objectID": "blog/2023/08/31/index.html",
    "href": "blog/2023/08/31/index.html",
    "title": "Joining messy dataframes using fuzzy joins, string cleaning, and column binding",
    "section": "",
    "text": "From the dataset description:\nThis last point is what I‚Äôll be focusing on in this post: The challenge of joining two datasets together that don‚Äôt line up for a clean join.\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(gt)\nfair_use_cases &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-29/fair_use_cases.csv')\nfair_use_findings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-29/fair_use_findings.csv')"
  },
  {
    "objectID": "blog/2023/08/31/index.html#validation-checks",
    "href": "blog/2023/08/31/index.html#validation-checks",
    "title": "Joining messy dataframes using fuzzy joins, string cleaning, and column binding",
    "section": "Validation checks",
    "text": "Validation checks\nWhen inspecting the final result I expect to see 251 rows; none of which should be duplicates, and I do.\n\nfinal_join %&gt;%\n  summarize(\n    n_rows = n(),\n    n_distinct_rows = n_distinct(case, title)\n  )\n\n# A tibble: 1 √ó 2\n  n_rows n_distinct_rows\n   &lt;int&gt;           &lt;int&gt;\n1    251             251\n\n\nAs another check, I can look at the same records that were misaligned with the first approach. I can see they are now aligned.\n\nfinal_join %&gt;% \n  select(case, title, case_number) %&gt;%\n  head(111) %&gt;%\n  tail(3) %&gt;%\n  gt() %&gt;%\n  opt_interactive() %&gt;%\n  tab_header(title = \"Rows that were previously misaligned with bind_cols() are now aligned\") %&gt;%\n  tab_options(table.background.color = '#f1f3f5',\n              ihtml.page_size_default = 3)\n\n\n\n\n\nRows that were previously misaligned with bind_cols() are now aligned\n\n\n\n\n\n\n\nAnd here is the full table.\n\nfinal_join %&gt;% \n  select(case, title, case_number) %&gt;%\n  gt() %&gt;%\n  opt_interactive() %&gt;%\n  tab_header(title = \"Final results table (case, title, case_number)\") %&gt;%\n  tab_options(table.background.color = '#f1f3f5',\n              ihtml.page_size_default = 3)\n\n\n\n\n\nFinal results table (case, title, case_number)\n\n\n\n\n\n\n\nIt looks like the matches are now correct."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "August 31, 2023\n        \n        \n            Joining messy dataframes using fuzzy joins, string cleaning, and column binding\n            \n                \n                    tidy-tuesday\n                \n                \n                    data-cleaning\n                \n                \n                    fuzzy\n                \n            \n            Tidy Tuesday this week presented a challenge: \"There are two datasets this week for which the rows align, but the values might not precisely line up for a clean join.\" In this post I walkthrough my solution that uses a combination of fuzzy joining, string cleaning, and column binding.\n        \n        \n            \n        \n    \n    \n                  \n            August 25, 2023\n        \n        \n            Using data normalization to better compare change over time in regions with different population sizes\n            \n                \n                    tidy-tuesday\n                \n                \n                    data-visualization\n                \n                \n                    data-normalization\n                \n            \n            I use data normalization to better compare the changes in refugee outflows in different regions from 2010 to 2022. Four regions are identified with large increases over their 2010 baseline.\n        \n        \n            \n        \n    \n    \n                  \n            August 19, 2023\n        \n        \n            Building a prediction model to detect spam email\n            \n                \n                    tidy-tuesday\n                \n                \n                    statistical-modeling\n                \n                \n                    machine-learning\n                \n                \n                    prediction\n                \n            \n            Using the spam email dataset from Tidy Tuesday Week 33, I walk through the process of building and evaluating a prediction model using decision tree and random forest machine learning algorithms.\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "",
    "text": "August 31, 2023\n        \n        \n            Joining messy dataframes using fuzzy joins, string cleaning, and column binding\n            \n                \n                    tidy-tuesday\n                \n                \n                    data-cleaning\n                \n                \n                    fuzzy\n                \n            \n            Tidy Tuesday this week presented a challenge: \"There are two datasets this week for which the rows align, but the values might not precisely line up for a clean join.\" In this post I walkthrough my solution that uses a combination of fuzzy joining, string cleaning, and column binding.\n        \n        \n            \n        \n    \n    \n                  \n            August 25, 2023\n        \n        \n            Using data normalization to better compare change over time in regions with different population sizes\n            \n                \n                    tidy-tuesday\n                \n                \n                    data-visualization\n                \n                \n                    data-normalization\n                \n            \n            I use data normalization to better compare the changes in refugee outflows in different regions from 2010 to 2022. Four regions are identified with large increases over their 2010 baseline.\n        \n        \n            \n        \n    \n    \n                  \n            August 19, 2023\n        \n        \n            Building a prediction model to detect spam email\n            \n                \n                    tidy-tuesday\n                \n                \n                    statistical-modeling\n                \n                \n                    machine-learning\n                \n                \n                    prediction\n                \n            \n            Using the spam email dataset from Tidy Tuesday Week 33, I walk through the process of building and evaluating a prediction model using decision tree and random forest machine learning algorithms.\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n    \n                  \n            September 27, 2019\n        \n        \n            Predicting t-shirt size from height and weight\n            \n                \n                    statistical-modeling\n                \n                \n                    machine-learning\n                \n                \n                    prediction\n                \n            \n            Using body measurement data from the National Health and Nutrition Examination Survey (NHANES), I created a model that predicts Gildan t-shirt sizes from height and weight.\n        \n        \n            \n        \n    \n    \n                  \n            September 14, 2019\n        \n        \n            Data science job market analysis\n            \n                \n                    data-visualization\n                \n                \n                    web-scraping\n                \n            \n            To better understand the market for data science jobs, I scraped 11,500 job ads from LinkedIn and explored the skills, tools, and qualifications being sought after in data scientist and analyst positions at different levels of seniority.\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I‚Äôm a Senior Data Scientist at Prodigy Education.\nI received a PhD in Applied Cognitive Science from the University of Guelph‚Äôs College of Social and Applied Human Sciences in 2017. As the top student in my doctoral cohort, I was honoured with the university‚Äôs ‚Äúmost prestigious convocating graduate award,‚Äù the John Vanderkamp Doctoral Graduate Medal.\nI‚Äôm an experienced data and research scientist, having worked in academia for more than 10 years and in industry at health-tech and ed-tech startups for more than 7 years.\nI‚Äôm passionate about applying research and data science to improve decision-making at organizations that want to help people and improve society."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2013/index.html",
    "href": "research/articles/burleigh-et-al-2013/index.html",
    "title": "Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces",
    "section": "",
    "text": "The uncanny valley theory (UVT) (Mori, 1970) proposes that when stimuli are defined by a near-perfect resemblance to humans they cause people to experience greater negative affect relative to when they have perfect human likeness (HL) or little to no HL. Empirical research to support this non-linear relationship between negative affect and HL has been inconclusive, however, and a satisfactory causal explanation has not yet emerged to explain existing findings. In two studies, we examined the relationship between HL and eeriness using digital human faces. First, we examined the relationship between HL and eeriness while controlling for extraneous variation in stimulus appearance. We created two HL continua by manipulating the facial proportions and polygon count of several digital human models. Second, we proposed and tested two causal hypotheses regarding the uncanny valley phenomenon that we refer to as category conflict and feature atypicality. We created two additional HL continua by manipulating the skin coloration and category membership of models. Across these continua we introduced an atypical feature. Our results suggest that HL is linearly related to emotional response, except under conditions where HL varies by category membership, suggesting that previous empirical findings might be explained as a category conflict."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2013/index.html#abstract",
    "href": "research/articles/burleigh-et-al-2013/index.html#abstract",
    "title": "Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces",
    "section": "",
    "text": "The uncanny valley theory (UVT) (Mori, 1970) proposes that when stimuli are defined by a near-perfect resemblance to humans they cause people to experience greater negative affect relative to when they have perfect human likeness (HL) or little to no HL. Empirical research to support this non-linear relationship between negative affect and HL has been inconclusive, however, and a satisfactory causal explanation has not yet emerged to explain existing findings. In two studies, we examined the relationship between HL and eeriness using digital human faces. First, we examined the relationship between HL and eeriness while controlling for extraneous variation in stimulus appearance. We created two HL continua by manipulating the facial proportions and polygon count of several digital human models. Second, we proposed and tested two causal hypotheses regarding the uncanny valley phenomenon that we refer to as category conflict and feature atypicality. We created two additional HL continua by manipulating the skin coloration and category membership of models. Across these continua we introduced an atypical feature. Our results suggest that HL is linearly related to emotional response, except under conditions where HL varies by category membership, suggesting that previous empirical findings might be explained as a category conflict."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2013/index.html#highlights",
    "href": "research/articles/burleigh-et-al-2013/index.html#highlights",
    "title": "Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces",
    "section": "Highlights",
    "text": "Highlights\nWe create four human likeness continua using multiple digital human models. This is done by manipulating facial proportions, realism, and category membership. We obtain human likeness and emotion ratings, and examine their relationships. Linearity is found among all continua except for the category membership continuum. We suggest that this result may explain previous evidence of the uncanny valley."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2017/index.html",
    "href": "research/articles/burleigh-et-al-2017/index.html",
    "title": "Wanting ‚Äòthe whole loaf‚Äô: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists",
    "section": "",
    "text": "Consensual non-monogamy (CNM) is a relationship in which individuals agree that romantic or sexual relationships with others are permissible or desirable (e.g.¬†polyamory or open relationships). Although anti-CNM prejudice is prevalent, it is not well understood. We propose that one of the bases of anti-CNM prejudice is zero-sum thinking about love ‚Äì the perception that one person‚Äôs love gained is another‚Äôs love lost. We outline our theory and then present three studies that test our predictions. In these studies, participants read a vignette that depicted characters who were in a CNM or monogamous relationship, and then judged aspects of the characters and their relationship. In Study 1, participants who read the CNM vignette judged the protagonist‚Äôs love for their initial romantic partner before and after they became involved with a second partner. Zero-sum thinking was operationally defined as the within-subject change in love ratings. In Studies 2 and 3, participants rated their agreement with items from a new preliminary measure of zero-sum romantic beliefs. We measured CNM devaluation by asking for ratings of the relationships and of individuals in the relationships. Supporting our predictions, in all three studies we found that zero-sum thinking about love was associated with increased CNM devaluation. We end by briefly discussing the implications of our findings."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2017/index.html#abstract",
    "href": "research/articles/burleigh-et-al-2017/index.html#abstract",
    "title": "Wanting ‚Äòthe whole loaf‚Äô: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists",
    "section": "",
    "text": "Consensual non-monogamy (CNM) is a relationship in which individuals agree that romantic or sexual relationships with others are permissible or desirable (e.g.¬†polyamory or open relationships). Although anti-CNM prejudice is prevalent, it is not well understood. We propose that one of the bases of anti-CNM prejudice is zero-sum thinking about love ‚Äì the perception that one person‚Äôs love gained is another‚Äôs love lost. We outline our theory and then present three studies that test our predictions. In these studies, participants read a vignette that depicted characters who were in a CNM or monogamous relationship, and then judged aspects of the characters and their relationship. In Study 1, participants who read the CNM vignette judged the protagonist‚Äôs love for their initial romantic partner before and after they became involved with a second partner. Zero-sum thinking was operationally defined as the within-subject change in love ratings. In Studies 2 and 3, participants rated their agreement with items from a new preliminary measure of zero-sum romantic beliefs. We measured CNM devaluation by asking for ratings of the relationships and of individuals in the relationships. Supporting our predictions, in all three studies we found that zero-sum thinking about love was associated with increased CNM devaluation. We end by briefly discussing the implications of our findings."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2017/index.html#important-figure",
    "href": "research/articles/burleigh-et-al-2017/index.html#important-figure",
    "title": "Wanting ‚Äòthe whole loaf‚Äô: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 2. This figure shows that increased zero-sum romantic beliefs were associated with greater devaluation of CNM (steeper slopes) in Study 2. It represents a simple slopes analysis of relationship vignette (monogamous, CNM) predicting evaluative judgment scores at 1 SD above the mean of zero-sum romantic beliefs, at the mean of zero-sum romantic beliefs, and 1 SD below the mean of zero-sum romantic beliefs."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2018/index.html",
    "href": "research/articles/burleigh-et-al-2018/index.html",
    "title": "How to Screen Out VPS and International Respondents Using Qualtrics: A Protocol",
    "section": "",
    "text": "This protocol provides a method and code used to screen out respondents on Amazon‚Äôs Mechanical Turk (MTurk), or other microtask service providers, who are using VPS to cover their location or are responding from a country other than the one the researcher is targeting. It is designed for surveys using Qualtrics software, although it could be easily adapted for other online survey systems that provide JavaScript support. This protocol is likely to be broadly useful in addressing the quality crisis that has affected MTurk studies."
  },
  {
    "objectID": "research/articles/burleigh-et-al-2018/index.html#abstract",
    "href": "research/articles/burleigh-et-al-2018/index.html#abstract",
    "title": "How to Screen Out VPS and International Respondents Using Qualtrics: A Protocol",
    "section": "",
    "text": "This protocol provides a method and code used to screen out respondents on Amazon‚Äôs Mechanical Turk (MTurk), or other microtask service providers, who are using VPS to cover their location or are responding from a country other than the one the researcher is targeting. It is designed for surveys using Qualtrics software, although it could be easily adapted for other online survey systems that provide JavaScript support. This protocol is likely to be broadly useful in addressing the quality crisis that has affected MTurk studies."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2013/index.html",
    "href": "research/articles/burleigh-meegan-2013/index.html",
    "title": "Keeping up with the Joneses affects perceptions of distributive justice",
    "section": "",
    "text": "An experimental field study investigated why people of higher social standing might jump to the conclusion that an injustice has occurred when an authority implements a program that benefits some constituents but not others. High-status individuals are uniquely vulnerable to downward mobility, especially in the event that a situation does not benefit them, but does benefit their high-status peers. In our study, students in a university course were asked to judge a bonus program by which the grades for some would increase and the grades for others would remain the same. Two framing conditions were used, each providing an example in which only one of two students would benefit from the program. In the peer-gets-ahead condition, the two students were of equal status before the program acted to differentiate them, and in the inferior-catches-up condition, the two students differed in status before the program acted to equate them. A majority of students responded favorably to the program, although this number was affected strongly by framing, with almost unanimous approval in the inferior-catches-up condition and comparatively modest approval in the peer-gets-ahead condition. Objections in the latter condition were most frequent among high-status students, who were implicitly uncomfortable with the possibility that their status could decrease relative to some of their high-status peers. Explicitly, their objections used the language of social injustice, especially claims of distributive unfairness. We argue that these perceptions of injustice are a cognitive manifestation of an aversion to any situation that could result in downward mobility."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2013/index.html#abstract",
    "href": "research/articles/burleigh-meegan-2013/index.html#abstract",
    "title": "Keeping up with the Joneses affects perceptions of distributive justice",
    "section": "",
    "text": "An experimental field study investigated why people of higher social standing might jump to the conclusion that an injustice has occurred when an authority implements a program that benefits some constituents but not others. High-status individuals are uniquely vulnerable to downward mobility, especially in the event that a situation does not benefit them, but does benefit their high-status peers. In our study, students in a university course were asked to judge a bonus program by which the grades for some would increase and the grades for others would remain the same. Two framing conditions were used, each providing an example in which only one of two students would benefit from the program. In the peer-gets-ahead condition, the two students were of equal status before the program acted to differentiate them, and in the inferior-catches-up condition, the two students differed in status before the program acted to equate them. A majority of students responded favorably to the program, although this number was affected strongly by framing, with almost unanimous approval in the inferior-catches-up condition and comparatively modest approval in the peer-gets-ahead condition. Objections in the latter condition were most frequent among high-status students, who were implicitly uncomfortable with the possibility that their status could decrease relative to some of their high-status peers. Explicitly, their objections used the language of social injustice, especially claims of distributive unfairness. We argue that these perceptions of injustice are a cognitive manifestation of an aversion to any situation that could result in downward mobility."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2017/index.html",
    "href": "research/articles/burleigh-meegan-2017/index.html",
    "title": "Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?",
    "section": "",
    "text": "When students are faced with the decision of whether to assist a peer, they should be sensitive to the potential risks associated with doing so. Two factors associated with risky helping behaviour in the classroom are: (1) the grading practices that are used, and (2) knowledge of a peer‚Äôs relative status. Normative (‚Äúcurved‚Äù) grading creates a situation in which peer-interactions are potentially competitive, but it is only those interactions with peers of a similar status that carry the potential for assistance to be costly to oneself. In two studies, we created hypothetical scenarios in which the grading practices (normative or absolute) and peer-status proximity (proximate, distant, or unknown) were manipulated, and asked participants to report their willingness to cooperate with a peer by sharing their notes from an important lecture. We found that when normative grading was used, individuals were less willing to assist a peer when they knew that the peer‚Äôs status was proximate to their own. There was also less cooperation when peer status was unknown, under normative grading, which is consistent with a risk-aversion tendency."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2017/index.html#abstract",
    "href": "research/articles/burleigh-meegan-2017/index.html#abstract",
    "title": "Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?",
    "section": "",
    "text": "When students are faced with the decision of whether to assist a peer, they should be sensitive to the potential risks associated with doing so. Two factors associated with risky helping behaviour in the classroom are: (1) the grading practices that are used, and (2) knowledge of a peer‚Äôs relative status. Normative (‚Äúcurved‚Äù) grading creates a situation in which peer-interactions are potentially competitive, but it is only those interactions with peers of a similar status that carry the potential for assistance to be costly to oneself. In two studies, we created hypothetical scenarios in which the grading practices (normative or absolute) and peer-status proximity (proximate, distant, or unknown) were manipulated, and asked participants to report their willingness to cooperate with a peer by sharing their notes from an important lecture. We found that when normative grading was used, individuals were less willing to assist a peer when they knew that the peer‚Äôs status was proximate to their own. There was also less cooperation when peer status was unknown, under normative grading, which is consistent with a risk-aversion tendency."
  },
  {
    "objectID": "research/articles/burleigh-meegan-2017/index.html#important-figure",
    "href": "research/articles/burleigh-meegan-2017/index.html#important-figure",
    "title": "Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 1. Histograms of responses in the Normative grading conditions, indicating the presence of a bimodal distribution in the Uncertain condition, and unimodal distributions in the Distant and Proximate conditions."
  },
  {
    "objectID": "research/articles/burleigh-schoenherr-2015/index.html",
    "href": "research/articles/burleigh-schoenherr-2015/index.html",
    "title": "A reappraisal of the Uncanny Valley: Categorical perception or frequency-based sensitization?",
    "section": "",
    "text": "The uncanny valley (UCV) hypothesis describes a non-linear relationship between perceived human-likeness and affective response. The ‚Äúuncanny valley‚Äù refers to an intermediate level of human-likeness that is associated with strong negative affect. Recent studies have suggested that the uncanny valley might result from the categorical perception of human-like stimuli during identification. When presented with stimuli sharing human-like traits, participants attempt to segment the continuum in ‚Äúhuman‚Äù and ‚Äúnon-human‚Äù categories. Due to the ambiguity of stimuli located at a category boundary, categorization difficulty gives rise to a strong, negative affective response. Importantly, researchers who have studied the UCV in terms of categorical perception have focused on categorization responses rather than affective ratings. In the present study, we examined whether the negative affect associated with the UCV might be explained in terms of an individual‚Äôs degree of exposure to stimuli. In two experiments, we tested a frequency-based model against a categorical perception model using a category-learning paradigm. We manipulated the frequency of exemplars that were presented to participants from two categories during a training phase. We then examined categorization and affective responses functions, as well as the relationship between categorization and affective responses. Supporting previous findings, categorization responses suggested that participants acquired novel category structures that reflected a category boundary. These category structures appeared to influence affective ratings of eeriness. Crucially, participants‚Äô ratings of eeriness were additionally affected by exemplar frequency. Taken together, these findings suggest that the UCV is determined by both categorical properties as well as the frequency of individual exemplars retained in memory."
  },
  {
    "objectID": "research/articles/burleigh-schoenherr-2015/index.html#abstract",
    "href": "research/articles/burleigh-schoenherr-2015/index.html#abstract",
    "title": "A reappraisal of the Uncanny Valley: Categorical perception or frequency-based sensitization?",
    "section": "",
    "text": "The uncanny valley (UCV) hypothesis describes a non-linear relationship between perceived human-likeness and affective response. The ‚Äúuncanny valley‚Äù refers to an intermediate level of human-likeness that is associated with strong negative affect. Recent studies have suggested that the uncanny valley might result from the categorical perception of human-like stimuli during identification. When presented with stimuli sharing human-like traits, participants attempt to segment the continuum in ‚Äúhuman‚Äù and ‚Äúnon-human‚Äù categories. Due to the ambiguity of stimuli located at a category boundary, categorization difficulty gives rise to a strong, negative affective response. Importantly, researchers who have studied the UCV in terms of categorical perception have focused on categorization responses rather than affective ratings. In the present study, we examined whether the negative affect associated with the UCV might be explained in terms of an individual‚Äôs degree of exposure to stimuli. In two experiments, we tested a frequency-based model against a categorical perception model using a category-learning paradigm. We manipulated the frequency of exemplars that were presented to participants from two categories during a training phase. We then examined categorization and affective responses functions, as well as the relationship between categorization and affective responses. Supporting previous findings, categorization responses suggested that participants acquired novel category structures that reflected a category boundary. These category structures appeared to influence affective ratings of eeriness. Crucially, participants‚Äô ratings of eeriness were additionally affected by exemplar frequency. Taken together, these findings suggest that the UCV is determined by both categorical properties as well as the frequency of individual exemplars retained in memory."
  },
  {
    "objectID": "research/articles/burleigh-schoenherr-2015/index.html#important-figure",
    "href": "research/articles/burleigh-schoenherr-2015/index.html#important-figure",
    "title": "A reappraisal of the Uncanny Valley: Categorical perception or frequency-based sensitization?",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFIGURE 12: Test categorization response accuracy in the unequal frequency, unequal distribution condition. Stimulus values correspond to stimuli selected from the training range (i.e., stimuli 3‚Äì13). Error bars represent 1 standard error of the mean (N = 60)."
  },
  {
    "objectID": "research/articles/ferrey-et-al-2015/index.html",
    "href": "research/articles/ferrey-et-al-2015/index.html",
    "title": "Stimulus-category competition, inhibition and affective devaluation: A novel account of the Uncanny Valley",
    "section": "",
    "text": "Stimuli that resemble humans, but are not perfectly human-like, are disliked compared to distinctly human and non-human stimuli. Accounts of this ‚ÄúUncanny Valley‚Äù effect often focus on how changes in human resemblance can evoke different emotional responses. We present an alternate account based on the novel hypothesis that the Uncanny Valley is not directly related to ‚Äòhuman-likeness‚Äô per se, but instead reflects a more general form of stimulus devaluation that occurs when inhibition is triggered to resolve conflict between competing stimulus-related representations. We consider existing support for this inhibitory-devaluation hypothesis and further assess its feasibility through tests of two corresponding predictions that arise from the link between conflict-resolving inhibition and aversive response: (1) that the pronounced disliking of Uncanny-type stimuli will occur for any image that strongly activates multiple competing stimulus representations, even in the absence of any human-likeness, and (2) that the negative peak of an ‚ÄòUncanny Valley‚Äô should occur at the point of greatest stimulus-related conflict and not (in the presence of human-likeness) always closer to the ‚Äòhuman‚Äô end of a perceptual continuum. We measured affective responses to a set of line drawings representing non-human animal‚Äìanimal morphs, in which each continuum midpoint was a bistable image (Experiment 1), as well as to sets of human-robot and human-animal computer-generated morphs (Experiment 2). Affective trends depicting classic Uncanny Valley functions occurred for all continua, including the non-human stimuli. Images at continua midpoints elicited significantly more negative affect than images at endpoints, even when the continua included a human endpoint. This illustrates the feasibility of the inhibitory-devaluation hypothesis and the need for further research into the possibility that the strong dislike of Uncanny-type stimuli reflects the negative affective consequences of cognitive inhibition."
  },
  {
    "objectID": "research/articles/ferrey-et-al-2015/index.html#abstract",
    "href": "research/articles/ferrey-et-al-2015/index.html#abstract",
    "title": "Stimulus-category competition, inhibition and affective devaluation: A novel account of the Uncanny Valley",
    "section": "",
    "text": "Stimuli that resemble humans, but are not perfectly human-like, are disliked compared to distinctly human and non-human stimuli. Accounts of this ‚ÄúUncanny Valley‚Äù effect often focus on how changes in human resemblance can evoke different emotional responses. We present an alternate account based on the novel hypothesis that the Uncanny Valley is not directly related to ‚Äòhuman-likeness‚Äô per se, but instead reflects a more general form of stimulus devaluation that occurs when inhibition is triggered to resolve conflict between competing stimulus-related representations. We consider existing support for this inhibitory-devaluation hypothesis and further assess its feasibility through tests of two corresponding predictions that arise from the link between conflict-resolving inhibition and aversive response: (1) that the pronounced disliking of Uncanny-type stimuli will occur for any image that strongly activates multiple competing stimulus representations, even in the absence of any human-likeness, and (2) that the negative peak of an ‚ÄòUncanny Valley‚Äô should occur at the point of greatest stimulus-related conflict and not (in the presence of human-likeness) always closer to the ‚Äòhuman‚Äô end of a perceptual continuum. We measured affective responses to a set of line drawings representing non-human animal‚Äìanimal morphs, in which each continuum midpoint was a bistable image (Experiment 1), as well as to sets of human-robot and human-animal computer-generated morphs (Experiment 2). Affective trends depicting classic Uncanny Valley functions occurred for all continua, including the non-human stimuli. Images at continua midpoints elicited significantly more negative affect than images at endpoints, even when the continua included a human endpoint. This illustrates the feasibility of the inhibitory-devaluation hypothesis and the need for further research into the possibility that the strong dislike of Uncanny-type stimuli reflects the negative affective consequences of cognitive inhibition."
  },
  {
    "objectID": "research/articles/ferrey-et-al-2015/index.html#important-figure",
    "href": "research/articles/ferrey-et-al-2015/index.html#important-figure",
    "title": "Stimulus-category competition, inhibition and affective devaluation: A novel account of the Uncanny Valley",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFIGURE 4. Computer-generated morphs (human-robot, human-stag, human-tiger, human-lion, human-bird)."
  },
  {
    "objectID": "research/articles/kennedy-et-al-2020/index.html",
    "href": "research/articles/kennedy-et-al-2020/index.html",
    "title": "The shape of and solutions to the MTurk quality crisis",
    "section": "",
    "text": "Amazon‚Äôs Mechanical Turk is widely used for data collection; however, data quality may be declining due to the use of virtual private servers to fraudulently gain access to studies. Unfortunately, we know little about the scale and consequence of this fraud, and tools for social scientists to detect and prevent this fraud are underdeveloped. We first analyze 38 studies and show that this fraud is not new, but has increased recently. We then show that these fraudulent respondents provide particularly low-quality data and can weaken treatment effects. Finally, we provide two solutions: an easy-to-use application for identifying fraud in the existing datasets and a method for blocking fraudulent respondents in Qualtrics surveys."
  },
  {
    "objectID": "research/articles/kennedy-et-al-2020/index.html#abstract",
    "href": "research/articles/kennedy-et-al-2020/index.html#abstract",
    "title": "The shape of and solutions to the MTurk quality crisis",
    "section": "",
    "text": "Amazon‚Äôs Mechanical Turk is widely used for data collection; however, data quality may be declining due to the use of virtual private servers to fraudulently gain access to studies. Unfortunately, we know little about the scale and consequence of this fraud, and tools for social scientists to detect and prevent this fraud are underdeveloped. We first analyze 38 studies and show that this fraud is not new, but has increased recently. We then show that these fraudulent respondents provide particularly low-quality data and can weaken treatment effects. Finally, we provide two solutions: an easy-to-use application for identifying fraud in the existing datasets and a method for blocking fraudulent respondents in Qualtrics surveys."
  },
  {
    "objectID": "research/articles/kennedy-et-al-2020/index.html#important-figure",
    "href": "research/articles/kennedy-et-al-2020/index.html#important-figure",
    "title": "The shape of and solutions to the MTurk quality crisis",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 5. Path diagram of screening protocol."
  },
  {
    "objectID": "research/articles/rubel-burleigh-2018/index.html",
    "href": "research/articles/rubel-burleigh-2018/index.html",
    "title": "Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy",
    "section": "",
    "text": "Despite a growing interest in polyamory, it is unknown how many polyamorists there are in the general population. In acknowledging that the meaning of ‚Äúpolyamory‚Äù is contested (e.g.¬†Klesse, 2014), we estimated the prevalence of polyamory when it was defined as: (1) an identity, (2) relationship beliefs/preferences, (3) relationship status, and (4) relationship agreements. We recruited 972 individuals from Mechanical Turk and used a sample weighting procedure to approximate a representative sample of the population of the USA. Point prevalence estimates ranged from about 0.6% to 5%, and lifetime estimates ranged from about 2% to 23%. Thus, we estimate that there are at least 1.44 million adults in the USA who count as polyamorous."
  },
  {
    "objectID": "research/articles/rubel-burleigh-2018/index.html#abstract",
    "href": "research/articles/rubel-burleigh-2018/index.html#abstract",
    "title": "Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy",
    "section": "",
    "text": "Despite a growing interest in polyamory, it is unknown how many polyamorists there are in the general population. In acknowledging that the meaning of ‚Äúpolyamory‚Äù is contested (e.g.¬†Klesse, 2014), we estimated the prevalence of polyamory when it was defined as: (1) an identity, (2) relationship beliefs/preferences, (3) relationship status, and (4) relationship agreements. We recruited 972 individuals from Mechanical Turk and used a sample weighting procedure to approximate a representative sample of the population of the USA. Point prevalence estimates ranged from about 0.6% to 5%, and lifetime estimates ranged from about 2% to 23%. Thus, we estimate that there are at least 1.44 million adults in the USA who count as polyamorous."
  },
  {
    "objectID": "research/articles/rubel-burleigh-2018/index.html#important-figure",
    "href": "research/articles/rubel-burleigh-2018/index.html#important-figure",
    "title": "Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFigure 1. Interest in polyamory has been increasing over time, as measured by records in Google Ngram database with ‚Äúpolyamory‚Äù keyword (top) and records in the Web of Science database with ‚Äúpolyamory‚Äù or ‚Äúpolyamorous‚Äù keywords (bottom)."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2015/index.html",
    "href": "research/articles/schoenherr-burleigh-2015/index.html",
    "title": "Uncanny sociocultural categories",
    "section": "",
    "text": "Considered individually, folkbiological categories, biological anomalies and monsters, as well as human categories represent individual cultural products of human categorization. Instead, we suggest that the uncanny valley might reflect a primary response to unfamiliar or covert categories. In the absence of having prior knowledge of an individual or group, the relative distinctiveness of a category, due to a lower frequency of exposure, will produce negative affect‚Äîan inversion of the mere-exposure effect. The deceptive simplicity of learning mechanisms can lead to important individual and social consequences."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2015/index.html#abstract",
    "href": "research/articles/schoenherr-burleigh-2015/index.html#abstract",
    "title": "Uncanny sociocultural categories",
    "section": "",
    "text": "Considered individually, folkbiological categories, biological anomalies and monsters, as well as human categories represent individual cultural products of human categorization. Instead, we suggest that the uncanny valley might reflect a primary response to unfamiliar or covert categories. In the absence of having prior knowledge of an individual or group, the relative distinctiveness of a category, due to a lower frequency of exposure, will produce negative affect‚Äîan inversion of the mere-exposure effect. The deceptive simplicity of learning mechanisms can lead to important individual and social consequences."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2020/index.html",
    "href": "research/articles/schoenherr-burleigh-2020/index.html",
    "title": "Dissociating affective and cognitive dimensions of uncertainty by altering regulatory focus",
    "section": "",
    "text": "Cognitive uncertainty is evidenced across learning, memory, and decision-making tasks. Uncertainty has also been examined in studies of positive affect and preference by manipulating stimulus presentation frequency. Despite the extensive research in both of these areas, there has been little systematic study into the relationship between affective and cognitive uncertainty. Using a categorization task, the present study examined changes in cognitive and affective uncertainty by manipulating stimulus presentation frequency and processing focus (i.e., promotion v. prevention focus). Following training, participants categorized stimuli and provided ratings of both typicality and negative affect. Results indicated that cognitive uncertainty was influenced by a categorical representation of stimuli whereas affective uncertainty was also influenced by exemplar presentation frequency during training. We additionally found that when the training was framed in terms of the avoidance of errors (i.e., a prevention focus), categorization performance was affected across the stimulus continuum whereas affective ratings remained unchanged."
  },
  {
    "objectID": "research/articles/schoenherr-burleigh-2020/index.html#abstract",
    "href": "research/articles/schoenherr-burleigh-2020/index.html#abstract",
    "title": "Dissociating affective and cognitive dimensions of uncertainty by altering regulatory focus",
    "section": "",
    "text": "Cognitive uncertainty is evidenced across learning, memory, and decision-making tasks. Uncertainty has also been examined in studies of positive affect and preference by manipulating stimulus presentation frequency. Despite the extensive research in both of these areas, there has been little systematic study into the relationship between affective and cognitive uncertainty. Using a categorization task, the present study examined changes in cognitive and affective uncertainty by manipulating stimulus presentation frequency and processing focus (i.e., promotion v. prevention focus). Following training, participants categorized stimuli and provided ratings of both typicality and negative affect. Results indicated that cognitive uncertainty was influenced by a categorical representation of stimuli whereas affective uncertainty was also influenced by exemplar presentation frequency during training. We additionally found that when the training was framed in terms of the avoidance of errors (i.e., a prevention focus), categorization performance was affected across the stimulus continuum whereas affective ratings remained unchanged."
  },
  {
    "objectID": "research/articles/sparks-et-al-2016/index.html",
    "href": "research/articles/sparks-et-al-2016/index.html",
    "title": "We can see inside: Accurate prediction of Prisoner‚Äôs Dilemma decisions in announced games following a face-to-face interaction",
    "section": "",
    "text": "Humans form impressions and make social judgments about others based on information that is quickly and easily available, such as facial and vocal traits. The evolutionary function of impression formation and social judgment mechanisms have received limited attention in psychology research; we argue that their function is to accurately forecast the behavior of others. There is some evidence for the predictive accuracy of social judgments, but much of it comes from situations where there is little incentive to deceive, which limits applicability to questions of the function of such mechanisms. A classic experiment that avoids this problem was conducted by R. H. Frank, T. Gilovich, and D. T. Regan (1993); their participants predicted each other‚Äôs Prisoner‚Äôs Dilemma Game decisions with above-chance accuracy after a short interaction period, knowing the game would follow. We report three original studies that replicate these aspects of the methods of Frank et al.¬†(1993) and reanalyze data from all known replications. Our meta-analysis of these studies confirms the original report: humans can predict each other‚Äôs Prisoner‚Äôs Dilemma decisions after a brief interaction with people who have incentive to deceive."
  },
  {
    "objectID": "research/articles/sparks-et-al-2016/index.html#abstract",
    "href": "research/articles/sparks-et-al-2016/index.html#abstract",
    "title": "We can see inside: Accurate prediction of Prisoner‚Äôs Dilemma decisions in announced games following a face-to-face interaction",
    "section": "",
    "text": "Humans form impressions and make social judgments about others based on information that is quickly and easily available, such as facial and vocal traits. The evolutionary function of impression formation and social judgment mechanisms have received limited attention in psychology research; we argue that their function is to accurately forecast the behavior of others. There is some evidence for the predictive accuracy of social judgments, but much of it comes from situations where there is little incentive to deceive, which limits applicability to questions of the function of such mechanisms. A classic experiment that avoids this problem was conducted by R. H. Frank, T. Gilovich, and D. T. Regan (1993); their participants predicted each other‚Äôs Prisoner‚Äôs Dilemma Game decisions with above-chance accuracy after a short interaction period, knowing the game would follow. We report three original studies that replicate these aspects of the methods of Frank et al.¬†(1993) and reanalyze data from all known replications. Our meta-analysis of these studies confirms the original report: humans can predict each other‚Äôs Prisoner‚Äôs Dilemma decisions after a brief interaction with people who have incentive to deceive."
  },
  {
    "objectID": "research/articles/sparks-et-al-2016/index.html#important-figure",
    "href": "research/articles/sparks-et-al-2016/index.html#important-figure",
    "title": "We can see inside: Accurate prediction of Prisoner‚Äôs Dilemma decisions in announced games following a face-to-face interaction",
    "section": "Important figure",
    "text": "Important figure\n\n\n\nFig. 1. Forest plot‚Äîmarker size indicates a study‚Äôs weight in the combined estimate."
  },
  {
    "objectID": "research/articles/winter-et-al-2019/index.html",
    "href": "research/articles/winter-et-al-2019/index.html",
    "title": "A simplified protocol to screen out VPS and international respondents using Qualtrics",
    "section": "",
    "text": "This protocol is a much simplified update of our original protocol for screening out international and VPS respondents from Qualtrics surveys. It utilizes Qualtrics‚Äô built-in Web Service functionality to interact with IP Hub in looking up potentially problematic respondents."
  },
  {
    "objectID": "research/articles/winter-et-al-2019/index.html#abstract",
    "href": "research/articles/winter-et-al-2019/index.html#abstract",
    "title": "A simplified protocol to screen out VPS and international respondents using Qualtrics",
    "section": "",
    "text": "This protocol is a much simplified update of our original protocol for screening out international and VPS respondents from Qualtrics surveys. It utilizes Qualtrics‚Äô built-in Web Service functionality to interact with IP Hub in looking up potentially problematic respondents."
  },
  {
    "objectID": "research/articles/wood-et-al-2018/index.html",
    "href": "research/articles/wood-et-al-2018/index.html",
    "title": "Reasons for sex and relational outcomes in consensually nonmonogamous and monogamous relationships: A self-determination theory approach",
    "section": "",
    "text": "Approximately 4% of individuals in North America participate in consensually nonmonogamous (CNM) relationships, wherein all partners have agreed to additional sexual and/or emotional partnerships. The CNM relationships are stigmatized and viewed as less stable and satisfying than monogamous relationships, a perception that persists despite research evidence. In our study, we assess the legitimacy of this negative perception by using a self-determination theory (SDT) framework to explore how sexual motivation impacts relational and sexual satisfaction among CNM and monogamous participants in romantic relationships. A total of 348 CNM (n = 142) and monogamous participants (n = 206) were recruited from Amazon‚Äôs Mechanical Turk (MTurk. (2016). www.mturk.com) to complete a cross-sectional survey. Participants reported on their sexual motivations during their most recent sexual event, their level of sexual need fulfillment, and measures of sexual and relational satisfaction with their current (primary) partner. The CNM and monogamous participants reported similar reasons for engaging in sex, though CNM participants were significantly more likely to have sex for personal intrinsic motives. No differences in mean levels of relationship and sexual satisfaction were found between CNM and monogamous individuals. Participants who engaged in sex for more self-determined reasons reported increased relational and sexual satisfaction. This relationship was mediated by sexual need fulfillment; participants who reported more self-determined motives reported higher levels of need fulfillment and, in turn, greater relationship and sexual satisfaction. This study indicates that CNM and monogamous individuals report similar levels of satisfaction within their relationship(s) and that the mechanisms that affect relational and sexual satisfaction are similar for both CNM and monogamous individuals. Our research extends theoretical understandings of motivation within romantic relationships and suggests that SDT is a useful framework for considering the impact of sexual motivation on relational outcomes."
  },
  {
    "objectID": "research/articles/wood-et-al-2018/index.html#abstract",
    "href": "research/articles/wood-et-al-2018/index.html#abstract",
    "title": "Reasons for sex and relational outcomes in consensually nonmonogamous and monogamous relationships: A self-determination theory approach",
    "section": "",
    "text": "Approximately 4% of individuals in North America participate in consensually nonmonogamous (CNM) relationships, wherein all partners have agreed to additional sexual and/or emotional partnerships. The CNM relationships are stigmatized and viewed as less stable and satisfying than monogamous relationships, a perception that persists despite research evidence. In our study, we assess the legitimacy of this negative perception by using a self-determination theory (SDT) framework to explore how sexual motivation impacts relational and sexual satisfaction among CNM and monogamous participants in romantic relationships. A total of 348 CNM (n = 142) and monogamous participants (n = 206) were recruited from Amazon‚Äôs Mechanical Turk (MTurk. (2016). www.mturk.com) to complete a cross-sectional survey. Participants reported on their sexual motivations during their most recent sexual event, their level of sexual need fulfillment, and measures of sexual and relational satisfaction with their current (primary) partner. The CNM and monogamous participants reported similar reasons for engaging in sex, though CNM participants were significantly more likely to have sex for personal intrinsic motives. No differences in mean levels of relationship and sexual satisfaction were found between CNM and monogamous individuals. Participants who engaged in sex for more self-determined reasons reported increased relational and sexual satisfaction. This relationship was mediated by sexual need fulfillment; participants who reported more self-determined motives reported higher levels of need fulfillment and, in turn, greater relationship and sexual satisfaction. This study indicates that CNM and monogamous individuals report similar levels of satisfaction within their relationship(s) and that the mechanisms that affect relational and sexual satisfaction are similar for both CNM and monogamous individuals. Our research extends theoretical understandings of motivation within romantic relationships and suggests that SDT is a useful framework for considering the impact of sexual motivation on relational outcomes."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research primarily falls within the domain of cognitive and social psychology. Most of my research has been experimental in nature. Topics include the ‚ÄúUncanny Valley‚Äù, zero-sum thinking, prejudice, social justice, competitiveness, and online research methods."
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n        \n            \n                Kennedy, R., Clifford, S., Burleigh, T., Waggoner, P. D., Jewell, R., & Winter, N. J. (2020). The shape of and solutions to the MTurk quality crisis. Political Science Research and Methods, 8(4), 614-629. doi: 10.1017/psrm.2020.6 \n            \n            \n                    \n                            Research Methods\n                        \n                    \n                            Online Research\n                        \n                    \n                            Mechanical Turk\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Open access\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Schoenherr, J. R., & Burleigh, T. J. (2020). Dissociating affective and cognitive dimensions of uncertainty by altering regulatory focus. Acta Psychologica, 205, 103017. doi: 10.1016/j.actpsy.2020.103017\n            \n            \n                    \n                            Uncanny Valley\n                        \n                    \n                            Cognition\n                        \n                    \n                            Categorical Perception\n                        \n                    \n                            Experimental\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Winter, N., Burleigh, T., Kennedy, R., & Clifford, S. (2019). A simplified protocol to screen out VPS and international respondents using Qualtrics. Available at SSRN 3327274. doi: 10.2139/ssrn.3327274 \n            \n            \n                    \n                            Online Research\n                        \n                    \n                            Research Methods\n                        \n                    \n                            Mechanical Turk\n                        \n                    \n                            Qualtrics\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Burleigh, T., Kennedy, R., & Clifford, S. (2018). How to screen out VPS and international respondents using Qualtrics: A protocol. Available at SSRN 3265459. doi: 10.2139/ssrn.3265459 \n            \n            \n                    \n                            Research Methods\n                        \n                    \n                            Online Research\n                        \n                    \n                            Mechanical Turk\n                        \n                    \n                            Qualtrics\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Burleigh, T., & Rubel, A. (2018). Counting polyamorists who count: Prevalence and definitions of an under-researched form of consensual nonmonogamy. doi: 10.1177/1363460718779781\n            \n            \n                    \n                            Online Research\n                        \n                    \n                            Demographics\n                        \n                    \n                            Sexuality\n                        \n                    \n                            Consensual Non-Monogamy\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Sparks, A., Burleigh, T., & Barclay, P. (2016). We can see inside: Accurate prediction of Prisoner's Dilemma decisions in announced games following a face-to-face interaction. Evolution and Human Behavior, 37(3), 210-216. doi: 10.1016/j.evolhumbehav.2015.11.003\n            \n            \n                    \n                            Social Cognition\n                        \n                    \n                            Experimental\n                        \n                    \n                            Meta-Analysis\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Open Access\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Wood, J., Desmarais, S., Burleigh, T., & Milhausen, R. (2018). Reasons for sex and relational outcomes in consensually nonmonogamous and monogamous relationships: A self-determination theory approach. Journal of Social and Personal Relationships, 35(4), 632-654. doi: 10.1177/0265407517743082\n            \n            \n                    \n                            Sexuality\n                        \n                    \n                            Consensual Non-Monogamy\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Burleigh, T. J., & Meegan, D. V. (2018). Risky prospects and risk aversion tendencies: does competition in the classroom depend on grading practices and knowledge of peer-status?. Social Psychology of Education, 21, 323-335. doi: 10.1007/s11218-017-9414-x\n            \n            \n                    \n                            Social Justice\n                        \n                    \n                            Social Cognition\n                        \n                    \n                            Competition\n                        \n                    \n                            Experimental\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Burleigh, T. J., Rubel, A. N., & Meegan, D. V. (2017). Wanting ‚Äòthe whole loaf‚Äô: Zero-sum thinking about love is associated with prejudice against consensual non-monogamists. Psychology & Sexuality, 8(1-2), 24-40. doi: 10.1080/19419899.2016.1269020\n            \n            \n                    \n                            Sexuality\n                        \n                    \n                            Prejudice\n                        \n                    \n                            Consensual Non-Monogamy\n                        \n                    \n                            Zero-sum Thinking\n                        \n                    \n                            Experimental\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Ferrey, A. E., Burleigh, T. J., & Fenske, M. J. (2015). Stimulus-category competition, inhibition, and affective devaluation: a novel account of the uncanny valley. Frontiers in psychology, 6, 249. doi: 10.3389/fpsyg.2015.00249\n            \n            \n                    \n                            Cognition\n                        \n                    \n                            Categorical Perception\n                        \n                    \n                            Experimental\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Burleigh, T. J., & Schoenherr, J. R. (2015). A reappraisal of the uncanny valley: categorical perception or frequency-based sensitization?. Frontiers in Psychology, 5, 1488. doi: 10.3389/fpsyg.2014.01488\n            \n            \n                    \n                            Uncanny Valley\n                        \n                    \n                            Categorical Perception\n                        \n                    \n                            Cognition\n                        \n                    \n                            Experimental\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Open access\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Schoenherr, J. R., & Burleigh, T. J. (2015). Uncanny sociocultural categories. Frontiers in Psychology, 5, 1456. doi: 10.3389/fpsyg.2014.01456\n            \n            \n                    \n                            Uncanny Valley\n                        \n                    \n                            Cognition\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Burleigh, T. J., Schoenherr, J. R., & Lacroix, G. L. (2013). Does the uncanny valley exist? An empirical test of the relationship between eeriness and the human likeness of digitally created faces. Computers in human behavior, 29(3), 759-771. doi: 10.1016/j.chb.2012.11.021\n            \n            \n                    \n                            Uncanny Valley\n                        \n                    \n                            Categorical Perception\n                        \n                    \n                            Cognition\n                        \n                    \n                            Experimental\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n        \n            \n                Burleigh, T. J., & Meegan, D. V. (2013). Keeping up with the Joneses affects perceptions of distributive justice. Social justice research, 26, 120-131. doi: 10.1007/s11211-013-0181-3\n            \n            \n                    \n                            Social Justice\n                        \n                    \n                            Social Cognition\n                        \n                    \n                            Fairness\n                        \n                    \n                            Experimental\n                        \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                \n            \n        \n\n\n\nNo matching items"
  }
]